#!/usr/bin/env python3
"""
è®­ç»ƒæ—¥å¿—å¯è§†åŒ–å·¥å…·
è§£ætrain.logæ–‡ä»¶ï¼Œç»˜åˆ¶Lossã€MAEã€CleanAccã€RobustAccã€FeatDiffæŠ˜çº¿å›¾
æ”¯æŒå¤šä¸ªè®­ç»ƒæ—¥å¿—æ–‡ä»¶å¯¹æ¯”ï¼Œä»¥åŠåŸå§‹CLIPå’ŒFAREæ¨¡å‹åŸºå‡†å¯¹æ¯”

åŸºå‡†æ•°æ®æ¥æºè¯´æ˜ï¼š
1. OpenAI CLIP (Baseline):
   - ä½¿ç”¨OpenAIé¢„è®­ç»ƒçš„ViT-L-14 CLIPæ¨¡å‹
   - åœ¨ImageNetéªŒè¯é›†ä¸Šæµ‹è¯•å¾—åˆ°CleanAccå’ŒRobustAcc
   
2. FAREæ¨¡å‹ (eps=2 å’Œ eps=4):
   - FARE: Feature-Aware Robust CLIPæ¨¡å‹
   - eps=2: ä½¿ç”¨L2èŒƒæ•°çº¦æŸï¼ˆeps=2ï¼‰è¿›è¡Œå¯¹æŠ—è®­ç»ƒçš„FAREæ¨¡å‹
   - eps=4: ä½¿ç”¨L-infinityèŒƒæ•°çº¦æŸï¼ˆeps=4ï¼‰è¿›è¡Œå¯¹æŠ—è®­ç»ƒçš„FAREæ¨¡å‹
   - æ¨¡å‹æƒé‡è·¯å¾„: models/fare_eps_2.pt, models/fare_eps_4.pt

å¦‚ä½•è·å–åŸºå‡†æ•°æ®ï¼š
  éœ€è¦è¿è¡Œè¯„ä¼°è„šæœ¬æ¥è·å–å‡†ç¡®çš„åŸºå‡†æ€§èƒ½ï¼š
  
  # è¯„ä¼°OpenAI CLIP
  python -m train.adversarial_training_clip \
      --clip_model_name ViT-L-14 --pretrained openai \
      --dataset imagenet --attack pgd --norm linf --eps 4.0 \
      --iterations_adv 10 --stepsize_adv 1.0 \
      --eval_only --batch_size 128
  
  # è¯„ä¼°FARE eps=4
  python -m train.adversarial_training_clip \
      --clip_model_name ViT-L-14 --pretrained models/fare_eps_4.pt \
      --dataset imagenet --attack pgd --norm linf --eps 4.0 \
      --iterations_adv 10 --stepsize_adv 1.0 \
      --eval_only --batch_size 128

æ³¨æ„ï¼š
- åŸºå‡†æ¨¡å‹æ˜¾ç¤ºä¸ºæ˜Ÿå½¢æ ‡è®°ç‚¹ï¼ˆä¸æ˜¯çº¿ï¼‰ï¼Œå› ä¸ºå®ƒä»¬æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„å›ºå®šæ€§èƒ½
- è®­ç»ƒæ¨¡å‹æ˜¾ç¤ºä¸ºæŠ˜çº¿å›¾ï¼Œå±•ç¤ºè®­ç»ƒè¿‡ç¨‹ä¸­æŒ‡æ ‡çš„å˜åŒ–è¶‹åŠ¿
- æ‰€æœ‰å…³é”®æ•°å€¼éƒ½ä¼šè‡ªåŠ¨æ ‡æ³¨åœ¨å›¾è¡¨ä¸Š
- è¯·é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æä¾›çœŸå®çš„è¯„ä¼°æ•°æ®ï¼Œä¸è¦ä½¿ç”¨é»˜è®¤å ä½ç¬¦
"""

import re
import os
import argparse
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple


def parse_log_file(log_path: str) -> Dict[str, List[float]]:
    """
    è§£æè®­ç»ƒæ—¥å¿—æ–‡ä»¶
    
    Args:
        log_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
        
    Returns:
        åŒ…å«å„æŒ‡æ ‡æ•°æ®çš„å­—å…¸ï¼Œæ ¼å¼: {'step': [...], 'loss': [...], 'mae': [...], ...}
    """
    data = {
        'step': [],
        'loss': [],
        'contrastive': [],
        'l2': [],
        'mae': [],
        'detect': [],
        'clean_acc': [],
        'robust_acc': [],
        'feat_diff': []
    }
    
    with open(log_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ¯ä¸ªæ­¥éª¤å—
    # åŒ¹é…æ ¼å¼: [x.x%] Step N/M ... Loss: ... CleanAcc: ... RobustAcc: ... FeatDiff: ...
    pattern = r'\[(\d+\.\d+)%\]\s+Step\s+(\d+)/(\d+).*?Loss:\s+([\d.]+).*?Contrastive:\s+([\d.]+).*?L2:\s+([\d.]+).*?MAE:\s+([\d.]+).*?Detect:\s+([\d.]+).*?CleanAcc:\s+([\d.]+).*?RobustAcc:\s+([\d.]+).*?FeatDiff:\s+([\d.]+)'
    
    matches = re.finditer(pattern, content, re.DOTALL)
    
    for match in matches:
        try:
            step = int(match.group(2))
            loss = float(match.group(4))
            contrastive = float(match.group(5))
            l2 = float(match.group(6))
            mae = float(match.group(7))
            detect = float(match.group(8))
            clean_acc = float(match.group(9))
            robust_acc = float(match.group(10))
            feat_diff = float(match.group(11))
            
            data['step'].append(step)
            data['loss'].append(loss)
            data['contrastive'].append(contrastive)
            data['l2'].append(l2)
            data['mae'].append(mae)
            data['detect'].append(detect)
            data['clean_acc'].append(clean_acc)
            data['robust_acc'].append(robust_acc)
            data['feat_diff'].append(feat_diff)
        except (ValueError, IndexError) as e:
            print(f"Warning: Failed to parse step {match.group(2)}: {e}")
            continue
    
    return data


def plot_training_curves(
    training_logs: Dict[str, Dict[str, List[float]]],
    baseline_models: Dict[str, Dict[str, float]] = None,
    output_dir: str = None
):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    
    Args:
        training_logs: è®­ç»ƒæ—¥å¿—æ•°æ®ï¼Œæ ¼å¼: {æ¨¡å‹åç§°: æ•°æ®å­—å…¸}
        baseline_models: åŸºå‡†æ¨¡å‹æ•°æ®ï¼Œæ ¼å¼: {æ¨¡å‹åç§°: {'clean_acc': x, 'robust_acc': y}}
        output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    """
    if output_dir is None:
        output_dir = "."
    os.makedirs(output_dir, exist_ok=True)
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“å’Œå›¾è¡¨æ ·å¼
    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    plt.style.use('seaborn-v0_8-darkgrid')
    
    # åˆ›å»º6ä¸ªå­å›¾: Loss, CleanAcc, RobustAcc, FeatDiff, MAE, Loss Components
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    fig.suptitle('Training Metrics Comparison (KeyToken Enhanced)', fontsize=16, fontweight='bold')
    
    # å®šä¹‰é¢œè‰²
    colors = plt.cm.tab10(np.linspace(0, 1, len(training_logs) + (len(baseline_models) if baseline_models else 0)))
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    for idx, (model_name, data) in enumerate(training_logs.items()):
        steps = data['step']
        color = colors[idx]
        
        # Loss
        if not all(np.isnan(data['loss'])):
            axes[0, 0].plot(steps, data['loss'], label=model_name, color=color, linewidth=2, marker='o', markersize=3)
            # æ ‡æ³¨æœ€åä¸€ä¸ªç‚¹çš„æ•°å€¼
            if steps:
                last_val = data['loss'][-1]
                if not np.isnan(last_val):
                    axes[0, 0].annotate(f"{last_val:.2f}", 
                                       xy=(steps[-1], last_val),
                                       xytext=(5, 5), textcoords='offset points',
                                       fontsize=8, color=color, fontweight='bold')
        
        # FeatDiff (ç§»åˆ°[0,1]ä½ç½®)
        if not all(np.isnan(data['feat_diff'])):
            axes[0, 1].plot(steps, data['feat_diff'], label=model_name, color=color, linewidth=2, marker='o', markersize=3)
            # æ ‡æ³¨æœ€åä¸€ä¸ªç‚¹çš„æ•°å€¼
            if steps:
                last_val = data['feat_diff'][-1]
                if not np.isnan(last_val):
                    axes[0, 1].annotate(f"{last_val:.4f}", 
                                       xy=(steps[-1], last_val),
                                       xytext=(5, 5), textcoords='offset points',
                                       fontsize=8, color=color, fontweight='bold')
        
        # CleanAcc
        axes[0, 2].plot(steps, data['clean_acc'], label=model_name, color=color, linewidth=2, marker='o', markersize=3)
        # æ ‡æ³¨æœ€åä¸€ä¸ªç‚¹çš„æ•°å€¼
        if steps:
            last_val = data['clean_acc'][-1]
            axes[0, 2].annotate(f"{last_val:.4f}", 
                               xy=(steps[-1], last_val),
                               xytext=(5, 5), textcoords='offset points',
                               fontsize=8, color=color, fontweight='bold')
        
        # RobustAcc
        axes[1, 0].plot(steps, data['robust_acc'], label=model_name, color=color, linewidth=2, marker='o', markersize=3)
        # æ ‡æ³¨æœ€åä¸€ä¸ªç‚¹çš„æ•°å€¼
        if steps:
            last_val = data['robust_acc'][-1]
            axes[1, 0].annotate(f"{last_val:.4f}", 
                               xy=(steps[-1], last_val),
                               xytext=(5, 5), textcoords='offset points',
                               fontsize=8, color=color, fontweight='bold')
        
        # MAE (ç§»åˆ°[1,1]ä½ç½®)
        if not all(np.isnan(data['mae'])):
            axes[1, 1].plot(steps, data['mae'], label=model_name, color=color, linewidth=2, marker='o', markersize=3)
            # æ ‡æ³¨æœ€åä¸€ä¸ªç‚¹çš„æ•°å€¼
            if steps:
                last_val = data['mae'][-1]
                if not np.isnan(last_val):
                    axes[1, 1].annotate(f"{last_val:.4f}", 
                                       xy=(steps[-1], last_val),
                                       xytext=(5, 5), textcoords='offset points',
                                       fontsize=8, color=color, fontweight='bold')
        
        # Loss Components (æ–°å¢[1,2]ä½ç½®ï¼šæŸå¤±åˆ†è§£)
        if not all(np.isnan(data['contrastive'])):
            axes[1, 2].plot(steps, data['contrastive'], label=f'{model_name} - Contrastive', 
                           color=color, linewidth=2, marker='o', markersize=2, linestyle='-')
        if not all(np.isnan(data['l2'])):
            axes[1, 2].plot(steps, data['l2'], label=f'{model_name} - L2 Robust', 
                           color=color, linewidth=2, marker='s', markersize=2, linestyle='--')
        if not all(np.isnan(data['mae'])):
            axes[1, 2].plot(steps, data['mae'], label=f'{model_name} - MAE', 
                           color=color, linewidth=2, marker='^', markersize=2, linestyle='-.')
        if not all(np.isnan(data['detect'])):
            axes[1, 2].plot(steps, data['detect'], label=f'{model_name} - Detect', 
                           color=color, linewidth=2, marker='d', markersize=2, linestyle=':')
    
    # æ·»åŠ åŸºå‡†æ¨¡å‹ï¼ˆæ•£ç‚¹ï¼Œæ˜¾ç¤ºåœ¨xè½´èµ·ç‚¹ä½ç½®ï¼‰
    if baseline_models:
        for idx, (model_name, metrics) in enumerate(baseline_models.items()):
            color = colors[len(training_logs) + idx]
            
            # è·å–æ‰€æœ‰è®­ç»ƒæ—¥å¿—çš„stepèŒƒå›´
            all_steps = []
            for data in training_logs.values():
                all_steps.extend(data['step'])
            if all_steps:
                # åŸºå‡†ç‚¹æ˜¾ç¤ºåœ¨ç¬¬ä¸€ä¸ªstepä½ç½®
                baseline_x = min(all_steps)
                
                # æ ‡æ³¨ä½ç½®é”™å¼€ï¼Œé¿å…é‡å 
                offset_y = 15 + idx * 25  # æ¯ä¸ªåŸºå‡†æ¨¡å‹é”™å¼€25ä¸ªç‚¹
                
                # CleanAccåŸºå‡†ç‚¹
                if 'clean_acc' in metrics:
                    axes[0, 2].scatter(baseline_x, metrics['clean_acc'], color=color, 
                                      s=200, marker='*', edgecolors='black', linewidth=1.5,
                                      label=f'{model_name}', zorder=10)
                    # æ·»åŠ æ•°å€¼æ ‡æ³¨
                    axes[0, 2].annotate(f"{metrics['clean_acc']:.4f}", 
                                       xy=(baseline_x, metrics['clean_acc']),
                                       xytext=(10, offset_y), textcoords='offset points',
                                       fontsize=9, fontweight='bold',
                                       bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.3),
                                       arrowprops=dict(arrowstyle='->', color=color, lw=1.5))
                
                # RobustAccåŸºå‡†ç‚¹
                if 'robust_acc' in metrics:
                    axes[1, 0].scatter(baseline_x, metrics['robust_acc'], color=color, 
                                      s=200, marker='*', edgecolors='black', linewidth=1.5,
                                      label=f'{model_name}', zorder=10)
                    # æ·»åŠ æ•°å€¼æ ‡æ³¨
                    axes[1, 0].annotate(f"{metrics['robust_acc']:.4f}", 
                                       xy=(baseline_x, metrics['robust_acc']),
                                       xytext=(10, offset_y), textcoords='offset points',
                                       fontsize=9, fontweight='bold',
                                       bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.3),
                                       arrowprops=dict(arrowstyle='->', color=color, lw=1.5))
    
    # è®¾ç½®å­å›¾æ ‡é¢˜å’Œæ ‡ç­¾
    axes[0, 0].set_title('Total Training Loss', fontsize=12, fontweight='bold')
    axes[0, 0].set_xlabel('Step')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].legend(loc='best')
    axes[0, 0].grid(True, alpha=0.3)
    
    axes[0, 1].set_title('Feature Difference (FeatDiff)', fontsize=12, fontweight='bold')
    axes[0, 1].set_xlabel('Step')
    axes[0, 1].set_ylabel('FeatDiff')
    axes[0, 1].legend(loc='best')
    axes[0, 1].grid(True, alpha=0.3)
    
    axes[0, 2].set_title('Clean Accuracy', fontsize=12, fontweight='bold')
    axes[0, 2].set_xlabel('Step')
    axes[0, 2].set_ylabel('Accuracy')
    axes[0, 2].legend(loc='best')
    axes[0, 2].grid(True, alpha=0.3)
    axes[0, 2].set_ylim([0, 1.0])
    
    axes[1, 0].set_title('Robust Accuracy', fontsize=12, fontweight='bold')
    axes[1, 0].set_xlabel('Step')
    axes[1, 0].set_ylabel('Accuracy')
    axes[1, 0].legend(loc='best')
    axes[1, 0].grid(True, alpha=0.3)
    axes[1, 0].set_ylim([0, 1.0])  # è®¾ç½®ä¸º[0, 1.0]ä»¥æ˜¾ç¤ºé²æ£’æ¨¡å‹çš„é«˜RobustAcc
    
    axes[1, 1].set_title('MAE Reconstruction Loss', fontsize=12, fontweight='bold')
    axes[1, 1].set_xlabel('Step')
    axes[1, 1].set_ylabel('MAE')
    axes[1, 1].legend(loc='best')
    axes[1, 1].grid(True, alpha=0.3)
    
    # æŸå¤±åˆ†è§£å­å›¾ï¼ˆå³ä¸‹è§’ï¼‰
    axes[1, 2].set_title('Loss Components Breakdown', fontsize=12, fontweight='bold')
    axes[1, 2].set_xlabel('Step')
    axes[1, 2].set_ylabel('Loss Value')
    axes[1, 2].legend(loc='best', fontsize=8)
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    output_path = os.path.join(output_dir, 'training_comparison.png')
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    
    plt.show()


def main():
    parser = argparse.ArgumentParser(
        description='è®­ç»ƒæ—¥å¿—å¯è§†åŒ–å·¥å…·',
        epilog='ç¤ºä¾‹: python3 tools/visualize_training.py --log_dirs output/stage1_freeze_all '
               '--baseline_clip_clean_acc 0.726 --baseline_clip_robust_acc 0.0 '
               '--fare_eps4_clean_acc 0.692 --fare_eps4_robust_acc 0.005')
    parser.add_argument('--log_dirs', nargs='+', required=True,
                       help='è®­ç»ƒæ—¥å¿—ç›®å½•åˆ—è¡¨ï¼Œä¾‹å¦‚: output/stage1_freeze_all output/stage2_unfreeze_6')
    parser.add_argument('--baseline_clip_clean_acc', type=float, default=None,
                       help='åŸå§‹CLIPçš„CleanAcc (å¿…é¡»æä¾›çœŸå®è¯„ä¼°æ•°æ®)')
    parser.add_argument('--baseline_clip_robust_acc', type=float, default=None,
                       help='åŸå§‹CLIPçš„RobustAcc (å¿…é¡»æä¾›çœŸå®è¯„ä¼°æ•°æ®)')
    parser.add_argument('--fare_eps4_clean_acc', type=float, default=None,
                       help='FARE eps=4æ¨¡å‹çš„CleanAcc (å¿…é¡»æä¾›çœŸå®è¯„ä¼°æ•°æ®)')
    parser.add_argument('--fare_eps4_robust_acc', type=float, default=None,
                       help='FARE eps=4æ¨¡å‹çš„RobustAcc (å¿…é¡»æä¾›çœŸå®è¯„ä¼°æ•°æ®)')
    parser.add_argument('--fare_eps2_clean_acc', type=float, default=None,
                       help='FARE eps=2æ¨¡å‹çš„CleanAcc (å¿…é¡»æä¾›çœŸå®è¯„ä¼°æ•°æ®)')
    parser.add_argument('--fare_eps2_robust_acc', type=float, default=None,
                       help='FARE eps=2æ¨¡å‹çš„RobustAcc (å¿…é¡»æä¾›çœŸå®è¯„ä¼°æ•°æ®)')
    parser.add_argument('--output_dir', default='output/visualizations',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: output/visualizations)')
    
    args = parser.parse_args()
    
    # è§£ææ‰€æœ‰è®­ç»ƒæ—¥å¿—
    training_logs = {}
    for log_dir in args.log_dirs:
        log_path = os.path.join(log_dir, 'train.log')
        if os.path.exists(log_path):
            model_name = os.path.basename(log_dir)
            print(f"ğŸ“Š è§£ææ—¥å¿—: {log_path}")
            data = parse_log_file(log_path)
            if data['step']:
                training_logs[model_name] = data
                print(f"   âœ“ æˆåŠŸæå– {len(data['step'])} ä¸ªæ•°æ®ç‚¹")
            else:
                print(f"   âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
        else:
            print(f"âš ï¸  è­¦å‘Š: æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_path}")
    
    if not training_logs:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„è®­ç»ƒæ—¥å¿—")
        return
    
    # å®šä¹‰åŸºå‡†æ¨¡å‹ï¼ˆåªåœ¨æä¾›äº†æ•°æ®æ—¶æ‰æ·»åŠ ï¼‰
    baseline_models = {}
    
    # OpenAI CLIP
    if args.baseline_clip_clean_acc is not None or args.baseline_clip_robust_acc is not None:
        baseline_models['OpenAI CLIP (Baseline)'] = {}
        if args.baseline_clip_clean_acc is not None:
            baseline_models['OpenAI CLIP (Baseline)']['clean_acc'] = args.baseline_clip_clean_acc
        if args.baseline_clip_robust_acc is not None:
            baseline_models['OpenAI CLIP (Baseline)']['robust_acc'] = args.baseline_clip_robust_acc
    
    # FARE eps=4
    if args.fare_eps4_clean_acc is not None or args.fare_eps4_robust_acc is not None:
        baseline_models['FARE (eps=4)'] = {}
        if args.fare_eps4_clean_acc is not None:
            baseline_models['FARE (eps=4)']['clean_acc'] = args.fare_eps4_clean_acc
        if args.fare_eps4_robust_acc is not None:
            baseline_models['FARE (eps=4)']['robust_acc'] = args.fare_eps4_robust_acc
    
    # FARE eps=2
    if args.fare_eps2_clean_acc is not None or args.fare_eps2_robust_acc is not None:
        baseline_models['FARE (eps=2)'] = {}
        if args.fare_eps2_clean_acc is not None:
            baseline_models['FARE (eps=2)']['clean_acc'] = args.fare_eps2_clean_acc
        if args.fare_eps2_robust_acc is not None:
            baseline_models['FARE (eps=2)']['robust_acc'] = args.fare_eps2_robust_acc
    
    print(f"\nğŸ“ˆ ç»˜åˆ¶è®­ç»ƒæ›²çº¿...")
    print(f"   è®­ç»ƒæ¨¡å‹: {', '.join(training_logs.keys())}")
    print(f"   åŸºå‡†æ¨¡å‹: {', '.join(baseline_models.keys())}")
    
    # ç»˜åˆ¶å›¾è¡¨
    plot_training_curves(training_logs, baseline_models, args.output_dir)


if __name__ == '__main__':
    main()
