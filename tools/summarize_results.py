#!/usr/bin/env python3
"""
ç»“æœæ±‡æ€»è„šæœ¬ - ç”Ÿæˆå¤šä»»åŠ¡è¯„ä¼°çš„æ±‡æ€»æŠ¥å‘Š
"""

import os
import argparse
import json
import glob
from datetime import datetime
from collections import defaultdict


def load_results(input_dir):
    """åŠ è½½æ‰€æœ‰è¯„ä¼°ç»“æœ"""
    results = defaultdict(lambda: defaultdict(dict))
    
    # éå†æ‰€æœ‰å­ç›®å½•
    for task_dir in ['zeroshot', 'vqa', 'caption', 'pope']:
        task_path = os.path.join(input_dir, task_dir)
        if not os.path.exists(task_path):
            continue
        
        # æŸ¥æ‰¾æ‰€æœ‰ç»“æœæ–‡ä»¶
        result_files = glob.glob(os.path.join(task_path, '*_results.json'))
        
        for result_file in result_files:
            with open(result_file, 'r') as f:
                data = json.load(f)
            
            # æå–ä¿¡æ¯
            model_path = data.get('model', '')
            model_name = os.path.basename(model_path).replace('.pt', '')
            
            if 'dataset' in data:
                dataset = data['dataset']
            elif 'split' in data:
                dataset = f"pope_{data['split']}"
            else:
                continue
            
            # å­˜å‚¨ç»“æœ
            results[model_name][task_dir][dataset] = data
    
    return results


def generate_markdown_table(results):
    """ç”ŸæˆMarkdownæ ¼å¼çš„ç»“æœè¡¨æ ¼"""
    md = []
    
    md.append("# å¤šä»»åŠ¡é²æ£’æ€§è¯„ä¼°æŠ¥å‘Š\n")
    md.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    md.append("---\n")
    
    # é›¶æ ·æœ¬åˆ†ç±»ç»“æœ
    md.append("## é›¶æ ·æœ¬åˆ†ç±» (Zero-Shot Classification)\n")
    md.append("| æ¨¡å‹ | æ•°æ®é›† | Eps | Gray-box | Clean Acc | Robust Acc | ç›¸å¯¹ä¸‹é™ |\n")
    md.append("|------|--------|-----|----------|-----------|------------|----------|\n")
    
    for model_name in sorted(results.keys()):
        if 'zeroshot' in results[model_name]:
            for dataset, data in sorted(results[model_name]['zeroshot'].items()):
                clean_acc = data.get('clean_acc', 0) * 100
                robust_acc = data.get('robust_acc', 0) * 100
                eps = data.get('eps', 0)
                gray_box = "âœ“" if data.get('gray_box', False) else ""
                drop = (clean_acc - robust_acc) / clean_acc * 100 if clean_acc > 0 else 0
                
                md.append(f"| {model_name} | {dataset} | {eps}/255 | {gray_box} | "
                         f"{clean_acc:.2f}% | {robust_acc:.2f}% | {drop:.1f}% |\n")
    
    # VQAç»“æœ
    md.append("\n## è§†è§‰é—®ç­” (VQA)\n")
    md.append("| æ¨¡å‹ | æ•°æ®é›† | Eps | Gray-box | Clean Acc | Robust Acc | ç›¸å¯¹ä¸‹é™ |\n")
    md.append("|------|--------|-----|----------|-----------|------------|----------|\n")
    
    for model_name in sorted(results.keys()):
        if 'vqa' in results[model_name]:
            for dataset, data in sorted(results[model_name]['vqa'].items()):
                clean_acc = data.get('clean_acc', 0) * 100
                robust_acc = data.get('robust_acc', 0) * 100
                eps = data.get('eps', 0)
                gray_box = "âœ“" if data.get('gray_box', False) else ""
                drop = (clean_acc - robust_acc) / clean_acc * 100 if clean_acc > 0 else 0
                
                md.append(f"| {model_name} | {dataset} | {eps}/255 | {gray_box} | "
                         f"{clean_acc:.2f}% | {robust_acc:.2f}% | {drop:.1f}% |\n")
    
    # Captionç»“æœ
    md.append("\n## å›¾åƒæè¿° (Caption)\n")
    md.append("| æ¨¡å‹ | æ•°æ®é›† | Eps | Gray-box | Clean CIDEr | Robust CIDEr | ç›¸å¯¹ä¸‹é™ |\n")
    md.append("|------|--------|-----|----------|-------------|--------------|----------|\n")
    
    for model_name in sorted(results.keys()):
        if 'caption' in results[model_name]:
            for dataset, data in sorted(results[model_name]['caption'].items()):
                clean_cider = data.get('clean_cider', 0) * 100
                robust_cider = data.get('robust_cider', 0) * 100
                eps = data.get('eps', 0)
                gray_box = "âœ“" if data.get('gray_box', False) else ""
                drop = (clean_cider - robust_cider) / clean_cider * 100 if clean_cider > 0 else 0
                
                md.append(f"| {model_name} | {dataset} | {eps}/255 | {gray_box} | "
                         f"{clean_cider:.2f} | {robust_cider:.2f} | {drop:.1f}% |\n")
    
    # POPEç»“æœ
    md.append("\n## å¹»è§‰è¯„ä¼° (POPE)\n")
    md.append("| æ¨¡å‹ | Split | Accuracy | Precision | Recall | F1 | Hallucination Rate |\n")
    md.append("|------|-------|----------|-----------|--------|----|-----------------|\n")
    
    for model_name in sorted(results.keys()):
        if 'pope' in results[model_name]:
            for dataset, data in sorted(results[model_name]['pope'].items()):
                metrics = data.get('metrics', {})
                accuracy = metrics.get('accuracy', 0) * 100
                precision = metrics.get('precision', 0) * 100
                recall = metrics.get('recall', 0) * 100
                f1 = metrics.get('f1', 0) * 100
                hall_rate = metrics.get('hallucination_rate', 0) * 100
                split = dataset.replace('pope_', '')
                
                md.append(f"| {model_name} | {split} | {accuracy:.2f}% | {precision:.2f}% | "
                         f"{recall:.2f}% | {f1:.2f}% | {hall_rate:.2f}% |\n")
    
    return ''.join(md)


def generate_comparison_summary(results):
    """ç”Ÿæˆå¯¹æ¯”æ±‡æ€»"""
    summary = {
        'models': list(results.keys()),
        'tasks': {},
        'timestamp': datetime.now().isoformat()
    }
    
    # å¯¹æ¯”å„ä»»åŠ¡çš„å¹³å‡æ€§èƒ½
    for task in ['zeroshot', 'vqa', 'caption', 'pope']:
        task_summary = {}
        
        for model_name in results.keys():
            if task not in results[model_name]:
                continue
            
            if task == 'pope':
                # POPE: å¹³å‡accuracyå’Œå¹»è§‰ç‡
                accuracies = []
                hall_rates = []
                for data in results[model_name][task].values():
                    metrics = data.get('metrics', {})
                    accuracies.append(metrics.get('accuracy', 0))
                    hall_rates.append(metrics.get('hallucination_rate', 0))
                
                task_summary[model_name] = {
                    'avg_accuracy': sum(accuracies) / len(accuracies) if accuracies else 0,
                    'avg_hallucination_rate': sum(hall_rates) / len(hall_rates) if hall_rates else 0
                }
            else:
                # å…¶ä»–ä»»åŠ¡: å¹³å‡cleanå’Œrobustæ€§èƒ½
                clean_scores = []
                robust_scores = []
                
                for data in results[model_name][task].values():
                    if task == 'caption':
                        clean_scores.append(data.get('clean_cider', 0))
                        robust_scores.append(data.get('robust_cider', 0))
                    else:
                        clean_scores.append(data.get('clean_acc', 0))
                        robust_scores.append(data.get('robust_acc', 0))
                
                task_summary[model_name] = {
                    'avg_clean': sum(clean_scores) / len(clean_scores) if clean_scores else 0,
                    'avg_robust': sum(robust_scores) / len(robust_scores) if robust_scores else 0
                }
        
        summary['tasks'][task] = task_summary
    
    return summary


def main():
    parser = argparse.ArgumentParser(description='æ±‡æ€»å¤šä»»åŠ¡è¯„ä¼°ç»“æœ')
    parser.add_argument('--input_dir', type=str, required=True,
                       help='è¯„ä¼°ç»“æœç›®å½•')
    parser.add_argument('--output_file', type=str, default='summary_report.json',
                       help='è¾“å‡ºJSONæ–‡ä»¶')
    
    args = parser.parse_args()
    
    print("ğŸ”„ åŠ è½½è¯„ä¼°ç»“æœ...")
    results = load_results(args.input_dir)
    
    if not results:
        print("âŒ æœªæ‰¾åˆ°è¯„ä¼°ç»“æœ")
        return
    
    print(f"   âœ“ æ‰¾åˆ° {len(results)} ä¸ªæ¨¡å‹çš„ç»“æœ")
    
    # ç”Ÿæˆå¯¹æ¯”æ±‡æ€»
    print("ğŸ”„ ç”Ÿæˆå¯¹æ¯”æ±‡æ€»...")
    summary = generate_comparison_summary(results)
    
    # ä¿å­˜JSON
    with open(args.output_file, 'w') as f:
        json.dump(summary, f, indent=2)
    print(f"   âœ“ å·²ä¿å­˜: {args.output_file}")
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_file = args.output_file.replace('.json', '.md')
    md_content = generate_markdown_table(results)
    with open(md_file, 'w') as f:
        f.write(md_content)
    print(f"   âœ“ å·²ä¿å­˜: {md_file}")
    
    # æ‰“å°ç®€è¦æ±‡æ€»
    print("\n" + "=" * 80)
    print("ğŸ“Š è¯„ä¼°æ±‡æ€»")
    print("=" * 80)
    
    for task, task_summary in summary['tasks'].items():
        print(f"\nã€{task.upper()}ã€‘")
        for model_name, metrics in task_summary.items():
            print(f"  {model_name}:")
            for metric, value in metrics.items():
                print(f"    {metric}: {value:.4f}")
    
    print("\n" + "=" * 80)
    print("âœ… æ±‡æ€»å®Œæˆ!")
    print("=" * 80)


if __name__ == '__main__':
    main()
