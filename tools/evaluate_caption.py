#!/usr/bin/env python3
"""
Image Captioningé²æ£’æ€§è¯„ä¼° - åŸºäºFAREè®ºæ–‡è®¾ç½®
æ”¯æŒCOCOå’ŒFlickr30kæ•°æ®é›†
æ”»å‡»pipeline: APGDåŠç²¾åº¦ -> å•ç²¾åº¦æ”»å‡»
è¯„ä¼°æŒ‡æ ‡: CIDEr score
Eps: 2/255 and 4/255
"""

import os
import sys
import argparse
import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime
from PIL import Image
from pycocotools.coco import COCO
from pycocoevalcap.eval import COCOEvalCap

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from train.adversarial_training_clip_enhanced import EnhancedClipVisionModel
from CLIP_eval.eval_utils import load_clip_model as load_baseline_clip_model
from CLIP_eval.eval_utils_enhanced import load_enhanced_clip_model
from autoattack import AutoAttack
import open_clip


def load_caption_dataset(dataset_name, dataset_root, split='val', max_samples=500):
    """åŠ è½½Captionæ•°æ®é›†"""
    print(f"ğŸ”„ åŠ è½½{dataset_name}æ•°æ®é›†...")
    
    if dataset_name == 'coco':
        # COCO Captions
        ann_file = os.path.join(dataset_root, 'annotations', f'captions_{split}2014.json')
        images_dir = os.path.join(dataset_root, f'{split}2014')
        
        coco = COCO(ann_file)
        img_ids = coco.getImgIds()
        
        # éšæœºé‡‡æ ·
        if max_samples > 0 and len(img_ids) > max_samples:
            img_ids = np.random.choice(img_ids, max_samples, replace=False).tolist()
        
        samples = []
        for img_id in img_ids:
            img_info = coco.loadImgs(img_id)[0]
            ann_ids = coco.getAnnIds(imgIds=img_id)
            anns = coco.loadAnns(ann_ids)
            
            captions = [ann['caption'] for ann in anns]
            image_path = os.path.join(images_dir, img_info['file_name'])
            
            samples.append({
                'image_id': img_id,
                'image_path': image_path,
                'captions': captions
            })
    
    elif dataset_name == 'flickr30k':
        # Flickr30k
        ann_file = os.path.join(dataset_root, 'results_20130124.token')
        images_dir = os.path.join(dataset_root, 'flickr30k_images')
        
        # è§£æannotationæ–‡ä»¶
        image_captions = {}
        with open(ann_file, 'r') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) < 2:
                    continue
                img_id = parts[0].split('#')[0]
                caption = parts[1]
                
                if img_id not in image_captions:
                    image_captions[img_id] = []
                image_captions[img_id].append(caption)
        
        # æ„å»ºæ ·æœ¬
        samples = []
        img_ids = list(image_captions.keys())
        
        if max_samples > 0 and len(img_ids) > max_samples:
            img_ids = np.random.choice(img_ids, max_samples, replace=False).tolist()
        
        for img_id in img_ids:
            image_path = os.path.join(images_dir, img_id)
            if os.path.exists(image_path):
                samples.append({
                    'image_id': img_id,
                    'image_path': image_path,
                    'captions': image_captions[img_id]
                })
    
    print(f"   âœ“ åŠ è½½ {len(samples)} ä¸ªæ ·æœ¬")
    return samples


class SimpleCaptionModel(torch.nn.Module):
    """ç®€åŒ–çš„Captionæ¨¡å‹ï¼ˆCLIP-basedï¼‰"""
    def __init__(self, vision_model, text_model, tokenizer, vocab, 
                 is_enhanced=False, mode='eval', gray_box=False):
        super().__init__()
        self.vision_model = vision_model
        self.text_model = text_model
        self.tokenizer = tokenizer
        self.vocab = vocab
        self.is_enhanced = is_enhanced
        self.mode = mode
        self.gray_box = gray_box
        
        # è¯æ±‡åµŒå…¥
        with torch.no_grad():
            word_tokens = tokenizer(vocab)
            self.word_embeddings = F.normalize(
                text_model(word_tokens.to(next(text_model.parameters()).device)),
                dim=-1
            )
    
    def forward(self, image):
        """è¿”å›å¯¹è¯æ±‡è¡¨çš„logits"""
        # å›¾åƒç¼–ç 
        if self.is_enhanced:
            if self.gray_box:
                image_emb, _, _, _, _, _ = self.vision_model(image, mode='attack')
            else:
                image_emb, _, _, _, _, _ = self.vision_model(image, mode=self.mode)
        else:
            # å¤„ç†DataParallelåŒ…è£…
            if isinstance(self.vision_model, torch.nn.DataParallel):
                image_emb = self.vision_model.module.encode_image(image)
            else:
                image_emb = self.vision_model.encode_image(image)
        
        image_emb = F.normalize(image_emb, dim=-1)
        
        # è®¡ç®—ä¸è¯æ±‡çš„ç›¸ä¼¼åº¦
        logits = 100.0 * image_emb @ self.word_embeddings.T
        
        return logits
    
    def generate_caption(self, image, max_length=20, beam_size=3):
        """ç”Ÿæˆcaptionï¼ˆbeam searchï¼‰"""
        device = image.device
        batch_size = image.size(0)
        
        # ç¼–ç å›¾åƒ
        with torch.no_grad():
            logits = self.forward(image)
            
            # ç®€å•greedy decoding (å®é™…åº”è¯¥ç”¨beam search)
            top_k = 10
            _, top_indices = logits.topk(top_k, dim=-1)
            
            # é€‰æ‹©top-kè¯æ±‡ç»„æˆcaption
            caption_words = []
            for i in range(min(max_length, top_k)):
                word_idx = top_indices[0, i].item()
                caption_words.append(self.vocab[word_idx])
            
            caption = ' '.join(caption_words)
        
        return caption


def build_vocab_from_captions(samples, max_vocab_size=5000):
    """ä»captionsæ„å»ºè¯æ±‡è¡¨"""
    from collections import Counter
    
    word_counts = Counter()
    for sample in samples:
        for caption in sample['captions']:
            words = caption.lower().split()
            word_counts.update(words)
    
    # å–æœ€å¸¸è§çš„è¯
    most_common = word_counts.most_common(max_vocab_size)
    vocab = [word for word, _ in most_common]
    
    print(f"   âœ“ è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
    return vocab


def attack_pipeline_caption(caption_model, image, target_caption_emb, eps, device):
    """
    FAREæ”»å‡»pipeline for Caption:
    1. APGDåŠç²¾åº¦100 iter
    2. æ£€æŸ¥é˜ˆå€¼
    3. APGDå•ç²¾åº¦æ”»å‡»
    """
    threshold = 0.3  # CIDEré˜ˆå€¼
    
    # åˆ›å»ºæ”»å‡»ç›®æ ‡ï¼ˆæœ€å°åŒ–ä¸ç›®æ ‡captionçš„ç›¸ä¼¼åº¦ï¼‰
    class CaptionLoss(torch.nn.Module):
        def __init__(self, model, target_emb):
            super().__init__()
            self.model = model
            self.target_emb = target_emb
        
        def forward(self, x):
            # è¿”å›ä¸ç›®æ ‡çš„è´Ÿç›¸ä¼¼åº¦ï¼ˆæœ€å¤§åŒ–è·ç¦»ï¼‰
            logits = self.model(x)
            # ç®€åŒ–ï¼šç›´æ¥ç”¨logitsä½œä¸ºembedding proxy
            return -F.cosine_similarity(logits.mean(dim=-1, keepdim=True), 
                                       self.target_emb, dim=-1)
    
    loss_fn = CaptionLoss(caption_model, target_caption_emb)
    
    # é˜¶æ®µ1: åŠç²¾åº¦APGD
    adversary_half = AutoAttack(
        caption_model,
        norm='Linf',
        eps=eps,
        version='custom',
        verbose=False
    )
    adversary_half.attacks_to_run = ['apgd-ce']
    adversary_half.apgd.n_iter = 100
    
    # ä½¿ç”¨dummy label
    dummy_label = torch.zeros(image.size(0), dtype=torch.long).to(device)
    
    with torch.cuda.amp.autocast():
        adv_image = adversary_half.run_standard_evaluation(
            image, dummy_label, bs=image.size(0)
        )
    
    # é˜¶æ®µ2: å•ç²¾åº¦æ”»å‡»
    adversary_full = AutoAttack(
        caption_model,
        norm='Linf',
        eps=eps,
        version='custom',
        verbose=False
    )
    adversary_full.attacks_to_run = ['apgd-ce']
    adversary_full.apgd.n_iter = 100
    
    with torch.enable_grad():
        adv_image = adversary_full.run_standard_evaluation(
            adv_image, dummy_label, bs=image.size(0)
        )
    
    return adv_image


def compute_cider_score(pred_captions, gt_captions_dict):
    """è®¡ç®—CIDEr score"""
    from pycocoevalcap.cider.cider import Cider
    
    # æ ¼å¼è½¬æ¢
    gts = {}
    res = {}
    for i, (img_id, pred_caption) in enumerate(pred_captions):
        res[i] = [pred_caption]
        gts[i] = gt_captions_dict[img_id]
    
    # è®¡ç®—CIDEr
    cider_scorer = Cider()
    score, _ = cider_scorer.compute_score(gts, res)
    
    return score


def evaluate_caption(model, tokenizer, samples, vocab, eps, device,
                    preprocess, normalizer, is_enhanced=False, mode='eval', gray_box=False):
    """è¯„ä¼°Captionç”Ÿæˆ"""
    print(f"ğŸ”„ å¼€å§‹Captionè¯„ä¼° (eps={eps})...")
    
    # åˆ›å»ºCaptionæ¨¡å‹ - å¤„ç†DataParallelåŒ…è£…
    base_model = model.module if isinstance(model, torch.nn.DataParallel) else model
    if is_enhanced:
        text_model = base_model.model.encode_text
    else:
        text_model = base_model.encode_text
    
    caption_model = SimpleCaptionModel(
        model, text_model, tokenizer, vocab,
        is_enhanced, mode, gray_box
    ).to(device)
    caption_model.eval()
    
    clean_captions = []
    robust_captions = []
    gt_captions_dict = {}
    
    for sample in tqdm(samples, desc="Caption eval"):
        # åŠ è½½å›¾åƒ
        image = Image.open(sample['image_path']).convert('RGB')
        image_tensor = preprocess(image).unsqueeze(0).to(device)
        image_tensor = normalizer(image_tensor)  # åº”ç”¨normalize
        
        img_id = sample['image_id']
        gt_captions = sample['captions']
        gt_captions_dict[img_id] = gt_captions
        
        # å¹²å‡€æ ·æœ¬ç”Ÿæˆ
        with torch.no_grad():
            clean_caption = caption_model.generate_caption(image_tensor)
            clean_captions.append((img_id, clean_caption))
        
        # å¯¹æŠ—æ ·æœ¬ç”Ÿæˆ
        # è®¡ç®—ç›®æ ‡caption embedding (ç¬¬ä¸€ä¸ªGT)
        with torch.no_grad():
            gt_tokens = tokenizer([gt_captions[0]]).to(device)
            target_emb = text_model(gt_tokens)
            target_emb = F.normalize(target_emb, dim=-1)
        
        with torch.enable_grad():
            adv_image = attack_pipeline_caption(
                caption_model, image_tensor, target_emb, eps, device
            )
        
        with torch.no_grad():
            robust_caption = caption_model.generate_caption(adv_image)
            robust_captions.append((img_id, robust_caption))
    
    # è®¡ç®—CIDEr scores
    clean_cider = compute_cider_score(clean_captions, gt_captions_dict)
    robust_cider = compute_cider_score(robust_captions, gt_captions_dict)
    
    print(f"   Clean CIDEr: {clean_cider:.4f}")
    print(f"   Robust CIDEr: {robust_cider:.4f}")
    
    return clean_cider, robust_cider


def main():
    parser = argparse.ArgumentParser(description='Captioné²æ£’æ€§è¯„ä¼°')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--checkpoint', type=str, required=True)
    parser.add_argument('--clip_model_name', type=str, default='ViT-L-14',
                       help='CLIPæ¨¡å‹æ¶æ„')
    parser.add_argument('--dataset', type=str, default='coco',
                       choices=['coco', 'flickr30k'])
    parser.add_argument('--dataset_root', type=str, required=True)
    
    # æ”»å‡»é…ç½®
    parser.add_argument('--eps', type=float, default=4.0)
    parser.add_argument('--gray_box', action='store_true')
    
    # è¯„ä¼°é…ç½®
    parser.add_argument('--mode', type=str, default='eval')
    parser.add_argument('--max_samples', type=int, default=500)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--output_dir', type=str, default='output/caption_eval')
    
    args = parser.parse_args()
    
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {args.checkpoint}")
    print(f"ğŸ“¦ CLIPæ¶æ„: {args.clip_model_name}")
    
    if 'fare' in args.checkpoint.lower() or 'tecoa' in args.checkpoint.lower():
        model, preprocess, normalizer = load_baseline_clip_model(
            args.clip_model_name, args.checkpoint
        )
        model = model.to(device)
        tokenizer = open_clip.get_tokenizer(args.clip_model_name)
        is_enhanced = False
    else:
        enhanced_model, preprocess, normalizer = load_enhanced_clip_model(
            args.clip_model_name, args.checkpoint
        )
        model = enhanced_model.to(device)
        tokenizer = open_clip.get_tokenizer(args.clip_model_name)
        is_enhanced = True
    
    # å¤šGPUæ”¯æŒ
    num_gpus = torch.cuda.device_count()
    if num_gpus > 1:
        print(f"ğŸ’» ä½¿ç”¨ {num_gpus} å¼ GPU (DataParallel)")
        model = torch.nn.DataParallel(model)
    
    model.eval()
    
    # åŠ è½½æ•°æ®é›†
    samples = load_caption_dataset(
        args.dataset, args.dataset_root, 'val', args.max_samples
    )
    
    # æ„å»ºè¯æ±‡è¡¨
    vocab = build_vocab_from_captions(samples)
    
    # è¯„ä¼°
    print("=" * 80)
    print(f"ğŸ“Š Captioné²æ£’æ€§è¯„ä¼°")
    print(f"   æ•°æ®é›†: {args.dataset}")
    print(f"   æ¨¡å‹: {args.checkpoint}")
    print(f"   Eps: {args.eps}/255")
    print("=" * 80)
    
    eps_normalized = args.eps / 255.0
    clean_cider, robust_cider = evaluate_caption(
        model, tokenizer, samples, vocab, eps_normalized, device,
        preprocess, normalizer, is_enhanced, args.mode, args.gray_box
    )
    
    # ä¿å­˜ç»“æœ
    os.makedirs(args.output_dir, exist_ok=True)
    model_name = os.path.basename(args.checkpoint).replace('.pt', '')
    eps_str = f"eps{int(args.eps)}"
    gray_str = "_graybox" if args.gray_box else ""
    
    result_file = os.path.join(
        args.output_dir,
        f"{model_name}_{args.dataset}_{eps_str}{gray_str}_results.json"
    )
    
    results = {
        'model': args.checkpoint,
        'dataset': args.dataset,
        'mode': args.mode,
        'eps': args.eps,
        'gray_box': args.gray_box,
        'clean_cider': clean_cider,
        'robust_cider': robust_cider,
        'num_samples': len(samples),
        'timestamp': datetime.now().isoformat()
    }
    
    with open(result_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print("=" * 80)
    print(f"âœ… ç»“æœå·²ä¿å­˜: {result_file}")


if __name__ == '__main__':
    main()
