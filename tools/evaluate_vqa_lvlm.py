#!/usr/bin/env python3
"""
å®Œæ•´LVLM VQAè¯„ä¼° - ä½¿ç”¨LLaVAå’ŒOpenFlamingo
éµå¾ªFAREè®ºæ–‡è®¾ç½®
"""

import os
import sys
import argparse
import json
import torch
import torch.nn.functional as F
from tqdm import tqdm
from pathlib import Path
from PIL import Image
import numpy as np
from autoattack import AutoAttack

# LLaVA imports
try:
    from llava.conversation import conv_templates, SeparatorStyle
    from llava.utils import disable_torch_init
    from llava.mm_utils import process_images, tokenizer_image_token
    from llava.constants import IMAGE_TOKEN_INDEX
except ImportError:
    print("è­¦å‘Š: LLaVAæœªå®‰è£…ï¼Œè¯·å®‰è£…llavaåŒ…")

# å¯¼å…¥LVLMå·¥å…·
from lvlm_utils import get_lvlm_model


def load_vqa_dataset(dataset_name: str, dataset_root: str, max_samples: int = -1):
    """åŠ è½½VQAæ•°æ®é›†"""
    if dataset_name == 'vqav2':
        questions_file = os.path.join(dataset_root, 'v2_OpenEnded_mscoco_val2014_questions.json')
        annotations_file = os.path.join(dataset_root, 'v2_mscoco_val2014_annotations.json')
        images_dir = os.path.join(dataset_root, 'val2014')
        
        with open(questions_file) as f:
            questions_data = json.load(f)
        with open(annotations_file) as f:
            annotations_data = json.load(f)
        
        # åˆ›å»ºç­”æ¡ˆæ˜ å°„
        ann_dict = {ann['question_id']: ann for ann in annotations_data['annotations']}
        
        samples = []
        for q in questions_data['questions']:
            qid = q['question_id']
            if qid not in ann_dict:
                continue
            
            img_id = q['image_id']
            img_path = os.path.join(images_dir, f"COCO_val2014_{img_id:012d}.jpg")
            
            if not os.path.exists(img_path):
                continue
            
            answer = ann_dict[qid]['multiple_choice_answer']
            
            samples.append({
                'image_path': img_path,
                'question': q['question'],
                'answer': answer,
                'question_id': qid
            })
    
    elif dataset_name == 'textvqa':
        annotations_file = os.path.join(dataset_root, 'TextVQA_0.5.1_val.json')
        images_dir = os.path.join(dataset_root, 'train_images')
        
        with open(annotations_file) as f:
            data = json.load(f)
        
        samples = []
        for item in data['data']:
            img_path = os.path.join(images_dir, item['image_id'] + '.jpg')
            if not os.path.exists(img_path):
                continue
            
            # ä½¿ç”¨æœ€å¸¸è§çš„ç­”æ¡ˆ
            answers = item['answers']
            answer = max(set(answers), key=answers.count)
            
            samples.append({
                'image_path': img_path,
                'question': item['question'],
                'answer': answer,
                'question_id': item['question_id']
            })
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    # éšæœºé‡‡æ ·
    if max_samples > 0 and len(samples) > max_samples:
        import random
        random.seed(42)
        samples = random.sample(samples, max_samples)
    
    print(f"âœ“ åŠ è½½ {len(samples)} ä¸ªVQAæ ·æœ¬")
    return samples


def llava_generate_answer(model, tokenizer, image_processor, image, question, device='cuda'):
    """ä½¿ç”¨LLaVAç”Ÿæˆç­”æ¡ˆ"""
    disable_torch_init()
    
    # å‡†å¤‡conversation
    conv = conv_templates["llava_v1"].copy()
    
    # æ·»åŠ å›¾åƒtoken
    if model.config.mm_use_im_start_end:
        question = f"<im_start><image><im_end>\n{question}"
    else:
        question = f"<image>\n{question}"
    
    conv.append_message(conv.roles[0], question)
    conv.append_message(conv.roles[1], None)
    prompt = conv.get_prompt()
    
    # å¤„ç†å›¾åƒ
    image_tensor = process_images([image], image_processor, model.config).to(device, dtype=torch.float16)
    
    # Tokenize
    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(device)
    
    # ç”Ÿæˆ
    with torch.inference_mode():
        output_ids = model.generate(
            input_ids,
            images=image_tensor,
            do_sample=False,
            max_new_tokens=128,
            use_cache=True,
        )
    
    # è§£ç 
    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
    
    return outputs


def flamingo_generate_answer(model, tokenizer, image_processor, image, question, device='cuda'):
    """ä½¿ç”¨OpenFlamingoç”Ÿæˆç­”æ¡ˆ"""
    # å‡†å¤‡prompt - OpenFlamingo zero-shotæ ¼å¼
    prompt = f"<image>Question: {question} Answer:"
    
    # å¤„ç†å›¾åƒ
    image_tensor = image_processor(image).unsqueeze(0).to(device)
    
    # Tokenize
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
    
    # ç”Ÿæˆ
    with torch.inference_mode():
        output_ids = model.generate(
            vision_x=image_tensor,
            lang_x=input_ids,
            max_new_tokens=32,
            num_beams=3,
        )
    
    # è§£ç 
    outputs = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    
    # æå–ç­”æ¡ˆï¼ˆå»æ‰promptéƒ¨åˆ†ï¼‰
    if "Answer:" in outputs:
        answer = outputs.split("Answer:")[-1].strip()
    else:
        answer = outputs.strip()
    
    return answer


class LVLMVQAWrapper(torch.nn.Module):
    """LVLM VQAåŒ…è£…å™¨ - ç”¨äºAutoAttack"""
    def __init__(self, lvlm_type, model, tokenizer, image_processor, normalizer, target_answer, device='cuda'):
        super().__init__()
        self.lvlm_type = lvlm_type
        self.model = model
        self.tokenizer = tokenizer
        self.image_processor = image_processor
        self.normalizer = normalizer
        self.target_answer = target_answer.lower()
        self.device = device
    
    def forward(self, images):
        """
        è¿”å›logits: [batch_size, 2]
        logits[:, 0] = ç­”æ¡ˆä¸åŒ¹é…çš„åˆ†æ•°
        logits[:, 1] = ç­”æ¡ˆåŒ¹é…çš„åˆ†æ•°
        """
        batch_size = images.size(0)
        logits = torch.zeros(batch_size, 2, device=images.device)
        
        for i in range(batch_size):
            # åå½’ä¸€åŒ–
            img_tensor = images[i]
            img_tensor = img_tensor * self.normalizer.std.view(3, 1, 1).to(img_tensor.device) + \
                        self.normalizer.mean.view(3, 1, 1).to(img_tensor.device)
            img_tensor = torch.clamp(img_tensor, 0, 1)
            
            # è½¬PIL
            img_np = (img_tensor.cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)
            pil_image = Image.fromarray(img_np)
            
            # ç”Ÿæˆç­”æ¡ˆ
            if self.lvlm_type == 'llava':
                generated = llava_generate_answer(
                    self.model, self.tokenizer, self.image_processor,
                    pil_image, self.question, self.device
                )
            else:
                generated = flamingo_generate_answer(
                    self.model, self.tokenizer, self.image_processor,
                    pil_image, self.question, self.device
                )
            
            # ç®€å•åŒ¹é…
            match_score = 1.0 if self.target_answer in generated.lower() else 0.0
            
            logits[i, 0] = 1.0 - match_score
            logits[i, 1] = match_score
        
        return logits
    
    def set_question(self, question):
        """è®¾ç½®å½“å‰é—®é¢˜"""
        self.question = question


def attack_vqa_sample(wrapper, image, question, answer, eps, device):
    """
    FAREä¸‰é˜¶æ®µæ”»å‡»pipeline for VQA
    """
    wrapper.set_question(question)
    
    threshold = 0.5
    
    # é˜¶æ®µ1: åŠç²¾åº¦APGD
    adversary_half = AutoAttack(
        wrapper,
        norm='Linf',
        eps=eps,
        version='custom',
        verbose=False
    )
    adversary_half.attacks_to_run = ['apgd-ce']
    adversary_half.apgd.n_iter = 100
    
    with torch.cuda.amp.autocast():
        adv_image = adversary_half.run_standard_evaluation(
            image.unsqueeze(0),
            torch.tensor([1]).to(device),  # target: åŒ¹é…ç­”æ¡ˆ
            bs=1
        )
    
    # æ£€æŸ¥
    with torch.no_grad():
        logits = wrapper(adv_image)
        score = F.softmax(logits, dim=-1)[0, 1].item()
    
    if score < threshold:
        return adv_image
    
    # é˜¶æ®µ2: å•ç²¾åº¦APGD
    adversary_full = AutoAttack(
        wrapper,
        norm='Linf',
        eps=eps,
        version='custom',
        verbose=False
    )
    adversary_full.attacks_to_run = ['apgd-ce']
    adversary_full.apgd.n_iter = 100
    
    with torch.enable_grad():
        adv_image = adversary_full.run_standard_evaluation(
            adv_image,
            torch.tensor([1]).to(device),
            bs=1
        )
    
    # æ£€æŸ¥
    with torch.no_grad():
        logits = wrapper(adv_image)
        score = F.softmax(logits, dim=-1)[0, 1].item()
    
    if score < threshold:
        return adv_image
    
    # é˜¶æ®µ3: Targetedæ”»å‡»
    adversary_targeted = AutoAttack(
        wrapper,
        norm='Linf',
        eps=eps,
        version='custom',
        verbose=False
    )
    adversary_targeted.attacks_to_run = ['apgd-dlr']
    adversary_targeted.apgd.n_iter = 100
    
    with torch.enable_grad():
        adv_image = adversary_targeted.run_standard_evaluation(
            adv_image,
            torch.tensor([1]).to(device),
            bs=1
        )
    
    return adv_image


def evaluate_vqa_lvlm(args):
    """ä¸»è¯„ä¼°å‡½æ•°"""
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    
    print("=" * 80)
    print(f"ğŸ“Š LVLM VQAè¯„ä¼° (FAREè®¾ç½®)")
    print(f"   LVLM: {args.lvlm_type}")
    print(f"   æ•°æ®é›†: {args.dataset}")
    print(f"   CLIP: {args.clip_checkpoint}")
    print(f"   Eps: {args.eps}/255")
    print("=" * 80)
    
    # åŠ è½½LVLM
    if args.lvlm_type == 'llava':
        tokenizer, model, image_processor, context_len, is_enhanced, normalizer = get_lvlm_model(
            lvlm_type='llava',
            lvlm_path=args.lvlm_path,
            clip_checkpoint=args.clip_checkpoint,
            clip_model_name=args.clip_model_name,
            device=device
        )
    else:
        model, image_processor, tokenizer, is_enhanced, normalizer = get_lvlm_model(
            lvlm_type='flamingo',
            lvlm_path=args.lvlm_path,
            clip_checkpoint=args.clip_checkpoint,
            clip_model_name=args.clip_model_name,
            device=device
        )
    
    # åŠ è½½æ•°æ®é›†
    samples = load_vqa_dataset(args.dataset, args.dataset_root, args.max_samples)
    
    # è¯„ä¼°clean accuracy
    print("\nğŸ”„ è¯„ä¼°å¹²å‡€æ ·æœ¬...")
    clean_correct = 0
    
    for sample in tqdm(samples, desc="Clean eval"):
        image = Image.open(sample['image_path']).convert('RGB')
        question = sample['question']
        gt_answer = sample['answer'].lower()
        
        if args.lvlm_type == 'llava':
            generated = llava_generate_answer(model, tokenizer, image_processor, image, question, device)
        else:
            generated = flamingo_generate_answer(model, tokenizer, image_processor, image, question, device)
        
        if gt_answer in generated.lower():
            clean_correct += 1
    
    clean_acc = clean_correct / len(samples)
    print(f"   Clean Accuracy: {clean_acc:.4f} ({clean_acc*100:.2f}%)")
    
    # è¯„ä¼°robust accuracy
    if args.eps > 0:
        print(f"\nğŸ”„ è¯„ä¼°å¯¹æŠ—é²æ£’æ€§ (eps={args.eps/255:.10f})...")
        
        eps_normalized = args.eps / 255.0
        robust_correct = 0
        
        for sample in tqdm(samples, desc="Robust eval"):
            image = Image.open(sample['image_path']).convert('RGB')
            question = sample['question']
            gt_answer = sample['answer']
            
            # é¢„å¤„ç†å›¾åƒ
            image_tensor = image_processor(image).to(device)
            
            # å½’ä¸€åŒ–
            image_normalized = normalizer(image_tensor)
            
            # åˆ›å»ºwrapper
            wrapper = LVLMVQAWrapper(
                args.lvlm_type, model, tokenizer, image_processor,
                normalizer, gt_answer, device
            )
            
            # æ”»å‡»
            adv_image = attack_vqa_sample(
                wrapper, image_normalized, question, gt_answer,
                eps_normalized, device
            )
            
            # åå½’ä¸€åŒ–
            adv_image = adv_image * normalizer.std.view(1, 3, 1, 1).to(device) + \
                       normalizer.mean.view(1, 3, 1, 1).to(device)
            adv_image = torch.clamp(adv_image, 0, 1)
            
            # è½¬PIL
            adv_img_np = (adv_image[0].cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)
            adv_pil = Image.fromarray(adv_img_np)
            
            # ç”Ÿæˆç­”æ¡ˆ
            if args.lvlm_type == 'llava':
                generated = llava_generate_answer(model, tokenizer, image_processor, adv_pil, question, device)
            else:
                generated = flamingo_generate_answer(model, tokenizer, image_processor, adv_pil, question, device)
            
            if gt_answer.lower() in generated.lower():
                robust_correct += 1
        
        robust_acc = robust_correct / len(samples)
        print(f"   Robust Accuracy: {robust_acc:.4f} ({robust_acc*100:.2f}%)")
    else:
        robust_acc = None
    
    # ä¿å­˜ç»“æœ
    results = {
        'lvlm_type': args.lvlm_type,
        'lvlm_path': args.lvlm_path,
        'clip_checkpoint': args.clip_checkpoint,
        'dataset': args.dataset,
        'eps': args.eps,
        'max_samples': args.max_samples,
        'clean_acc': clean_acc,
        'robust_acc': robust_acc,
        'is_enhanced': is_enhanced
    }
    
    os.makedirs(args.output_dir, exist_ok=True)
    output_file = os.path.join(
        args.output_dir,
        f"{args.lvlm_type}_{args.dataset}_eps{args.eps}_results.json"
    )
    
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_file}")
    
    return results


def main():
    parser = argparse.ArgumentParser(description='LVLM VQAè¯„ä¼° (FAREè®¾ç½®)')
    
    # LVLMé…ç½®
    parser.add_argument('--lvlm_type', type=str, required=True,
                       choices=['llava', 'flamingo'],
                       help='LVLMç±»å‹')
    parser.add_argument('--lvlm_path', type=str, required=True,
                       help='LVLMæ¨¡å‹è·¯å¾„')
    
    # CLIPé…ç½®
    parser.add_argument('--clip_checkpoint', type=str, required=True,
                       help='é²æ£’CLIP checkpointè·¯å¾„')
    parser.add_argument('--clip_model_name', type=str, default='ViT-L-14',
                       help='CLIPæ¶æ„')
    
    # æ•°æ®é›†é…ç½®
    parser.add_argument('--dataset', type=str, required=True,
                       choices=['vqav2', 'textvqa'],
                       help='VQAæ•°æ®é›†')
    parser.add_argument('--dataset_root', type=str, required=True,
                       help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--max_samples', type=int, default=500,
                       help='æœ€å¤§æ ·æœ¬æ•°')
    
    # æ”»å‡»é…ç½®
    parser.add_argument('--eps', type=int, default=4,
                       help='æ‰°åŠ¨å¼ºåº¦ (2 or 4 for 2/255 or 4/255)')
    
    # å…¶ä»–
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--output_dir', type=str, default='output/lvlm_vqa')
    
    args = parser.parse_args()
    
    evaluate_vqa_lvlm(args)


if __name__ == '__main__':
    main()
