#!/usr/bin/env python3
"""
é›¶æ ·æœ¬åˆ†ç±»è¯„ä¼°è„šæœ¬ - åŸºäºFAREè®ºæ–‡è®¾ç½®
æ”¯æŒImageNetå’Œ13ä¸ªé›¶æ ·æœ¬æ•°æ®é›†
æ”»å‡»: APGD-CE + APGD-DLR (100 iterations each)
Eps: 2/255 and 4/255
"""

import os
import sys
import argparse
import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from train.adversarial_training_clip_enhanced import EnhancedClipVisionModel
from train.test_time_defense import ZeroPurDefense, InterpretabilityGuidedDefense, CombinedDefense
from autoattack import AutoAttack
import open_clip
from CLIP_eval.eval_utils import load_clip_model as load_baseline_clip_model
from CLIP_eval.eval_utils_enhanced import load_enhanced_clip_model
from open_flamingo.eval.classification_utils import IMAGENET_1K_CLASS_ID_TO_LABEL


# 13ä¸ªé›¶æ ·æœ¬æ•°æ®é›†çš„é…ç½®
ZEROSHOT_DATASETS = {
    'imagenet': {'path': 'datasets/imagenet/val', 'templates': 'imagenet'},
    'cifar10': {'path': 'datasets/webdatasets/cifar10/test', 'templates': 'simple'},
    'cifar100': {'path': 'datasets/webdatasets/cifar100/test', 'templates': 'simple'},
    'flowers102': {'path': 'datasets/webdatasets/flowers/test', 'templates': 'flowers'},
    'imagenet_r': {'path': 'datasets/webdatasets/imagenet_r/test', 'templates': 'simple'},
    'imagenet_sketch': {'path': 'datasets/webdatasets/imagenet_sketch/test', 'templates': 'simple'},
    'pets': {'path': 'datasets/webdatasets/pets/test', 'templates': 'pets'},
    'cars': {'path': 'datasets/webdatasets/cars/test', 'templates': 'simple'},
    'dtd': {'path': 'datasets/webdatasets/dtd/test', 'templates': 'dtd'},
    'caltech101': {'path': 'datasets/webdatasets/caltech101/test', 'templates': 'simple'},
    'aircraft': {'path': 'datasets/webdatasets/fgvc_aircraft/test', 'templates': 'aircraft'},
    'eurosat': {'path': 'datasets/webdatasets/eurosat/test', 'templates': 'simple'},
    'pcam': {'path': 'datasets/webdatasets/pcam/test', 'templates': 'simple'},
    'stl10': {'path': 'datasets/webdatasets/stl10/test', 'templates': 'simple'},
}


# CLIP prompt templates
PROMPT_TEMPLATES = {
    'imagenet': [
        'a photo of a {}.',
        'a blurry photo of a {}.',
        'a photo of many {}.',
        'a photo of the large {}.',
        'a photo of the small {}.',
    ],
    'simple': ['a photo of a {}.'],
    'flowers': ['a photo of a {}, a type of flower.'],
    'food': ['a photo of {}, a type of food.'],
    'pets': ['a photo of a {}, a type of pet.'],
    'aircraft': ['a photo of a {}, a type of aircraft.'],
    'dtd': ['{} texture.'],
}


class VisionEncoderWrapper(torch.nn.Module):
    """å°†encode_imageåŒ…è£…åˆ°forwardä¸­ï¼Œä½¿DataParallelèƒ½æ­£ç¡®åˆ†å¸ƒè®¡ç®—"""
    def __init__(self, clip_model, is_enhanced=False):
        super().__init__()
        self.clip_model = clip_model
        self.is_enhanced = is_enhanced
    
    def forward(self, x, mode='eval', gray_box=False):
        if self.is_enhanced:
            if gray_box:
                # ç°ç›’ï¼šåªæ”»å‡»backbone
                image_emb, _, _, _, _, _ = self.clip_model(x, mode='attack')
            else:
                # ç™½ç›’ï¼šæ”»å‡»å®Œæ•´é˜²å¾¡
                image_emb, _, _, _, _, _ = self.clip_model(x, mode=mode)
        else:
            image_emb = self.clip_model.encode_image(x)
        return image_emb
    
    def encode_text(self, text):
        """Delegate to clip_model"""
        return self.clip_model.encode_text(text)


def load_clip_model(checkpoint_path, device, clip_model_name='ViT-L-14'):
    """åŠ è½½CLIPæ¨¡å‹ - ä½¿ç”¨CLIP_eval/eval_utils.pyä¸­çš„æ ‡å‡†åŠ è½½æ–¹å¼"""
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {checkpoint_path}")
    print(f"ğŸ“¦ CLIPæ¶æ„: {clip_model_name}")
    
    checkpoint_lower = checkpoint_path.lower()
    is_enhanced = not ('fare' in checkpoint_lower or 'tecoa' in checkpoint_lower)
    
    if is_enhanced:
        print("ğŸ“¦ åŠ è½½KeyTokenå¢å¼ºæ¨¡å‹...")
        enhanced_model, preprocessor_no_norm, normalizer = load_enhanced_clip_model(
            clip_model_name, checkpoint_path
        )
        vision_base = enhanced_model
        
        # ç‹¬ç«‹åŠ è½½æ–‡æœ¬æ¨¡å‹ (openaiæƒé‡)
        text_clip_model, _, _ = open_clip.create_model_and_transforms(
            clip_model_name, pretrained='openai', device='cpu'
        )
        text_model = text_clip_model.to(device)
        
    else:
        # FARE/TeCoAæ¨¡å‹ - ä½¿ç”¨eval_utilsä¸­çš„æ ‡å‡†åŠ è½½å‡½æ•°
        print("ğŸ“¦ åŠ è½½FARE/TeCoAæ¨¡å‹...")
        base_model, preprocessor_no_norm, normalizer = load_baseline_clip_model(
            clip_model_name, checkpoint_path
        )
        vision_base = base_model
        text_model = base_model.to(device)  # baselineæ¨¡å‹åŒ…å«å®Œæ•´CLIP
    
    tokenizer = open_clip.get_tokenizer(clip_model_name)
    preprocess = preprocessor_no_norm
    
    # Vision encoder wrapper
    model = vision_base.to(device)
    
    # åˆ›å»ºVision Encoder Wrapper
    vision_wrapper = VisionEncoderWrapper(model, is_enhanced)
    
    # å¤šGPUæ”¯æŒ - åŒ…è£…wrapperè€Œä¸æ˜¯åŸå§‹æ¨¡å‹
    num_gpus = torch.cuda.device_count()
    if num_gpus > 1:
        print(f"ğŸ’» ä½¿ç”¨ {num_gpus} å¼ GPU (DataParallel)")
        vision_wrapper = torch.nn.DataParallel(vision_wrapper)
    
    vision_wrapper.eval()
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (is_enhanced={is_enhanced})")
    
    # è¿”å›vision_wrapperä½œä¸ºä¸»æ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™text_modelç”¨äºæ–‡æœ¬ç¼–ç 
    return vision_wrapper, text_model, tokenizer, preprocess, normalizer, is_enhanced


def load_dataset(dataset_name, preprocess, max_samples=-1):
    """åŠ è½½æ•°æ®é›†"""
    config = ZEROSHOT_DATASETS[dataset_name]
    dataset_path = config['path']
    
    print(f"ğŸ”„ åŠ è½½æ•°æ®é›†: {dataset_name}")
    print(f"   è·¯å¾„: {dataset_path}")
    
    from torchvision import datasets, transforms
    from torchvision.datasets import ImageFolder
    
    # ImageNetä½¿ç”¨æ ‡å‡†ImageFolderæ ¼å¼
    if dataset_name == 'imagenet':
        dataset = ImageFolder(dataset_path, transform=preprocess)
    # webdatasetæ ¼å¼æ•°æ®é›†
    elif 'webdatasets' in dataset_path:
        import webdataset as wds
        from PIL import Image
        import io
        import glob
        
        # webdatasetåŠ è½½é€»è¾‘ - ä½¿ç”¨globå±•å¼€taræ–‡ä»¶åˆ—è¡¨
        tar_pattern = os.path.join(dataset_path, "*.tar")
        tar_files = sorted(glob.glob(tar_pattern))
        
        if not tar_files:
            raise FileNotFoundError(f"No .tar files found in {dataset_path}")
        
        dataset_wds = wds.WebDataset(tar_files).decode("pil").to_tuple("jpg;png;webp", "cls")
        
        # è¯»å–ç±»åˆ«åç§°
        dataset_root = os.path.dirname(dataset_path)  # å»æ‰ /test åç¼€
        classnames_file = os.path.join(dataset_root, "classnames.txt")
        if os.path.exists(classnames_file):
            with open(classnames_file, 'r') as f:
                classes = [line.strip() for line in f if line.strip()]
        else:
            classes = None
        
        # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥æ”¯æŒlenå’Œç´¢å¼•è®¿é—®
        dataset_list = []
        for img, label in dataset_wds:
            if preprocess is not None:
                img = preprocess(img)
            dataset_list.append((img, label))
            if max_samples > 0 and len(dataset_list) >= max_samples:
                break
        
        # åˆ›å»ºç®€å•çš„åŒ…è£…ç±»
        class SimpleDataset(torch.utils.data.Dataset):
            def __init__(self, data, classes=None):
                self.data = data
                self.classes = classes
            def __len__(self):
                return len(self.data)
            def __getitem__(self, idx):
                return self.data[idx]
        
        dataset = SimpleDataset(dataset_list, classes)
    else:
        # å…¶ä»–æ•°æ®é›†ä½¿ç”¨ImageFolderæ ¼å¼
        dataset = ImageFolder(dataset_path, transform=preprocess)
    
    if max_samples > 0 and 'webdatasets' not in dataset_path:
        indices = torch.randperm(len(dataset))[:max_samples]
        dataset = torch.utils.data.Subset(dataset, indices)
    
    print(f"   âœ“ åŠ è½½ {len(dataset)} å¼ å›¾ç‰‡")
    
    return dataset


def get_text_embeddings(text_model, tokenizer, classnames, templates, device):
    """è®¡ç®—ç±»åˆ«æ–‡æœ¬åµŒå…¥"""
    print(f"ğŸ”„ è®¡ç®—æ–‡æœ¬åµŒå…¥ ({len(classnames)} ç±»)...")
    
    text_embeddings = []
    
    with torch.no_grad():
        for classname in tqdm(classnames, desc="Text embeddings"):
            # å¯¹æ¯ä¸ªç±»åˆ«åº”ç”¨æ‰€æœ‰æ¨¡æ¿
            texts = [template.format(classname) for template in templates]
            texts_tokenized = tokenizer(texts).to(device)
            
            # ç¼–ç æ–‡æœ¬ï¼ˆå¤„ç†å¯èƒ½çš„DataParallelåŒ…è£…ï¼‰
            text_encoder = text_model.module if isinstance(text_model, torch.nn.DataParallel) else text_model
            class_embeddings = text_encoder.encode_text(texts_tokenized)
            
            # å½’ä¸€åŒ–å¹¶å¹³å‡
            class_embeddings = F.normalize(class_embeddings, dim=-1)
            class_embedding = class_embeddings.mean(dim=0)
            class_embedding = F.normalize(class_embedding, dim=-1)
            
            text_embeddings.append(class_embedding)
    
    text_embeddings = torch.stack(text_embeddings)
    print(f"   âœ“ æ–‡æœ¬åµŒå…¥: {text_embeddings.shape}")
    
    return text_embeddings


def evaluate_clean(model, dataloader, text_embeddings, device, normalizer, is_enhanced=False, mode='eval'):
    """è¯„ä¼°å¹²å‡€æ ·æœ¬å‡†ç¡®ç‡"""
    print("ğŸ”„ è¯„ä¼°å¹²å‡€æ ·æœ¬...")
    
    correct = 0
    total = 0
    
    model.eval()
    with torch.no_grad():
        for images, labels in tqdm(dataloader, desc="Clean eval"):
            images = images.to(device)
            labels = labels.to(device)
            
            # baselineæ¨¡å‹éœ€è¦é¢å¤–normalizeï¼›å¢å¼ºæ¨¡å‹å†…éƒ¨å·²å¤„ç†
            if not is_enhanced:
                images = normalizer(images)
            
            # ç¼–ç å›¾åƒ - VisionEncoderWrapperå·²åŒ…è£…äº†DataParallel
            image_embeddings = model(images)
            
            image_embeddings = F.normalize(image_embeddings, dim=-1)
            
            # åˆ†ç±»
            logits = 100.0 * image_embeddings @ text_embeddings.T
            predictions = logits.argmax(dim=-1)
            
            correct += (predictions == labels).sum().item()
            total += labels.size(0)
    
    accuracy = correct / total
    print(f"   Clean Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    return accuracy


def evaluate_robust(model, dataloader, text_embeddings, device, eps, iterations,
                   normalizer, is_enhanced=False, mode='eval', gray_box=False, defense_type=None, noise_std=0.0):
    """è¯„ä¼°å¯¹æŠ—é²æ£’æ€§"""
    print(f"ğŸ”„ è¯„ä¼°å¯¹æŠ—é²æ£’æ€§ (eps={eps}, iter={iterations})...")
    if defense_type:
        print(f"   ğŸ›¡ï¸  æµ‹è¯•æ—¶é˜²å¾¡: {defense_type}")
    if noise_std > 0:
        print(f"   ğŸ² è¾“å…¥å™ªå£°: std={noise_std} (Randomized Smoothing)")
    
    # åˆ›å»ºåˆ†ç±»åŒ…è£…å™¨
    class CLIPClassifier(torch.nn.Module):
        def __init__(self, vision_model, text_embeddings, normalizer, is_enhanced, mode, gray_box, noise_std=0.0):
            super().__init__()
            self.vision_model = vision_model
            # æ³¨å†Œä¸ºbufferä½¿DataParallelèƒ½æ­£ç¡®åˆ†é…åˆ°å¯¹åº”è®¾å¤‡
            self.register_buffer('text_embeddings', text_embeddings)
            self.normalizer = normalizer
            self.is_enhanced = is_enhanced
            self.mode = mode
            self.gray_box = gray_box
            self.noise_std = noise_std
        
        def forward(self, x):
            # åº”ç”¨è¾“å…¥å™ªå£°ï¼ˆRandomized Smoothingï¼‰
            if self.noise_std > 0:
                noise = torch.randn_like(x) * self.noise_std
                x = x + noise
                x = torch.clamp(x, 0, 1)
            
            # baselineæ¨¡å‹éœ€è¦é¢å¤–normalizeï¼›å¢å¼ºæ¨¡å‹å†…éƒ¨å·²å¤„ç†
            if not self.is_enhanced:
                x = self.normalizer(x)
            
            # VisionEncoderWrapperå·²å¤„ç†DataParallelå’Œis_enhancedé€»è¾‘
            image_emb = self.vision_model(x)
            
            image_emb = F.normalize(image_emb, dim=-1)
            logits = 100.0 * image_emb @ self.text_embeddings.T
            return logits
    
    # è§£åŒ…vision_modelçš„DataParallelï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œç„¶åå¯¹æ•´ä¸ªclassifierè¿›è¡ŒDataParallel
    # é¿å…åŒé‡åŒ…è£…å¯¼è‡´æ€§èƒ½ä¸‹é™
    base_vision_model = model.module if isinstance(model, torch.nn.DataParallel) else model
    
    classifier = CLIPClassifier(base_vision_model, text_embeddings, normalizer, is_enhanced, mode, gray_box, noise_std)
    
    # å¤šGPUåŒ…è£…classifier - AutoAttackéœ€è¦å¯¹æ•´ä¸ªclassifierè¿›è¡ŒDataParallel
    num_gpus = torch.cuda.device_count()
    if num_gpus > 1:
        classifier = torch.nn.DataParallel(classifier)
    
    classifier.eval()
    
    # åˆå§‹åŒ–æµ‹è¯•æ—¶é˜²å¾¡
    test_defense = None
    if defense_type and is_enhanced:
        if defense_type == 'zeropur':
            test_defense = ZeroPurDefense(sigma=0.5, alpha=0.3, num_steps=5)
        elif defense_type == 'interpretability':
            test_defense = InterpretabilityGuidedDefense(model, top_k_ratio=0.3)
        elif defense_type == 'combined':
            # ç»„åˆé˜²å¾¡ï¼šåŒæ—¶ä½¿ç”¨ZeroPurå’ŒInterpretability-Guided
            test_defense = CombinedDefense(
                model,
                use_interpretability=True,
                use_zeropur=True,
                sigma=0.5,
                alpha=0.3,
                num_steps=5,
                top_k_ratio=0.3
            )
    
    # AutoAttacké…ç½® - ä¸¥æ ¼æŒ‰ç…§FAREè®ºæ–‡è®¾ç½®
    adversary = AutoAttack(
        classifier,
        norm='Linf',
        eps=eps,
        version='custom',  # ä½¿ç”¨customç‰ˆæœ¬ï¼Œä¸evaluate_robust.pyä¸€è‡´
        attacks_to_run=['apgd-ce', 'apgd-dlr'],  # FAREè®ºæ–‡ä½¿ç”¨çš„ä¸¤ç§æ”»å‡»
        verbose=False,
        device=device
    )
    
    # è®¾ç½®æ”»å‡»è¿­ä»£æ¬¡æ•°
    adversary.apgd.n_iter = iterations
    adversary.apgd_targeted.n_iter = iterations
    
    correct = 0
    total = 0
    
    for images, labels in tqdm(dataloader, desc="Robust eval"):
        images = images.to(device)
        labels = labels.to(device)
        
        # AutoAttack
        with torch.enable_grad():
            adv_images = adversary.run_standard_evaluation(images, labels, bs=images.size(0))
        
        # åº”ç”¨æµ‹è¯•æ—¶é˜²å¾¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if test_defense is not None:
            with torch.no_grad():
                logits_pred = classifier(adv_images)
                pred_classes = logits_pred.argmax(dim=-1)
            
            if defense_type == 'zeropur':
                with torch.enable_grad():
                    adv_images = test_defense.purify(adv_images, model)
            elif defense_type == 'interpretability':
                purified_images = []
                for i in range(adv_images.size(0)):
                    purified = test_defense.purify(
                        adv_images[i:i+1], 
                        pred_classes[i].item()
                    )
                    purified_images.append(purified)
                adv_images = torch.cat(purified_images, dim=0)
            elif defense_type == 'combined':
                purified_images = []
                for i in range(adv_images.size(0)):
                    with torch.enable_grad():
                        purified = test_defense.purify(
                            adv_images[i:i+1], 
                            pred_classes[i].item()
                        )
                    purified_images.append(purified)
                adv_images = torch.cat(purified_images, dim=0)
        
        # è¯„ä¼°
        with torch.no_grad():
            logits = classifier(adv_images)
            predictions = logits.argmax(dim=-1)
            correct += (predictions == labels).sum().item()
            total += labels.size(0)
    
    accuracy = correct / total
    print(f"   Robust Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    return accuracy


def main():
    parser = argparse.ArgumentParser(description='é›¶æ ·æœ¬åˆ†ç±»é²æ£’æ€§è¯„ä¼°')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹æƒé‡è·¯å¾„')
    parser.add_argument('--clip_model_name', type=str, default='ViT-L-14',
                       help='CLIPæ¨¡å‹æ¶æ„')
    parser.add_argument('--dataset', type=str, default='imagenet',
                       choices=list(ZEROSHOT_DATASETS.keys()),
                       help='æ•°æ®é›†åç§°')
    
    # æ”»å‡»é…ç½®
    parser.add_argument('--eps', type=float, default=4.0,
                       help='æ‰°åŠ¨å¼ºåº¦ (x/255)')
    parser.add_argument('--iterations', type=int, default=100,
                       help='APGDè¿­ä»£æ¬¡æ•°')
    parser.add_argument('--gray_box', action='store_true',
                       help='ç°ç›’æ”»å‡»ï¼ˆä»…æ”»å‡»backboneï¼‰')
    
    # è¯„ä¼°é…ç½®
    parser.add_argument('--mode', type=str, default='eval',
                       choices=['eval', 'baseline'],
                       help='æ¨ç†æ¨¡å¼')
    parser.add_argument('--batch_size', type=int, default=64)  # 4GPUå¯ç”¨64
    parser.add_argument('--robust_batch_size', type=int, default=64,
                       help='Robust eval batch size (same as batch_size with multi-GPU)')
    parser.add_argument('--max_samples', type=int, default=-1,
                       help='æœ€å¤§æ ·æœ¬æ•°ï¼ˆ-1è¡¨ç¤ºå…¨éƒ¨ï¼‰')
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--output_dir', type=str, default='output/zeroshot_eval')
    
    # æµ‹è¯•æ—¶é˜²å¾¡ç­–ç•¥
    parser.add_argument('--defense', type=str, default=None,
                       choices=[None, 'zeropur', 'interpretability', 'combined'],
                       help='æµ‹è¯•æ—¶é˜²å¾¡ç­–ç•¥ï¼ˆæ— éœ€è®­ç»ƒï¼‰ã€‚combined=ZeroPur+Interpretability')
    parser.add_argument('--noise_std', type=float, default=0.0,
                       help='è¾“å…¥éšæœºå™ªå£°æ ‡å‡†å·®ï¼ˆRandomized Smoothingï¼‰ï¼Œ0=ç¡®å®šæ€§')
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹ - vision_modelæ˜¯DataParallelåŒ…è£…çš„VisionEncoderWrapper, text_modelæ˜¯åŸå§‹CLIPæ¨¡å‹
    vision_model, text_model, tokenizer, preprocess, normalizer, is_enhanced = load_clip_model(
        args.checkpoint, device, args.clip_model_name
    )
    
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset(args.dataset, preprocess, args.max_samples)
    dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=args.batch_size, shuffle=False, num_workers=4
    )
    
    # è·å–ç±»åˆ«åç§°å’Œæ¨¡æ¿
    if args.dataset == 'imagenet':
        # ImageNetéœ€è¦ä½¿ç”¨äººç±»å¯è¯»çš„ç±»åˆ«åç§°ï¼Œè€Œä¸æ˜¯WordNet ID
        classnames = list(IMAGENET_1K_CLASS_ID_TO_LABEL.values())
    else:
        classnames = dataset.dataset.classes if hasattr(dataset, 'dataset') else dataset.classes
    template_key = ZEROSHOT_DATASETS[args.dataset]['templates']
    templates = PROMPT_TEMPLATES[template_key]
    
    # è®¡ç®—æ–‡æœ¬åµŒå…¥ - ä½¿ç”¨åŸå§‹text_model
    text_embeddings = get_text_embeddings(
        text_model, tokenizer, classnames, templates, device
    )
    
    # è¯„ä¼°
    print("=" * 80)
    print(f"ğŸ“Š é›¶æ ·æœ¬åˆ†ç±»è¯„ä¼°")
    print(f"   æ•°æ®é›†: {args.dataset}")
    print(f"   æ¨¡å‹: {args.checkpoint}")
    print(f"   Eps: {args.eps}/255")
    print("=" * 80)
    
    # å¹²å‡€æ ·æœ¬ - ä½¿ç”¨vision_model
    clean_acc = evaluate_clean(vision_model, dataloader, text_embeddings, device, normalizer, is_enhanced, args.mode)
    
    # å¯¹æŠ—æ ·æœ¬ - ä½¿ç”¨æ›´å°çš„batch_sizeé¿å…OOM
    eps_normalized = args.eps / 255.0
    robust_dataloader = torch.utils.data.DataLoader(
        dataset, batch_size=args.robust_batch_size, shuffle=False, num_workers=4
    )
    robust_acc = evaluate_robust(
        vision_model, robust_dataloader, text_embeddings, device,
        eps_normalized, args.iterations, normalizer, is_enhanced, args.mode, args.gray_box,
        defense_type=args.defense, noise_std=args.noise_std
    )
    
    # ä¿å­˜ç»“æœ
    os.makedirs(args.output_dir, exist_ok=True)
    model_name = os.path.basename(args.checkpoint).replace('.pt', '')
    eps_str = f"eps{int(args.eps)}"
    gray_str = "_graybox" if args.gray_box else ""
    defense_str = f"_{args.defense}" if args.defense else ""
    noise_str = f"_noise{args.noise_std}" if args.noise_std > 0 else ""
    
    result_file = os.path.join(
        args.output_dir,
        f"{model_name}_{args.dataset}_{eps_str}{gray_str}{defense_str}{noise_str}_results.json"
    )
    
    results = {
        'model': args.checkpoint,
        'dataset': args.dataset,
        'mode': args.mode,
        'eps': args.eps,
        'iterations': args.iterations,
        'gray_box': args.gray_box,
        'defense': args.defense,
        'noise_std': args.noise_std,
        'clean_acc': clean_acc,
        'robust_acc': robust_acc,
        'timestamp': datetime.now().isoformat()
    }
    
    with open(result_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print("=" * 80)
    print(f"âœ… ç»“æœå·²ä¿å­˜: {result_file}")
    print("=" * 80)


if __name__ == '__main__':
    main()
