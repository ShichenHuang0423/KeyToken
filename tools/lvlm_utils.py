"""
LVLMå·¥å…· - åŠ è½½LLaVAå’ŒOpenFlamingoå¹¶æ›¿æ¢CLIP encoder
"""

import torch
import open_clip
from typing import Optional, Tuple
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig
from llava.model.builder import load_pretrained_model as load_llava_model
from llava.mm_utils import get_model_name_from_path
from open_flamingo import create_model_and_transforms


def load_robust_clip_encoder(checkpoint_path: str, clip_model_name: str = 'ViT-L-14', device='cuda'):
    """
    åŠ è½½é²æ£’CLIP vision encoder
    
    Args:
        checkpoint_path: CLIP checkpointè·¯å¾„ (FARE/KeyToken)
        clip_model_name: CLIPæ¶æ„åç§°
        device: è®¾å¤‡
        
    Returns:
        vision_model, preprocess, normalizer, is_enhanced
    """
    from CLIP_eval.eval_utils import load_clip_model as load_baseline_clip_model
    from CLIP_eval.eval_utils_enhanced import load_enhanced_clip_model
    
    if 'fare' in checkpoint_path.lower() or 'tecoa' in checkpoint_path.lower():
        # FARE/TeCoAåŸºçº¿æ¨¡å‹
        model, preprocessor_no_norm, normalizer = load_baseline_clip_model(
            clip_model_name, checkpoint_path
        )
        is_enhanced = False
    else:
        # KeyTokenå¢å¼ºæ¨¡å‹
        enhanced_model, preprocessor_no_norm, normalizer = load_enhanced_clip_model(
            clip_model_name, checkpoint_path
        )
        model = enhanced_model
        is_enhanced = True
    
    model = model.to(device)
    model.eval()
    
    return model, preprocessor_no_norm, normalizer, is_enhanced


def replace_llava_clip_encoder(
    llava_model_path: str,
    clip_checkpoint_path: str,
    clip_model_name: str = 'ViT-L-14',
    device: str = 'cuda',
    load_8bit: bool = False,
    load_4bit: bool = False
):
    """
    åŠ è½½LLaVAå¹¶æ›¿æ¢å…¶CLIP vision encoder
    
    Args:
        llava_model_path: LLaVAæ¨¡å‹è·¯å¾„
        clip_checkpoint_path: é²æ£’CLIP checkpoint
        clip_model_name: CLIPæ¶æ„
        device: è®¾å¤‡
        load_8bit: æ˜¯å¦8bité‡åŒ–
        load_4bit: æ˜¯å¦4bité‡åŒ–
        
    Returns:
        tokenizer, model, image_processor, context_len, is_enhanced
    """
    print(f"ğŸ”„ åŠ è½½LLaVAæ¨¡å‹: {llava_model_path}")
    
    # åŠ è½½LLaVA
    model_name = get_model_name_from_path(llava_model_path)
    tokenizer, model, image_processor, context_len = load_llava_model(
        model_path=llava_model_path,
        model_base=None,
        model_name=model_name,
        load_8bit=load_8bit,
        load_4bit=load_4bit,
        device=device
    )
    
    print(f"ğŸ”„ æ›¿æ¢CLIP vision encoder: {clip_checkpoint_path}")
    
    # åŠ è½½é²æ£’CLIP encoder
    robust_vision_model, preprocess, normalizer, is_enhanced = load_robust_clip_encoder(
        clip_checkpoint_path, clip_model_name, device
    )
    
    # æ›¿æ¢LLaVAçš„vision tower
    if hasattr(model.get_vision_tower(), 'vision_tower'):
        # LLaVA-1.5ç»“æ„
        original_vision_tower = model.get_vision_tower().vision_tower
        
        # ä¿ç•™åŸå§‹é…ç½®
        config = original_vision_tower.config if hasattr(original_vision_tower, 'config') else None
        
        # æ›¿æ¢vision encoder
        if is_enhanced:
            # KeyTokenå¢å¼ºæ¨¡å‹
            model.get_vision_tower().vision_tower = robust_vision_model.model.visual
        else:
            # FARE/TeCoAåŸºçº¿
            model.get_vision_tower().vision_tower = robust_vision_model.visual
        
        if config is not None:
            model.get_vision_tower().vision_tower.config = config
        
        print(f"âœ… å·²æ›¿æ¢LLaVA vision encoder (is_enhanced={is_enhanced})")
    else:
        raise ValueError("æ— æ³•æ‰¾åˆ°LLaVAçš„vision tower")
    
    # æ›´æ–°image_processorä½¿ç”¨é²æ£’CLIPçš„é¢„å¤„ç†
    # æ³¨æ„ï¼šLLaVAä½¿ç”¨ç‰¹å®šçš„å›¾åƒé¢„å¤„ç†ï¼Œæˆ‘ä»¬åªæ›¿æ¢normalizeéƒ¨åˆ†
    image_processor.image_mean = normalizer.mean.tolist()
    image_processor.image_std = normalizer.std.tolist()
    
    return tokenizer, model, image_processor, context_len, is_enhanced, normalizer


def replace_flamingo_clip_encoder(
    flamingo_checkpoint_path: str,
    clip_checkpoint_path: str,
    clip_model_name: str = 'ViT-L-14',
    lang_encoder_path: str = "mosaicml/mpt-7b",
    tokenizer_path: str = "mosaicml/mpt-7b",
    cross_attn_every_n_layers: int = 4,
    device: str = 'cuda'
):
    """
    åŠ è½½OpenFlamingoå¹¶æ›¿æ¢å…¶CLIP vision encoder
    
    Args:
        flamingo_checkpoint_path: Flamingo checkpointè·¯å¾„
        clip_checkpoint_path: é²æ£’CLIP checkpoint
        clip_model_name: CLIPæ¶æ„
        lang_encoder_path: è¯­è¨€æ¨¡å‹è·¯å¾„
        tokenizer_path: Tokenizerè·¯å¾„
        cross_attn_every_n_layers: Cross attentioné¢‘ç‡
        device: è®¾å¤‡
        
    Returns:
        model, image_processor, tokenizer, is_enhanced, normalizer
    """
    print(f"ğŸ”„ åŠ è½½OpenFlamingoæ¨¡å‹: {flamingo_checkpoint_path}")
    
    # åŠ è½½é²æ£’CLIP encoder
    robust_vision_model, preprocess, normalizer, is_enhanced = load_robust_clip_encoder(
        clip_checkpoint_path, clip_model_name, device
    )
    
    # åˆ›å»ºOpenFlamingoæ¨¡å‹
    # æ³¨æ„ï¼šæˆ‘ä»¬ä¸ä½¿ç”¨create_model_and_transformsï¼Œè€Œæ˜¯æ‰‹åŠ¨åˆ›å»ºå¹¶æ›¿æ¢vision encoder
    from open_flamingo.src.flamingo import Flamingo
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    # åŠ è½½è¯­è¨€æ¨¡å‹
    print(f"ğŸ”„ åŠ è½½è¯­è¨€æ¨¡å‹: {lang_encoder_path}")
    lang_model = AutoModelForCausalLM.from_pretrained(
        lang_encoder_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map=device
    )
    
    # åŠ è½½tokenizer
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
    tokenizer.padding_side = "left"
    
    # åˆ›å»ºFlamingoæ¨¡å‹
    model = Flamingo(
        vision_encoder=robust_vision_model.visual if not is_enhanced else robust_vision_model.model.visual,
        lang_encoder=lang_model,
        eoc_token_id=tokenizer.encode("<|endofchunk|>")[-1],
        media_token_id=tokenizer.encode("<image>")[-1],
        vis_dim=robust_vision_model.visual.output_dim if not is_enhanced else robust_vision_model.model.visual.output_dim,
        cross_attn_every_n_layers=cross_attn_every_n_layers,
    )
    
    # åŠ è½½Flamingo checkpoint
    print(f"ğŸ”„ åŠ è½½Flamingo checkpoint...")
    checkpoint = torch.load(flamingo_checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint, strict=False)
    
    model = model.to(device)
    model.eval()
    
    print(f"âœ… å·²åŠ è½½OpenFlamingo (is_enhanced={is_enhanced})")
    
    return model, preprocess, tokenizer, is_enhanced, normalizer


def get_lvlm_model(
    lvlm_type: str,
    lvlm_path: str,
    clip_checkpoint: str,
    clip_model_name: str = 'ViT-L-14',
    device: str = 'cuda'
):
    """
    ç»Ÿä¸€æ¥å£ï¼šåŠ è½½LVLMå¹¶æ›¿æ¢CLIP encoder
    
    Args:
        lvlm_type: 'llava' or 'flamingo'
        lvlm_path: LVLMæ¨¡å‹è·¯å¾„
        clip_checkpoint: é²æ£’CLIP checkpoint
        clip_model_name: CLIPæ¶æ„
        device: è®¾å¤‡
        
    Returns:
        æ ¹æ®lvlm_typeè¿”å›ç›¸åº”çš„æ¨¡å‹ç»„ä»¶
    """
    if lvlm_type.lower() == 'llava':
        return replace_llava_clip_encoder(
            llava_model_path=lvlm_path,
            clip_checkpoint_path=clip_checkpoint,
            clip_model_name=clip_model_name,
            device=device
        )
    elif lvlm_type.lower() == 'flamingo':
        flamingo_checkpoint = f"{lvlm_path}/checkpoint.pt"
        return replace_flamingo_clip_encoder(
            flamingo_checkpoint_path=flamingo_checkpoint,
            clip_checkpoint_path=clip_checkpoint,
            clip_model_name=clip_model_name,
            device=device
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„LVLMç±»å‹: {lvlm_type}")
