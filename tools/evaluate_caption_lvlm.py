#!/usr/bin/env python3
"""
å®Œæ•´LVLM Captionè¯„ä¼° - ä½¿ç”¨LLaVAå’ŒOpenFlamingo
éµå¾ªFAREè®ºæ–‡è®¾ç½®
"""

import os
import sys
import argparse
import json
import torch
import torch.nn.functional as F
from tqdm import tqdm
from pathlib import Path
from PIL import Image
import numpy as np
from autoattack import AutoAttack
from pycocotools.coco import COCO
from pycocoevalcap.eval import COCOEvalCap

# LLaVA imports
try:
    from llava.conversation import conv_templates, SeparatorStyle
    from llava.utils import disable_torch_init
    from llava.mm_utils import process_images, tokenizer_image_token
    from llava.constants import IMAGE_TOKEN_INDEX
except ImportError:
    print("è­¦å‘Š: LLaVAæœªå®‰è£…")

# å¯¼å…¥LVLMå·¥å…·
from lvlm_utils import get_lvlm_model


def load_caption_dataset(dataset_name: str, dataset_root: str, max_samples: int = -1):
    """åŠ è½½Captionæ•°æ®é›†"""
    if dataset_name == 'coco':
        ann_file = os.path.join(dataset_root, 'annotations/captions_val2014.json')
        images_dir = os.path.join(dataset_root, 'val2014')
        
        coco = COCO(ann_file)
        img_ids = coco.getImgIds()
        
        samples = []
        for img_id in img_ids:
            img_info = coco.loadImgs(img_id)[0]
            img_path = os.path.join(images_dir, img_info['file_name'])
            
            if not os.path.exists(img_path):
                continue
            
            # è·å–å‚è€ƒcaptions
            ann_ids = coco.getAnnIds(imgIds=img_id)
            anns = coco.loadAnns(ann_ids)
            captions = [ann['caption'] for ann in anns]
            
            samples.append({
                'image_id': img_id,
                'image_path': img_path,
                'captions': captions
            })
    
    elif dataset_name == 'flickr30k':
        ann_file = os.path.join(dataset_root, 'results_20130124.token')
        images_dir = os.path.join(dataset_root, 'flickr30k_images')
        
        # è§£æFlickr30k annotations
        image_captions = {}
        with open(ann_file) as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) < 2:
                    continue
                img_name = parts[0].split('#')[0]
                caption = parts[1]
                
                if img_name not in image_captions:
                    image_captions[img_name] = []
                image_captions[img_name].append(caption)
        
        samples = []
        for img_name, captions in image_captions.items():
            img_path = os.path.join(images_dir, img_name)
            if not os.path.exists(img_path):
                continue
            
            samples.append({
                'image_id': img_name,
                'image_path': img_path,
                'captions': captions
            })
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    # éšæœºé‡‡æ ·
    if max_samples > 0 and len(samples) > max_samples:
        import random
        random.seed(42)
        samples = random.sample(samples, max_samples)
    
    print(f"âœ“ åŠ è½½ {len(samples)} ä¸ªCaptionæ ·æœ¬")
    return samples


def llava_generate_caption(model, tokenizer, image_processor, image, device='cuda'):
    """ä½¿ç”¨LLaVAç”Ÿæˆcaption"""
    disable_torch_init()
    
    # å‡†å¤‡conversation
    conv = conv_templates["llava_v1"].copy()
    
    # Caption prompt
    question = "Provide a detailed description of the image."
    
    # æ·»åŠ å›¾åƒtoken
    if model.config.mm_use_im_start_end:
        question = f"<im_start><image><im_end>\n{question}"
    else:
        question = f"<image>\n{question}"
    
    conv.append_message(conv.roles[0], question)
    conv.append_message(conv.roles[1], None)
    prompt = conv.get_prompt()
    
    # å¤„ç†å›¾åƒ
    image_tensor = process_images([image], image_processor, model.config).to(device, dtype=torch.float16)
    
    # Tokenize
    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(device)
    
    # ç”Ÿæˆ
    with torch.inference_mode():
        output_ids = model.generate(
            input_ids,
            images=image_tensor,
            do_sample=False,
            max_new_tokens=128,
            use_cache=True,
        )
    
    # è§£ç 
    caption = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()
    
    return caption


def flamingo_generate_caption(model, tokenizer, image_processor, image, device='cuda'):
    """ä½¿ç”¨OpenFlamingoç”Ÿæˆcaption"""
    # OpenFlamingo zero-shot caption prompt
    prompt = "<image>Output: A caption of this image:"
    
    # å¤„ç†å›¾åƒ
    image_tensor = image_processor(image).unsqueeze(0).to(device)
    
    # Tokenize
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)
    
    # ç”Ÿæˆ
    with torch.inference_mode():
        output_ids = model.generate(
            vision_x=image_tensor,
            lang_x=input_ids,
            max_new_tokens=64,
            num_beams=3,
        )
    
    # è§£ç 
    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    
    # æå–captionï¼ˆå»æ‰promptéƒ¨åˆ†ï¼‰
    if ":" in caption:
        caption = caption.split(":")[-1].strip()
    
    return caption


class LVLMCaptionWrapper(torch.nn.Module):
    """LVLM CaptionåŒ…è£…å™¨ - ç”¨äºAutoAttack"""
    def __init__(self, lvlm_type, model, tokenizer, image_processor, normalizer, 
                 reference_captions, device='cuda'):
        super().__init__()
        self.lvlm_type = lvlm_type
        self.model = model
        self.tokenizer = tokenizer
        self.image_processor = image_processor
        self.normalizer = normalizer
        self.reference_captions = reference_captions
        self.device = device
        
        # ä½¿ç”¨CLIPè®¡ç®—å‚è€ƒcaption embeddingsï¼ˆç®€åŒ–ï¼‰
        # åœ¨å®é™…å®ç°ä¸­åº”è¯¥ä½¿ç”¨CIDErç­‰æŒ‡æ ‡
    
    def forward(self, images):
        """
        è¿”å›è´Ÿçš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆè¶Šé«˜è¶Šå¥½çš„æ”»å‡»ï¼‰
        """
        batch_size = images.size(0)
        scores = []
        
        for i in range(batch_size):
            # åå½’ä¸€åŒ–
            img_tensor = images[i]
            img_tensor = img_tensor * self.normalizer.std.view(3, 1, 1).to(img_tensor.device) + \
                        self.normalizer.mean.view(3, 1, 1).to(img_tensor.device)
            img_tensor = torch.clamp(img_tensor, 0, 1)
            
            # è½¬PIL
            img_np = (img_tensor.cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)
            pil_image = Image.fromarray(img_np)
            
            # ç”Ÿæˆcaption
            if self.lvlm_type == 'llava':
                generated = llava_generate_caption(
                    self.model, self.tokenizer, self.image_processor,
                    pil_image, self.device
                )
            else:
                generated = flamingo_generate_caption(
                    self.model, self.tokenizer, self.image_processor,
                    pil_image, self.device
                )
            
            # è®¡ç®—ä¸å‚è€ƒcaptionsçš„ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ï¼šå…³é”®è¯overlapï¼‰
            gen_words = set(generated.lower().split())
            max_overlap = 0
            for ref_cap in self.reference_captions:
                ref_words = set(ref_cap.lower().split())
                overlap = len(gen_words & ref_words) / max(len(gen_words), 1)
                max_overlap = max(max_overlap, overlap)
            
            # è¿”å›è´Ÿåˆ†æ•°ï¼ˆæ”»å‡»ç›®æ ‡æ˜¯æœ€å°åŒ–overlapï¼‰
            scores.append(-max_overlap)
        
        return torch.tensor(scores, device=images.device).unsqueeze(1)


def attack_caption_sample(wrapper, image, eps, device):
    """
    FAREä¸¤é˜¶æ®µæ”»å‡»pipeline for Caption
    """
    # é˜¶æ®µ1: åŠç²¾åº¦APGD
    adversary_half = AutoAttack(
        wrapper,
        norm='Linf',
        eps=eps,
        version='custom',
        verbose=False
    )
    adversary_half.attacks_to_run = ['apgd-ce']
    adversary_half.apgd.n_iter = 100
    
    with torch.cuda.amp.autocast():
        adv_image = adversary_half.run_standard_evaluation(
            image.unsqueeze(0),
            torch.tensor([0]).to(device),  # dummy label
            bs=1
        )
    
    # é˜¶æ®µ2: å•ç²¾åº¦APGD
    adversary_full = AutoAttack(
        wrapper,
        norm='Linf',
        eps=eps,
        version='custom',
        verbose=False
    )
    adversary_full.attacks_to_run = ['apgd-ce']
    adversary_full.apgd.n_iter = 100
    
    with torch.enable_grad():
        adv_image = adversary_full.run_standard_evaluation(
            adv_image,
            torch.tensor([0]).to(device),
            bs=1
        )
    
    return adv_image


def compute_cider_score(generated_captions, reference_captions_dict):
    """è®¡ç®—CIDEråˆ†æ•°"""
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ç”¨äºCOCOEvalCap
        import tempfile
        
        # å‡†å¤‡resultsæ ¼å¼
        results = []
        for img_id, caption in generated_captions.items():
            results.append({
                'image_id': img_id,
                'caption': caption
            })
        
        # å‡†å¤‡annotationsæ ¼å¼
        annotations = []
        images = []
        ann_id = 0
        for img_id, captions in reference_captions_dict.items():
            images.append({'id': img_id})
            for cap in captions:
                annotations.append({
                    'image_id': img_id,
                    'id': ann_id,
                    'caption': cap
                })
                ann_id += 1
        
        # åˆ›å»ºCOCOæ ¼å¼
        coco_format = {
            'images': images,
            'annotations': annotations
        }
        
        # å†™å…¥ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(coco_format, f)
            ann_file = f.name
        
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(results, f)
            res_file = f.name
        
        # è®¡ç®—CIDEr
        coco = COCO(ann_file)
        coco_result = coco.loadRes(res_file)
        coco_eval = COCOEvalCap(coco, coco_result)
        coco_eval.evaluate()
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(ann_file)
        os.remove(res_file)
        
        return coco_eval.eval['CIDEr']
    
    except Exception as e:
        print(f"è­¦å‘Š: CIDErè®¡ç®—å¤±è´¥: {e}")
        return 0.0


def evaluate_caption_lvlm(args):
    """ä¸»è¯„ä¼°å‡½æ•°"""
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    
    print("=" * 80)
    print(f"ğŸ“Š LVLM Captionè¯„ä¼° (FAREè®¾ç½®)")
    print(f"   LVLM: {args.lvlm_type}")
    print(f"   æ•°æ®é›†: {args.dataset}")
    print(f"   CLIP: {args.clip_checkpoint}")
    print(f"   Eps: {args.eps}/255")
    print("=" * 80)
    
    # åŠ è½½LVLM
    if args.lvlm_type == 'llava':
        tokenizer, model, image_processor, context_len, is_enhanced, normalizer = get_lvlm_model(
            lvlm_type='llava',
            lvlm_path=args.lvlm_path,
            clip_checkpoint=args.clip_checkpoint,
            clip_model_name=args.clip_model_name,
            device=device
        )
    else:
        model, image_processor, tokenizer, is_enhanced, normalizer = get_lvlm_model(
            lvlm_type='flamingo',
            lvlm_path=args.lvlm_path,
            clip_checkpoint=args.clip_checkpoint,
            clip_model_name=args.clip_model_name,
            device=device
        )
    
    # åŠ è½½æ•°æ®é›†
    samples = load_caption_dataset(args.dataset, args.dataset_root, args.max_samples)
    
    # è¯„ä¼°clean captions
    print("\nğŸ”„ è¯„ä¼°å¹²å‡€æ ·æœ¬...")
    clean_captions = {}
    reference_captions = {}
    
    for sample in tqdm(samples, desc="Clean eval"):
        image = Image.open(sample['image_path']).convert('RGB')
        img_id = sample['image_id']
        
        if args.lvlm_type == 'llava':
            caption = llava_generate_caption(model, tokenizer, image_processor, image, device)
        else:
            caption = flamingo_generate_caption(model, tokenizer, image_processor, image, device)
        
        clean_captions[img_id] = caption
        reference_captions[img_id] = sample['captions']
    
    clean_cider = compute_cider_score(clean_captions, reference_captions)
    print(f"   Clean CIDEr: {clean_cider:.4f}")
    
    # è¯„ä¼°robust captions
    if args.eps > 0:
        print(f"\nğŸ”„ è¯„ä¼°å¯¹æŠ—é²æ£’æ€§ (eps={args.eps/255:.10f})...")
        
        eps_normalized = args.eps / 255.0
        robust_captions = {}
        
        for sample in tqdm(samples, desc="Robust eval"):
            image = Image.open(sample['image_path']).convert('RGB')
            img_id = sample['image_id']
            
            # é¢„å¤„ç†å›¾åƒ
            image_tensor = image_processor(image).to(device)
            
            # å½’ä¸€åŒ–
            image_normalized = normalizer(image_tensor)
            
            # åˆ›å»ºwrapper
            wrapper = LVLMCaptionWrapper(
                args.lvlm_type, model, tokenizer, image_processor,
                normalizer, sample['captions'], device
            )
            
            # æ”»å‡»
            adv_image = attack_caption_sample(wrapper, image_normalized, eps_normalized, device)
            
            # åå½’ä¸€åŒ–
            adv_image = adv_image * normalizer.std.view(1, 3, 1, 1).to(device) + \
                       normalizer.mean.view(1, 3, 1, 1).to(device)
            adv_image = torch.clamp(adv_image, 0, 1)
            
            # è½¬PIL
            adv_img_np = (adv_image[0].cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)
            adv_pil = Image.fromarray(adv_img_np)
            
            # ç”Ÿæˆcaption
            if args.lvlm_type == 'llava':
                caption = llava_generate_caption(model, tokenizer, image_processor, adv_pil, device)
            else:
                caption = flamingo_generate_caption(model, tokenizer, image_processor, adv_pil, device)
            
            robust_captions[img_id] = caption
        
        robust_cider = compute_cider_score(robust_captions, reference_captions)
        print(f"   Robust CIDEr: {robust_cider:.4f}")
        
        cider_drop = (clean_cider - robust_cider) / clean_cider * 100 if clean_cider > 0 else 0
        print(f"   CIDEr Drop: {cider_drop:.2f}%")
    else:
        robust_cider = None
        cider_drop = None
    
    # ä¿å­˜ç»“æœ
    results = {
        'lvlm_type': args.lvlm_type,
        'lvlm_path': args.lvlm_path,
        'clip_checkpoint': args.clip_checkpoint,
        'dataset': args.dataset,
        'eps': args.eps,
        'max_samples': args.max_samples,
        'clean_cider': clean_cider,
        'robust_cider': robust_cider,
        'cider_drop': cider_drop,
        'is_enhanced': is_enhanced
    }
    
    os.makedirs(args.output_dir, exist_ok=True)
    output_file = os.path.join(
        args.output_dir,
        f"{args.lvlm_type}_{args.dataset}_eps{args.eps}_results.json"
    )
    
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_file}")
    
    return results


def main():
    parser = argparse.ArgumentParser(description='LVLM Captionè¯„ä¼° (FAREè®¾ç½®)')
    
    # LVLMé…ç½®
    parser.add_argument('--lvlm_type', type=str, required=True,
                       choices=['llava', 'flamingo'],
                       help='LVLMç±»å‹')
    parser.add_argument('--lvlm_path', type=str, required=True,
                       help='LVLMæ¨¡å‹è·¯å¾„')
    
    # CLIPé…ç½®
    parser.add_argument('--clip_checkpoint', type=str, required=True,
                       help='é²æ£’CLIP checkpointè·¯å¾„')
    parser.add_argument('--clip_model_name', type=str, default='ViT-L-14',
                       help='CLIPæ¶æ„')
    
    # æ•°æ®é›†é…ç½®
    parser.add_argument('--dataset', type=str, required=True,
                       choices=['coco', 'flickr30k'],
                       help='Captionæ•°æ®é›†')
    parser.add_argument('--dataset_root', type=str, required=True,
                       help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--max_samples', type=int, default=500,
                       help='æœ€å¤§æ ·æœ¬æ•°')
    
    # æ”»å‡»é…ç½®
    parser.add_argument('--eps', type=int, default=4,
                       help='æ‰°åŠ¨å¼ºåº¦ (2 or 4 for 2/255 or 4/255)')
    
    # å…¶ä»–
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--output_dir', type=str, default='output/lvlm_caption')
    
    args = parser.parse_args()
    
    evaluate_caption_lvlm(args)


if __name__ == '__main__':
    main()
