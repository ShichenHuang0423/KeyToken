#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„é²æ£’æ€§è¯„ä¼°è„šæœ¬
æ”¯æŒè®ºæ–‡ä¸­çš„æ”»å‡»è®¾ç½®ï¼šAPGD-CE + APGD-DLR (targeted), å„100è¿­ä»£
æ”¯æŒå¢å¼ºæ¨¡å—çš„eval/attackæ¨¡å¼åˆ‡æ¢

ç”¨æ³•:
    # è¯„ä¼°åŸºçº¿æ¨¡å‹ï¼ˆFAREç­‰ï¼‰
    python tools/evaluate_robust.py --pretrained models/fare_eps_4.pt --mode baseline
    
    # è¯„ä¼°å¢å¼ºæ¨¡å‹ - å®Œæ•´é˜²å¾¡æ¨¡å¼
    python tools/evaluate_robust.py --pretrained output/stage1.pt --mode eval
    
    # è¯„ä¼°å¢å¼ºæ¨¡å‹ - æ— é˜²å¾¡æ¨¡å¼ï¼ˆä»…backboneï¼‰
    python tools/evaluate_robust.py --pretrained output/stage1.pt --mode attack
"""

import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import argparse
import torch
import torch.nn.functional as F
import open_clip
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm
import json
from datetime import datetime

from train.datasets import ImageNetDataset
from CLIP_eval.eval_utils import load_clip_model
from CLIP_eval.eval_utils_enhanced import load_enhanced_clip_model
from train.utils import AverageMeter
from open_flamingo.eval.classification_utils import IMAGENET_1K_CLASS_ID_TO_LABEL


class ClipVisionModel(torch.nn.Module):
    """CLIP Visionæ¨¡å‹åŒ…è£…å™¨"""
    def __init__(self, model, mean, std):
        super().__init__()
        self.model = model
        self.register_buffer('mean', torch.tensor(mean).view(1, 3, 1, 1))
        self.register_buffer('std', torch.tensor(std).view(1, 3, 1, 1))

    def forward(self, x, output_normalize=False):
        x = (x - self.mean) / self.std
        embedding = self.model(x)
        if output_normalize:
            embedding = F.normalize(embedding, dim=-1)
        return embedding


def get_text_embeddings(model, tokenizer, device):
    """è®¡ç®—ImageNetç±»åˆ«çš„æ–‡æœ¬åµŒå…¥"""
    class_names = [IMAGENET_1K_CLASS_ID_TO_LABEL[i] for i in range(1000)]
    templates = [
        "a photo of a {}.",
        "a blurry photo of a {}.",
        "a photo of many {}.",
        "a photo of the large {}.",
        "a photo of the small {}.",
    ]
    
    with torch.no_grad():
        text_embeddings = []
        for class_name in tqdm(class_names, desc="Computing text embeddings", leave=False):
            texts = [template.format(class_name) for template in templates]
            tokens = tokenizer(texts).to(device)
            embeddings = model.encode_text(tokens)
            embeddings = F.normalize(embeddings, dim=-1)
            text_embeddings.append(embeddings.mean(dim=0))
        
        text_embeddings = torch.stack(text_embeddings)
        text_embeddings = F.normalize(text_embeddings, dim=-1)
    
    return text_embeddings


def autoattack_eval(model, images, targets, text_embeddings, eps, iterations, 
                    device, is_enhanced=False, inference_mode='eval', ensemble_size=1, noise_std=0.01,
                    gray_box=False, randomize_defense=False):
    """
    ä½¿ç”¨è®ºæ–‡çš„æ”»å‡»è®¾ç½®ï¼šAPGD-CE + APGD-DLR (targeted)
    
    Args:
        model: æ¨¡å‹
        images: è¾“å…¥å›¾åƒ [0,1]èŒƒå›´ï¼Œå·²å½’ä¸€åŒ–
        targets: çœŸå®æ ‡ç­¾
        text_embeddings: æ–‡æœ¬åµŒå…¥
        eps: æ‰°åŠ¨åŠå¾„
        iterations: è¿­ä»£æ¬¡æ•°
        device: è®¾å¤‡
        is_enhanced: æ˜¯å¦ä¸ºå¢å¼ºæ¨¡å‹
        inference_mode: æ¨ç†æ¨¡å¼ ('eval'=å®Œæ•´é˜²å¾¡, 'attack'=æ— é˜²å¾¡)
        ensemble_size: é›†æˆæ ·æœ¬æ•°
        noise_std: éšæœºå™ªå£°æ ‡å‡†å·®
        gray_box: æ˜¯å¦ä½¿ç”¨ç°ç›’æ”»å‡»ï¼ˆAPGDåªæ”»å‡»backboneï¼Œä¸çŸ¥é“é˜²å¾¡ç­–ç•¥ï¼‰
        randomize_defense: æ˜¯å¦å¯ç”¨å†…ç½®éšæœºåŒ–é˜²å¾¡é“¾ï¼ˆæ‰“ç ´APGDæ¢¯åº¦ä¼°è®¡ï¼‰
    
    Returns:
        adv_images: å¯¹æŠ—æ ·æœ¬
    """
    from autoattack.autoattack import AutoAttack
    import torch.nn as nn
    
    # åˆ›å»ºä¸­é—´wrapperï¼šå°†modeå‚æ•°å›ºå®šï¼Œåªæ¥å—xä½œä¸ºè¾“å…¥ï¼Œé¿å…DataParallel kwargsé—®é¢˜
    if is_enhanced:
        # å…ˆè§£åŒ…DataParallel
        base_model = model.module if isinstance(model, nn.DataParallel) else model
        
        class EnhancedModelWithMode(nn.Module):
            def __init__(self, base_model, mode, ensemble_size=1, noise_std=0.01, randomize_defense=False):
                super().__init__()
                # ç›´æ¥å­˜å‚¨base modelçš„forwardæ–¹æ³•å¼•ç”¨å’Œmode
                self.base_model = base_model
                self.mode = mode
                self.ensemble_size = ensemble_size
                self.noise_std = noise_std  # éšæœºå™ªå£°æ ‡å‡†å·®
                self.randomize_defense = randomize_defense  # å†…ç½®éšæœºåŒ–é˜²å¾¡é“¾
            
            def forward(self, x):
                # è°ƒç”¨base_modelï¼Œæ‰€æœ‰å‚æ•°éƒ½ç”¨ä½ç½®ä¼ é€’é¿å…DataParallel kwargsé—®é¢˜
                # EnhancedClipVisionModel.forward(self, x, vision_clean=None, output_normalize=False, mode='train', randomize_defense=False)
                if self.ensemble_size == 1:
                    # å•æ¬¡å‰å‘ä¼ æ’­ï¼ˆå¯ç”¨å†…ç½®éšæœºåŒ–é˜²å¾¡é“¾ï¼‰
                    result = self.base_model(x, None, False, self.mode, self.randomize_defense)
                    embeddings = result[0]  # ç¬¬ä¸€ä¸ªè¿”å›å€¼æ˜¯embedding
                else:
                    # é›†æˆé˜²å¾¡ï¼šå¤šæ¬¡å‰å‘ä¼ æ’­å–å¹³å‡
                    embeddings_list = []
                    for i in range(self.ensemble_size):
                        # æ¯æ¬¡æ·»åŠ ä¸åŒçš„å°å™ªå£°ï¼ˆå…³é”®ï¼ï¼‰
                        if self.noise_std > 0:
                            noise = torch.randn_like(x) * self.noise_std
                            x_noisy = torch.clamp(x + noise, 0, 1)
                        else:
                            x_noisy = x
                        
                        # ç¬¬ä¸€æ¬¡éœ€è¦æ¢¯åº¦ï¼ˆç”¨äºæ”»å‡»ï¼‰ï¼Œåç»­ä¸éœ€è¦
                        # æ³¨æ„ï¼šæ¯æ¬¡å‰å‘ä¼ æ’­éƒ½å¯ç”¨randomize_defenseï¼Œæ¯æ¬¡é˜²å¾¡è¡Œä¸ºéƒ½ä¸åŒ
                        if i == 0:
                            result = self.base_model(x_noisy, None, False, self.mode, self.randomize_defense)
                            embeddings_list.append(result[0])
                        else:
                            with torch.no_grad():
                                result = self.base_model(x_noisy, None, False, self.mode, self.randomize_defense)
                                embeddings_list.append(result[0])
                    
                    embeddings = torch.stack(embeddings_list).mean(dim=0)
                return embeddings
        
        # ensemble_sizeå’Œnoise_stdå·²ä½œä¸ºå‚æ•°ä¼ å…¥
        # å¦‚æœä¸æ˜¯ensembleæ¨¡å¼ï¼Œå°†noise_stdè®¾ä¸º0ï¼ˆä¸æ·»åŠ å™ªå£°ï¼‰
        actual_noise_std = noise_std if ensemble_size > 1 else 0.0
        
        # ç°ç›’æ”»å‡»ï¼šAPGDç”Ÿæˆå¯¹æŠ—æ ·æœ¬æ—¶ç”¨attack modeï¼ˆæ— é˜²å¾¡ï¼‰
        attack_mode = 'attack' if gray_box else inference_mode
        
        # wrapæˆåªæ¥å—xçš„æ¨¡å‹
        model_with_mode = EnhancedModelWithMode(base_model, attack_mode, 
                                                 ensemble_size=ensemble_size,
                                                 noise_std=actual_noise_std,
                                                 randomize_defense=randomize_defense)
        # é‡æ–°wrap DataParallel
        if torch.cuda.device_count() > 1:
            model_with_mode = nn.DataParallel(model_with_mode)
        model_to_use = model_with_mode
    else:
        model_to_use = model
    
    # æœ€ç»ˆwrapperï¼šæ·»åŠ text embeddingå’Œlogit_scale
    class ModelWrapper(nn.Module):
        def __init__(self, model, text_embeddings, logit_scale=100.0):
            super().__init__()
            self.model = model
            self.register_buffer('text_embeddings', text_embeddings)
            self.logit_scale = logit_scale
        
        def forward(self, x):
            embeddings = self.model(x)
            embeddings = F.normalize(embeddings.float(), dim=-1)
            logits = (embeddings @ self.text_embeddings.T) * self.logit_scale
            return logits
    
    wrapper = ModelWrapper(model_to_use, text_embeddings)
    
    # åˆ›å»ºAutoAttackå®ä¾‹
    # è®ºæ–‡è®¾ç½®: APGD-CE + APGD-DLR (targeted), å„100è¿­ä»£
    adversary = AutoAttack(
        wrapper, 
        norm='Linf', 
        eps=eps,
        version='custom',  # ä½¿ç”¨è‡ªå®šä¹‰æ”»å‡»ç»„åˆ
        attacks_to_run=['apgd-ce', 'apgd-dlr'],  # è®ºæ–‡ä½¿ç”¨çš„ä¸¤ç§æ”»å‡»
        verbose=False,
        device=device
    )
    
    # è®¾ç½®æ”»å‡»è¿­ä»£æ¬¡æ•°å’Œå…¶ä»–å‚æ•°
    adversary.apgd.n_iter = iterations
    adversary.apgd_targeted.n_iter = iterations
    adversary.apgd.loss = 'ce'
    adversary.apgd.n_restarts = 1
    
    # è¿è¡Œæ”»å‡»
    adv_images = adversary.run_standard_evaluation(images, targets, bs=images.shape[0])
    
    return adv_images


def evaluate_model(args):
    """è¯„ä¼°æ¨¡å‹"""
    # CUDA_VISIBLE_DEVICESå·²è®¾ç½®ï¼ŒPyTorchçœ‹åˆ°çš„æ˜¯ç›¸å¯¹è®¾å¤‡ID
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    # è§£æGPU
    if torch.cuda.is_available():
        num_gpus = torch.cuda.device_count()
        gpu_ids = list(range(num_gpus))
        print(f"ğŸ® å¯ç”¨GPU: {num_gpus}å¼ å¡ (ç›¸å¯¹ID: {gpu_ids})")
    
    eps = args.eps / 255.0
    
    print("=" * 80)
    print(f"ğŸ“Š è¯„ä¼°é…ç½®:")
    print(f"   æ¨¡å‹: {args.pretrained}")
    print(f"   æ¨ç†æ¨¡å¼: {args.mode}")
    print(f"   æ”»å‡»: AutoAttack (APGD-CE + APGD-DLR targeted)")
    print(f"   è¿­ä»£: {args.iterations}")
    print(f"   eps: {args.eps}/255 = {eps:.6f}")
    print(f"   æ ·æœ¬æ•°: {args.max_samples if args.max_samples > 0 else 'å…¨éƒ¨'}")
    print("=" * 80)
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ”„ åŠ è½½æ¨¡å‹...")
    
    if args.mode == 'baseline':
        # åŸºçº¿æ¨¡å‹ï¼ˆFARE, TeCoAç­‰ï¼‰
        model, preprocessor_no_norm, normalizer = load_clip_model(args.clip_model_name, args.pretrained)
        
        # è·å–normalizeå‚æ•°
        mean = normalizer.mean
        std = normalizer.std
        
        # åŒ…è£…æ¨¡å‹
        wrapped_model = ClipVisionModel(model.visual, mean, std).to(device)
        is_enhanced = False
        
        # å¤šGPU
        if torch.cuda.device_count() > 1:
            wrapped_model = torch.nn.DataParallel(wrapped_model, device_ids=gpu_ids)
        
        # æ–‡æœ¬ç¼–ç å™¨ï¼ˆéœ€è¦ç§»åˆ°GPUç”¨äºè®¡ç®—text embeddingsï¼‰
        base_clip = model.to(device)
        tokenizer = open_clip.get_tokenizer(args.clip_model_name)
        
    else:
        # å¢å¼ºæ¨¡å‹ (evalæˆ–attackæ¨¡å¼)
        enhanced_model, preprocessor_no_norm, normalizer = load_enhanced_clip_model(
            args.clip_model_name, args.pretrained
        )
        wrapped_model = enhanced_model.to(device)
        is_enhanced = True
        
        # å¤šGPU
        if torch.cuda.device_count() > 1:
            wrapped_model = torch.nn.DataParallel(wrapped_model, device_ids=gpu_ids)
        
        # æ–‡æœ¬ç¼–ç å™¨
        base_clip, _, _ = open_clip.create_model_and_transforms(
            args.clip_model_name, pretrained='openai', device=device
        )
        tokenizer = open_clip.get_tokenizer(args.clip_model_name)
    
    wrapped_model.eval()
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (is_enhanced={is_enhanced})")
    
    # åŠ è½½æ•°æ®é›†
    print("\nğŸ”„ åŠ è½½ImageNetéªŒè¯é›†...")
    val_root = os.path.join(args.imagenet_root, 'val')
    dataset = ImageNetDataset(
        root=val_root,
        transform=preprocessor_no_norm
    )
    
    if args.max_samples > 0:
        indices = list(range(min(args.max_samples, len(dataset))))
        dataset = torch.utils.data.Subset(dataset, indices)
    
    dataloader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        num_workers=4,
        pin_memory=True
    )
    print(f"   âœ“ åŠ è½½ {len(dataset)} å¼ å›¾ç‰‡")
    
    # è®¡ç®—æ–‡æœ¬åµŒå…¥
    print("\nğŸ”„ è®¡ç®—ç±»åˆ«æ–‡æœ¬åµŒå…¥...")
    text_embeddings = get_text_embeddings(base_clip, tokenizer, device)
    print(f"   âœ“ æ–‡æœ¬åµŒå…¥è®¡ç®—å®Œæˆ")
    
    # è¯„ä¼°
    print(f"\nğŸ”„ å¼€å§‹è¯„ä¼° (batch_size={args.batch_size})...")
    
    correct_clean = 0
    correct_robust = 0
    total = 0
    
    inference_mode = args.mode if args.mode in ['eval', 'attack'] else 'eval'
    
    for batch_idx, (images, targets) in enumerate(tqdm(dataloader, desc="è¯„ä¼°è¿›åº¦")):
        images = images.to(device)
        targets = targets.to(device)
        batch_size = images.shape[0]
        
        # Cleanå‡†ç¡®ç‡
        with torch.no_grad():
            if is_enhanced:
                embeddings_clean, *_ = wrapped_model(images, mode=inference_mode)
            else:
                if isinstance(wrapped_model, torch.nn.DataParallel):
                    embeddings_clean = wrapped_model.module(images)
                else:
                    embeddings_clean = wrapped_model(images)
            
            embeddings_clean = F.normalize(embeddings_clean.float(), dim=-1)
            logits_clean = (embeddings_clean @ text_embeddings.T) * 100.0  # CLIP logit_scale
            preds_clean = logits_clean.argmax(dim=-1)
            correct_clean += (preds_clean == targets).sum().item()
        
        # Robustå‡†ç¡®ç‡ï¼ˆä½¿ç”¨AutoAttackï¼‰
        if args.attack:
            adv_images = autoattack_eval(
                wrapped_model, images, targets, text_embeddings,
                eps=eps, iterations=args.iterations,
                device=device, is_enhanced=is_enhanced,
                inference_mode=inference_mode,
                ensemble_size=args.ensemble_size,
                noise_std=args.noise_std,
                gray_box=args.gray_box,
                randomize_defense=args.randomize_defense
            )
            
            with torch.no_grad():
                if is_enhanced:
                    embeddings_adv, *_ = wrapped_model(adv_images, mode=inference_mode)
                else:
                    if isinstance(wrapped_model, torch.nn.DataParallel):
                        embeddings_adv = wrapped_model.module(adv_images)
                    else:
                        embeddings_adv = wrapped_model(adv_images)
                
                embeddings_adv = F.normalize(embeddings_adv.float(), dim=-1)
                logits_adv = (embeddings_adv @ text_embeddings.T) * 100.0  # CLIP logit_scale
                preds_adv = logits_adv.argmax(dim=-1)
                correct_robust += (preds_adv == targets).sum().item()
        
        total += batch_size
    
    clean_acc = correct_clean / total
    robust_acc = correct_robust / total if args.attack else 0.0
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"   Clean Accuracy:  {clean_acc:.4f} ({clean_acc*100:.2f}%)")
    if args.attack:
        print(f"   Robust Accuracy: {robust_acc:.4f} ({robust_acc*100:.2f}%)")
    print("=" * 80)
    
    # ä¿å­˜ç»“æœ
    os.makedirs(args.output_dir, exist_ok=True)
    
    # ç”Ÿæˆç»“æœæ–‡ä»¶åï¼ˆåŒ…å«ensembleã€éšæœºåŒ–å’Œç°ç›’æ”»å‡»ä¿¡æ¯ï¼‰
    model_name = os.path.basename(args.pretrained).replace('.pt', '')
    
    # Ensembleåç¼€
    if args.ensemble_size > 1:
        if args.noise_std > 0:
            ensemble_suffix = f"_ensemble{args.ensemble_size}_rand{args.noise_std}"
        else:
            ensemble_suffix = f"_ensemble{args.ensemble_size}_det"
    else:
        ensemble_suffix = ""
    
    # ç°ç›’æ”»å‡»åç¼€
    gray_box_suffix = "_graybox" if args.gray_box else ""
    
    result_file = os.path.join(args.output_dir, f"{model_name}_{args.mode}{ensemble_suffix}{gray_box_suffix}_results.txt")
    
    with open(result_file, 'w') as f:
        f.write(f"Model: {args.pretrained}\n")
        f.write(f"Mode: {args.mode}\n")
        if args.ensemble_size > 1:
            f.write(f"EnsembleSize: {args.ensemble_size}\n")
        f.write(f"CleanAcc: {clean_acc:.4f}\n")
        f.write(f"RobustAcc: {robust_acc:.4f}\n")
        f.write(f"Attack: AutoAttack (APGD-CE + APGD-DLR targeted)\n")
        f.write(f"Iterations: {args.iterations}\n")
        f.write(f"Eps: {args.eps}/255\n")
        f.write(f"Samples: {total}\n")
        f.write(f"Timestamp: {datetime.now().isoformat()}\n")
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    return clean_acc, robust_acc


def main():
    parser = argparse.ArgumentParser(description='ç»Ÿä¸€çš„é²æ£’æ€§è¯„ä¼°è„šæœ¬')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--clip_model_name', type=str, default='ViT-L-14',
                       help='CLIPæ¨¡å‹æ¶æ„')
    parser.add_argument('--pretrained', type=str, required=True,
                       help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
    
    # æ¨ç†æ¨¡å¼
    parser.add_argument('--mode', type=str, default='baseline',
                       choices=['baseline', 'eval', 'attack'],
                       help='æ¨ç†æ¨¡å¼: baseline=åŸºçº¿æ¨¡å‹, eval=å¢å¼ºæ¨¡å‹å®Œæ•´é˜²å¾¡, attack=å¢å¼ºæ¨¡å‹æ— é˜²å¾¡')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--imagenet_root', type=str, 
                       default='/home/ubuntu/data/KeyToken/datasets/imagenet',
                       help='ImageNetæ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=64,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max_samples', type=int, default=-1,
                       help='æœ€å¤§è¯„ä¼°æ ·æœ¬æ•° (-1è¡¨ç¤ºå…¨éƒ¨)')
    
    # æ”»å‡»å‚æ•°
    parser.add_argument('--attack', action='store_true', default=True,
                       help='æ˜¯å¦è¿›è¡Œå¯¹æŠ—æ”»å‡»è¯„ä¼°')
    parser.add_argument('--no_attack', action='store_false', dest='attack',
                       help='ä»…è¯„ä¼°Cleanå‡†ç¡®ç‡')
    parser.add_argument('--eps', type=float, default=4.0,
                       help='æ‰°åŠ¨å¹…åº¦ (å°†é™¤ä»¥255)')
    parser.add_argument('--iterations', type=int, default=100,
                       help='APGDè¿­ä»£æ¬¡æ•° (è®ºæ–‡é»˜è®¤100)')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str, default='output/robust_eval',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--ensemble_size', type=int, default=1,
                       help='é›†æˆé˜²å¾¡æ ·æœ¬æ•°ï¼ˆ1=å•æ¬¡ï¼Œ3-5=é›†æˆï¼‰')
    parser.add_argument('--noise_std', type=float, default=0.01,
                       help='éšæœºåŒ–ensembleçš„å™ªå£°æ ‡å‡†å·®ï¼ˆ0=ç¡®å®šæ€§ï¼Œ0.01=æ¨èï¼‰')
    parser.add_argument('--gray_box', action='store_true', default=False,
                       help='ç°ç›’æ”»å‡»ï¼šAPGDåªæ”»å‡»backboneï¼Œä¸çŸ¥é“é˜²å¾¡ç­–ç•¥ï¼ˆæ›´å¼ºé˜²å¾¡ï¼‰')
    parser.add_argument('--randomize_defense', action='store_true', default=False,
                       help='å¯ç”¨å†…ç½®éšæœºåŒ–é˜²å¾¡é“¾ï¼šå¯¹é˜ˆå€¼ã€ä¸Šä¸‹æ–‡æ‰©å±•ã€ç‰¹å¾èåˆæ³¨å…¥éšæœºæ€§ï¼ˆæ‰“ç ´APGDæ¢¯åº¦ä¼°è®¡ï¼‰')
    parser.add_argument('--gpu', type=str, default=None,
                       help='ä½¿ç”¨çš„GPUç¼–å·ï¼ˆä¸æŒ‡å®šåˆ™ä½¿ç”¨CUDA_VISIBLE_DEVICESç¯å¢ƒå˜é‡ï¼‰')
    
    args = parser.parse_args()
    
    # è®¾ç½®GPUï¼ˆä»…å½“æ˜ç¡®æŒ‡å®š--gpuæ—¶æ‰è¦†ç›–ç¯å¢ƒå˜é‡ï¼‰
    if args.gpu is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    
    evaluate_model(args)


if __name__ == '__main__':
    main()
