#!/usr/bin/env python3
"""
è¯„ä¼°åŸºå‡†æ¨¡å‹è„šæœ¬
ç”¨äºè¯„ä¼°OpenAI CLIPå’ŒFAREæ¨¡å‹åœ¨ImageNetéªŒè¯é›†ä¸Šçš„CleanAccå’ŒRobustAcc
"""

import sys
sys.path.insert(0, '/home/ubuntu/data/KeyToken')

import os
import argparse
import torch
import torch.nn.functional as F
import open_clip
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm

from train.datasets import ImageNetDataset
from CLIP_eval.eval_utils import load_clip_model
from train.pgd_train import pgd
from train.utils import AverageMeter
from open_flamingo.eval.classification_utils import IMAGENET_1K_CLASS_ID_TO_LABEL


class ClipVisionModel(torch.nn.Module):
    """CLIP Visionæ¨¡å‹åŒ…è£…å™¨ï¼Œåœ¨forwardå†…éƒ¨åº”ç”¨normalize"""
    def __init__(self, model, mean, std):
        super().__init__()
        self.model = model
        # å­˜å‚¨normalizeå‚æ•°è€Œä¸æ˜¯Transformå¯¹è±¡
        self.register_buffer('mean', torch.tensor(mean).view(1, 3, 1, 1))
        self.register_buffer('std', torch.tensor(std).view(1, 3, 1, 1))

    def forward(self, vision, output_normalize=False):
        # visionæ˜¯[0,1]èŒƒå›´çš„åŸå§‹å›¾åƒï¼Œæ‰‹åŠ¨normalize
        vision = (vision - self.mean) / self.std
        embedding = self.model(vision)
        if output_normalize:
            embedding = F.normalize(embedding, dim=-1)
        return embedding


def evaluate_model(args):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print("=" * 80)
    print(f"ğŸ“Š è¯„ä¼°é…ç½®:")
    print(f"   æ¨¡å‹: {args.pretrained}")
    print(f"   æ”»å‡»: {args.attack} (norm={args.norm}, eps={args.eps})")
    print(f"   æ•°æ®é›†: ImageNetéªŒè¯é›†")
    print("=" * 80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ”„ åŠ è½½æ¨¡å‹...")
    if args.pretrained == 'openai':
        clip_model, _, image_processor = open_clip.create_model_and_transforms(
            args.clip_model_name, pretrained='openai'
        )
        print("   âœ“ åŠ è½½OpenAI CLIPé¢„è®­ç»ƒæ¨¡å‹")
    else:
        # åŠ è½½FAREæˆ–å…¶ä»–æ¨¡å‹
        clip_model, _, image_processor = load_clip_model(args.clip_model_name, args.pretrained)
        print(f"   âœ“ åŠ è½½æ¨¡å‹: {args.pretrained}")
    
    # ç›´æ¥æ„å»ºé¢„å¤„ç†pipelineï¼ˆä¸åŒ…å«normalizeï¼Œä¿æŒ[0,1]èŒƒå›´ï¼‰
    preprocessor_without_normalize = transforms.Compose([
        transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
    ])
    
    # CLIPæ ‡å‡†normalizeå‚æ•°
    normalize_mean = [0.48145466, 0.4578275, 0.40821073]
    normalize_std = [0.26862954, 0.26130258, 0.27577711]
    del image_processor
    
    # ä¿å­˜å®Œæ•´CLIPæ¨¡å‹ç”¨äºæ–‡æœ¬ç¼–ç 
    clip_model = clip_model.to(device)
    clip_model.eval()
    
    # åŒ…è£…visionæ¨¡å‹ï¼Œä¼ å…¥normalizeå‚æ•°
    vision_model = ClipVisionModel(
        model=clip_model.visual,
        mean=normalize_mean,
        std=normalize_std
    )
    
    # å¤šGPUæ”¯æŒ
    if torch.cuda.device_count() > 1:
        print(f"   âœ“ ä½¿ç”¨ {torch.cuda.device_count()} å¼ GPUè¿›è¡ŒDataParallel")
        vision_model = torch.nn.DataParallel(vision_model)
    
    vision_model = vision_model.to(device)
    vision_model.eval()
    
    # åŠ è½½æ•°æ®é›†ï¼ˆä¸åŒ…å«normalizeï¼Œä¿æŒ[0,1]èŒƒå›´ä¾›PGDä½¿ç”¨ï¼‰
    print("\nğŸ”„ åŠ è½½ImageNetéªŒè¯é›†...")
    dataset = ImageNetDataset(
        root=os.path.join(args.imagenet_root, 'val'),
        transform=preprocessor_without_normalize,
    )
    dataloader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=False, 
        num_workers=4,
        pin_memory=True
    )
    print(f"   âœ“ åŠ è½½ {len(dataset)} å¼ å›¾ç‰‡")
    
    # è·å–ImageNetç±»åˆ«æ–‡æœ¬åµŒå…¥
    print("\nğŸ”„ è®¡ç®—ç±»åˆ«æ–‡æœ¬åµŒå…¥...")
    template = 'This is a photo of a {}'
    texts = [template.format(c) for c in IMAGENET_1K_CLASS_ID_TO_LABEL.values()]
    text_tokens = open_clip.tokenize(texts)
    
    with torch.no_grad():
        # åˆ†æ‰¹å¤„ç†é¿å…OOM
        embedding_text_labels_norm = []
        for el in (text_tokens[:500], text_tokens[500:]):
            # ä½¿ç”¨å®Œæ•´CLIPæ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨
            emb = clip_model.encode_text(el.to(device), normalize=True)
            embedding_text_labels_norm.append(emb.cpu())
        embedding_text_labels_norm = torch.cat(embedding_text_labels_norm).T.to(device)
    print("   âœ“ æ–‡æœ¬åµŒå…¥è®¡ç®—å®Œæˆ")
    
    # è¯„ä¼°æŒ‡æ ‡
    clean_acc_meter = AverageMeter('CleanAcc')
    robust_acc_meter = AverageMeter('RobustAcc')
    
    # è¯„ä¼°å¾ªç¯
    print(f"\nğŸ”„ å¼€å§‹è¯„ä¼° (batch_size={args.batch_size})...")
    
    # é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    total_batches = len(dataloader)
    if args.max_samples > 0:
        total_batches = min(total_batches, args.max_samples // args.batch_size)
    
    with torch.no_grad():
        for batch_idx, (data, targets) in enumerate(tqdm(dataloader, desc="è¯„ä¼°è¿›åº¦", total=total_batches)):
            if batch_idx >= total_batches:
                break
            
            data = data.to(device)
            targets = targets.to(device)
            n_samples = data.shape[0]
            
            # è°ƒè¯•ï¼šæ‰“å°æ•°æ®å½¢çŠ¶
            if batch_idx == 0:
                print(f"\n   [DEBUG] data shape: {data.shape}")
                print(f"   [DEBUG] data min/max: {data.min():.4f}/{data.max():.4f}")
        
            # 1. è¯„ä¼°Clean Accuracy
            embedding_clean = vision_model(data, output_normalize=True)
            logits_clean = embedding_clean @ embedding_text_labels_norm
            pred_clean = logits_clean.argmax(dim=1)
            clean_acc = (pred_clean == targets).float().mean().item()
            clean_acc_meter.update(clean_acc, n_samples)
            
            # 2. è¯„ä¼°Robust Accuracyï¼ˆå¦‚æœæŒ‡å®šæ”»å‡»ï¼‰
            if args.attack != 'none':
                # è®¾ç½®æ¨¡å‹ä¸ºevalæ¨¡å¼
                vision_model.eval()
                
                # å®šä¹‰æ”»å‡»æŸå¤±å‡½æ•°ï¼šæ¥æ”¶embeddingä½œä¸ºè¾“å…¥ï¼ˆä¸æ˜¯å›¾åƒï¼‰
                # loss_fn(out, targets) å…¶ä¸­outæ˜¯forwardçš„è¿”å›å€¼ï¼ˆembeddingï¼‰
                def attack_loss_fn(emb_adv, targets):
                    # è´Ÿçš„ä½™å¼¦ç›¸ä¼¼åº¦ = æœ€å¤§åŒ–è·ç¦»
                    return -F.cosine_similarity(emb_adv, embedding_clean.detach(), dim=1).mean()
                
                # ç”Ÿæˆå¯¹æŠ—æ ·æœ¬
                data_adv = pgd(
                    forward=lambda x, output_normalize: vision_model(x, output_normalize),
                    loss_fn=attack_loss_fn,  # loss_fnæ¥æ”¶embedding
                    data_clean=data,
                    targets=None,
                    norm=args.norm,
                    eps=args.eps,
                    iterations=args.iterations_adv,
                    stepsize=args.stepsize_adv,
                    output_normalize=True,
                    perturbation=torch.zeros_like(data).uniform_(-args.eps, args.eps).requires_grad_(True),
                    mode='max',
                    verbose=False
                )
                
                # è¯„ä¼°å¯¹æŠ—æ ·æœ¬
                embedding_adv = vision_model(data_adv, output_normalize=True)
                logits_adv = embedding_adv @ embedding_text_labels_norm
                pred_adv = logits_adv.argmax(dim=1)
                robust_acc = (pred_adv == targets).float().mean().item()
                robust_acc_meter.update(robust_acc, n_samples)
    
    # æ‰“å°ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š è¯„ä¼°ç»“æœ:")
    print("=" * 80)
    print(f"æ¨¡å‹: {args.pretrained}")
    print(f"CleanAcc:  {clean_acc_meter.avg:.4f}")
    if args.attack != 'none':
        print(f"RobustAcc: {robust_acc_meter.avg:.4f} (æ”»å‡»: {args.attack}, norm={args.norm}, eps={args.eps})")
    print("=" * 80)
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    output_file = os.path.join(args.output_dir, f"{os.path.basename(args.pretrained).replace('.pt', '')}_results.txt")
    os.makedirs(args.output_dir, exist_ok=True)
    with open(output_file, 'w') as f:
        f.write(f"Model: {args.pretrained}\n")
        f.write(f"CleanAcc: {clean_acc_meter.avg:.4f}\n")
        if args.attack != 'none':
            f.write(f"RobustAcc: {robust_acc_meter.avg:.4f}\n")
            f.write(f"Attack: {args.attack}, norm={args.norm}, eps={args.eps}\n")
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return {
        'clean_acc': clean_acc_meter.avg,
        'robust_acc': robust_acc_meter.avg if args.attack != 'none' else None
    }


def main():
    parser = argparse.ArgumentParser(description='è¯„ä¼°CLIPåŸºå‡†æ¨¡å‹')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--clip_model_name', type=str, default='ViT-L-14',
                       help='CLIPæ¨¡å‹æ¶æ„')
    parser.add_argument('--pretrained', type=str, required=True,
                       help='é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„æˆ–åç§° (openai, models/fare_eps_4.pt, etc.)')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--imagenet_root', type=str, 
                       default='/home/ubuntu/data/KeyToken/datasets/imagenet',
                       help='ImageNetæ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=128,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max_samples', type=int, default=-1,
                       help='æœ€å¤§è¯„ä¼°æ ·æœ¬æ•° (-1è¡¨ç¤ºå…¨éƒ¨)')
    
    # æ”»å‡»å‚æ•°
    parser.add_argument('--attack', type=str, default='pgd',
                       choices=['pgd', 'none'],
                       help='æ”»å‡»ç±»å‹')
    parser.add_argument('--norm', type=str, default='linf',
                       choices=['linf', 'l2'],
                       help='æ‰°åŠ¨èŒƒæ•°')
    parser.add_argument('--eps', type=float, default=4.0,
                       help='æ‰°åŠ¨å¹…åº¦')
    parser.add_argument('--iterations_adv', type=int, default=10,
                       help='æ”»å‡»è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--stepsize_adv', type=float, default=1.0,
                       help='æ”»å‡»æ­¥é•¿')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output_dir', type=str, default='output/baseline_eval',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--gpu', type=str, default='0',
                       help='ä½¿ç”¨çš„GPUç¼–å·ï¼Œå¤šå¡ç”¨é€—å·åˆ†éš” (ä¾‹å¦‚: 0,5,6,7)')
    
    args = parser.parse_args()
    
    # è½¬æ¢epså’Œstepsizeï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
    args.eps /= 255
    args.stepsize_adv /= 255
    
    # è®¾ç½®ä½¿ç”¨çš„GPU
    import os
    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu
    gpu_list = args.gpu.split(',')
    print(f"ğŸ® ä½¿ç”¨GPU: {args.gpu} (å…±{len(gpu_list)}å¼ å¡)")
    
    # è¯„ä¼°æ¨¡å‹
    results = evaluate_model(args)


if __name__ == '__main__':
    main()
