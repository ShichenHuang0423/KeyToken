#!/usr/bin/env python3
"""
POPEå¹»è§‰è¯„ä¼°è„šæœ¬ - Polling-based Object Probing Evaluation
è¯„ä¼°LVLMçš„å¯¹è±¡å¹»è§‰é—®é¢˜
äºŒåˆ†ç±»ä»»åŠ¡ï¼šå¯¹è±¡æ˜¯å¦åœ¨å›¾åƒä¸­
"""

import os
import sys
import argparse
import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime
from PIL import Image

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from train.adversarial_training_clip_enhanced import EnhancedClipVisionModel
from CLIP_eval.eval_utils import load_clip_model as load_baseline_clip_model
from CLIP_eval.eval_utils_enhanced import load_enhanced_clip_model
import open_clip


def load_pope_dataset(dataset_root, split='random'):
    """
    åŠ è½½POPEæ•°æ®é›†
    split: random, popular, adversarial
    """
    print(f"ğŸ”„ åŠ è½½POPEæ•°æ®é›† (split={split})...")
    
    # POPE annotationæ–‡ä»¶
    ann_file = os.path.join(dataset_root, f'coco_pope_{split}.json')
    images_dir = os.path.join(dataset_root, 'val2014')
    
    with open(ann_file, 'r') as f:
        data = json.load(f)
    
    samples = []
    for item in data:
        image_id = item['image']
        question = item['text']  # "Is there a [object] in the image?"
        answer = item['label']  # "yes" or "no"
        
        # æå–å¯¹è±¡åç§°
        object_name = question.replace("Is there a ", "").replace(" in the image?", "").strip()
        
        image_path = os.path.join(images_dir, image_id)
        
        samples.append({
            'image_path': image_path,
            'question': question,
            'object': object_name,
            'answer': answer
        })
    
    print(f"   âœ“ åŠ è½½ {len(samples)} ä¸ªæ ·æœ¬")
    return samples


class POPEModel(torch.nn.Module):
    """POPEäºŒåˆ†ç±»æ¨¡å‹ï¼ˆåŸºäºCLIPï¼‰"""
    def __init__(self, vision_model, text_model, tokenizer, 
                 is_enhanced=False, mode='eval'):
        super().__init__()
        self.vision_model = vision_model
        self.text_model = text_model
        self.tokenizer = tokenizer
        self.is_enhanced = is_enhanced
        self.mode = mode
        
        # Yes/NoåµŒå…¥
        with torch.no_grad():
            yes_no_tokens = tokenizer(["yes", "no"])
            device = next(text_model.parameters()).device
            self.answer_embeddings = F.normalize(
                text_model(yes_no_tokens.to(device)),
                dim=-1
            )
    
    def forward(self, image, question):
        """è¿”å›yes/noçš„logits"""
        # å›¾åƒç¼–ç 
        if self.is_enhanced:
            image_emb, _, _, _, _, _ = self.vision_model(image, mode=self.mode)
        else:
            # å¤„ç†DataParallelåŒ…è£…
            if isinstance(self.vision_model, torch.nn.DataParallel):
                image_emb = self.vision_model.module.encode_image(image)
            else:
                image_emb = self.vision_model.encode_image(image)
        
        image_emb = F.normalize(image_emb, dim=-1)
        
        # é—®é¢˜ç¼–ç 
        question_tokens = self.tokenizer([question]).to(image.device)
        question_emb = self.text_model(question_tokens)
        question_emb = F.normalize(question_emb, dim=-1)
        
        # å¤šæ¨¡æ€èåˆ
        multimodal_emb = (image_emb + question_emb) / 2.0
        
        # Yes/Noé¢„æµ‹
        logits = 100.0 * multimodal_emb @ self.answer_embeddings.T
        
        return logits


def evaluate_pope(model, tokenizer, samples, device, preprocess, normalizer, is_enhanced=False, mode='eval'):
    """è¯„ä¼°POPE"""
    print("ğŸ”„ å¼€å§‹POPEè¯„ä¼°...")
    
    # åˆ›å»ºPOPEæ¨¡å‹ - å¤„ç†DataParallelåŒ…è£…
    base_model = model.module if isinstance(model, torch.nn.DataParallel) else model
    if is_enhanced:
        text_model = base_model.model.encode_text
    else:
        text_model = base_model.encode_text
    
    pope_model = POPEModel(
        model, text_model, tokenizer,
        is_enhanced, mode
    ).to(device)
    pope_model.eval()
    
    correct = 0
    total = 0
    
    # ç»Ÿè®¡æŒ‡æ ‡
    true_positive = 0  # æ­£ç¡®è¯†åˆ«å­˜åœ¨çš„å¯¹è±¡
    false_positive = 0  # é”™è¯¯è¯†åˆ«ä¸å­˜åœ¨çš„å¯¹è±¡ï¼ˆå¹»è§‰ï¼‰
    true_negative = 0  # æ­£ç¡®è¯†åˆ«ä¸å­˜åœ¨çš„å¯¹è±¡
    false_negative = 0  # é”™è¯¯è¯†åˆ«å­˜åœ¨çš„å¯¹è±¡
    
    predictions = []
    
    for sample in tqdm(samples, desc="POPE eval"):
        # åŠ è½½å›¾åƒ
        image = Image.open(sample['image_path']).convert('RGB')
        image_tensor = preprocess(image).unsqueeze(0).to(device)
        image_tensor = normalizer(image_tensor)  # åº”ç”¨normalize
        
        question = sample['question']
        gt_answer = sample['answer']
        
        # é¢„æµ‹
        with torch.no_grad():
            logits = pope_model(image_tensor, question)
            pred_idx = logits.argmax(dim=-1).item()
            pred_answer = "yes" if pred_idx == 0 else "no"
            
            predictions.append({
                'question': question,
                'gt_answer': gt_answer,
                'pred_answer': pred_answer
            })
            
            # ç»Ÿè®¡
            if pred_answer == gt_answer:
                correct += 1
                if gt_answer == "yes":
                    true_positive += 1
                else:
                    true_negative += 1
            else:
                if pred_answer == "yes" and gt_answer == "no":
                    false_positive += 1  # å¹»è§‰
                elif pred_answer == "no" and gt_answer == "yes":
                    false_negative += 1
            
            total += 1
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = correct / total if total > 0 else 0
    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0
    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    # å¹»è§‰ç‡
    hallucination_rate = false_positive / total if total > 0 else 0
    
    print(f"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   Precision: {precision:.4f}")
    print(f"   Recall: {recall:.4f}")
    print(f"   F1: {f1:.4f}")
    print(f"   Hallucination Rate: {hallucination_rate:.4f} ({hallucination_rate*100:.2f}%)")
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'hallucination_rate': hallucination_rate,
        'true_positive': true_positive,
        'false_positive': false_positive,
        'true_negative': true_negative,
        'false_negative': false_negative
    }, predictions


def main():
    parser = argparse.ArgumentParser(description='POPEå¹»è§‰è¯„ä¼°')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--checkpoint', type=str, required=True)
    parser.add_argument('--clip_model_name', type=str, default='ViT-L-14',
                       help='CLIPæ¨¡å‹æ¶æ„')
    parser.add_argument('--dataset_root', type=str, required=True,
                       help='POPEæ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--split', type=str, default='random',
                       choices=['random', 'popular', 'adversarial'])
    
    # è¯„ä¼°é…ç½®
    parser.add_argument('--mode', type=str, default='eval')
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--output_dir', type=str, default='output/pope_eval')
    
    args = parser.parse_args()
    
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {args.checkpoint}")
    print(f"ğŸ“¦ CLIPæ¶æ„: {args.clip_model_name}")
    
    if 'fare' in args.checkpoint.lower() or 'tecoa' in args.checkpoint.lower():
        model, preprocess, normalizer = load_baseline_clip_model(
            args.clip_model_name, args.checkpoint
        )
        model = model.to(device)
        tokenizer = open_clip.get_tokenizer(args.clip_model_name)
        is_enhanced = False
    else:
        enhanced_model, preprocess, normalizer = load_enhanced_clip_model(
            args.clip_model_name, args.checkpoint
        )
        model = enhanced_model.to(device)
        tokenizer = open_clip.get_tokenizer(args.clip_model_name)
        is_enhanced = True
    
    # å¤šGPUæ”¯æŒ
    num_gpus = torch.cuda.device_count()
    if num_gpus > 1:
        print(f"ğŸ’» ä½¿ç”¨ {num_gpus} å¼ GPU (DataParallel)")
        model = torch.nn.DataParallel(model)
    
    model.eval()
    
    # åŠ è½½æ•°æ®é›†
    samples = load_pope_dataset(args.dataset_root, args.split)
    
    # è¯„ä¼°
    print("=" * 80)
    print(f"ğŸ“Š POPEå¹»è§‰è¯„ä¼°")
    print(f"   Split: {args.split}")
    print(f"   æ¨¡å‹: {args.checkpoint}")
    print("=" * 80)
    
    metrics, predictions = evaluate_pope(
        model, tokenizer, samples, device,
        preprocess, normalizer, is_enhanced, args.mode
    )
    
    # ä¿å­˜ç»“æœ
    os.makedirs(args.output_dir, exist_ok=True)
    model_name = os.path.basename(args.checkpoint).replace('.pt', '')
    
    result_file = os.path.join(
        args.output_dir,
        f"{model_name}_pope_{args.split}_results.json"
    )
    
    results = {
        'model': args.checkpoint,
        'split': args.split,
        'mode': args.mode,
        'metrics': metrics,
        'num_samples': len(samples),
        'timestamp': datetime.now().isoformat()
    }
    
    with open(result_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    # ä¿å­˜è¯¦ç»†é¢„æµ‹
    pred_file = os.path.join(
        args.output_dir,
        f"{model_name}_pope_{args.split}_predictions.json"
    )
    with open(pred_file, 'w') as f:
        json.dump(predictions, f, indent=2)
    
    print("=" * 80)
    print(f"âœ… ç»“æœå·²ä¿å­˜: {result_file}")
    print(f"âœ… é¢„æµ‹å·²ä¿å­˜: {pred_file}")


if __name__ == '__main__':
    main()
