#!/usr/bin/env python3
"""
VQAé²æ£’æ€§è¯„ä¼°è„šæœ¬ - åŸºäºFAREè®ºæ–‡è®¾ç½®
æ”¯æŒVQAv2å’ŒTextVQAæ•°æ®é›†
æ”»å‡»pipeline: APGDåŠç²¾åº¦100iter -> å•ç²¾åº¦æ”»å‡» -> Targetedæ”»å‡»
Eps: 2/255 and 4/255
"""

import os
import sys
import argparse
import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
import json
from datetime import datetime
from PIL import Image

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from train.adversarial_training_clip_enhanced import EnhancedClipVisionModel
from CLIP_eval.eval_utils import load_clip_model as load_baseline_clip_model
from CLIP_eval.eval_utils_enhanced import load_enhanced_clip_model
from autoattack import AutoAttack
import open_clip


def load_vqa_dataset(dataset_name, dataset_root, split='val', max_samples=500):
    """åŠ è½½VQAæ•°æ®é›†"""
    print(f"ğŸ”„ åŠ è½½{dataset_name}æ•°æ®é›†...")
    
    if dataset_name == 'vqav2':
        # VQAv2æ ¼å¼
        questions_file = os.path.join(dataset_root, f'v2_OpenEnded_mscoco_{split}2014_questions.json')
        annotations_file = os.path.join(dataset_root, f'v2_mscoco_{split}2014_annotations.json')
        images_dir = os.path.join(dataset_root, f'{split}2014')
        
        with open(questions_file, 'r') as f:
            questions_data = json.load(f)
        with open(annotations_file, 'r') as f:
            annotations_data = json.load(f)
        
        # æ„å»ºæ ·æœ¬
        samples = []
        for q, a in zip(questions_data['questions'], annotations_data['annotations']):
            image_id = q['image_id']
            image_path = os.path.join(images_dir, f"COCO_{split}2014_{image_id:012d}.jpg")
            
            samples.append({
                'image_path': image_path,
                'question': q['question'],
                'answer': a['multiple_choice_answer'],
                'all_answers': [ans['answer'] for ans in a['answers']]
            })
    
    elif dataset_name == 'textvqa':
        # TextVQAæ ¼å¼
        annotations_file = os.path.join(dataset_root, f'TextVQA_{split}.json')
        images_dir = os.path.join(dataset_root, 'train_images')
        
        with open(annotations_file, 'r') as f:
            data = json.load(f)
        
        samples = []
        for item in data['data']:
            samples.append({
                'image_path': os.path.join(images_dir, item['image_id'] + '.jpg'),
                'question': item['question'],
                'answer': item['answers'][0],
                'all_answers': item['answers']
            })
    
    # éšæœºé‡‡æ ·
    if max_samples > 0 and len(samples) > max_samples:
        indices = np.random.choice(len(samples), max_samples, replace=False)
        samples = [samples[i] for i in indices]
    
    print(f"   âœ“ åŠ è½½ {len(samples)} ä¸ªæ ·æœ¬")
    return samples


def vqa_accuracy(pred_answer, gt_answers):
    """è®¡ç®—VQAå‡†ç¡®ç‡ï¼ˆè€ƒè™‘å¤šä¸ªground truthï¼‰"""
    pred_answer = pred_answer.lower().strip()
    
    # VQAè¯„ä¼°è§„åˆ™ï¼šè‡³å°‘3ä¸ªæ ‡æ³¨è€…ç»™å‡ºç›¸åŒç­”æ¡ˆæ‰ç®—æ­£ç¡®
    answer_counts = {}
    for ans in gt_answers:
        ans = ans.lower().strip()
        answer_counts[ans] = answer_counts.get(ans, 0) + 1
    
    for ans, count in answer_counts.items():
        if pred_answer == ans and count >= 3:
            return 1.0
        elif pred_answer == ans:
            return count / 3.0
    
    return 0.0


class VQAModel(torch.nn.Module):
    """VQAæ¨¡å‹åŒ…è£…å™¨ï¼ˆCLIP + ç®€å•ç­”æ¡ˆé¢„æµ‹ï¼‰"""
    def __init__(self, vision_model, text_model, tokenizer, answer_vocab, 
                 is_enhanced=False, mode='eval', gray_box=False):
        super().__init__()
        self.vision_model = vision_model
        self.text_model = text_model
        self.tokenizer = tokenizer
        self.answer_vocab = answer_vocab
        self.is_enhanced = is_enhanced
        self.mode = mode
        self.gray_box = gray_box
        
        # ç­”æ¡ˆåµŒå…¥
        with torch.no_grad():
            answer_texts = tokenizer(answer_vocab)
            self.answer_embeddings = F.normalize(
                text_model(answer_texts.to(next(text_model.parameters()).device)),
                dim=-1
            )
    
    def forward(self, image, question_text):
        """å‰å‘ä¼ æ’­"""
        # å›¾åƒç¼–ç 
        if self.is_enhanced:
            if self.gray_box:
                image_emb, _, _, _, _, _ = self.vision_model(image, mode='attack')
            else:
                image_emb, _, _, _, _, _ = self.vision_model(image, mode=self.mode)
        else:
            # å¤„ç†DataParallelåŒ…è£…
            if isinstance(self.vision_model, torch.nn.DataParallel):
                image_emb = self.vision_model.module.encode_image(image)
            else:
                image_emb = self.vision_model.encode_image(image)
        
        image_emb = F.normalize(image_emb, dim=-1)
        
        # é—®é¢˜ç¼–ç 
        question_tokens = self.tokenizer([question_text]).to(image.device)
        question_emb = self.text_model(question_tokens)
        question_emb = F.normalize(question_emb, dim=-1)
        
        # å¤šæ¨¡æ€èåˆï¼ˆç®€å•æ‹¼æ¥ï¼‰
        multimodal_emb = (image_emb + question_emb) / 2.0
        
        # ç­”æ¡ˆé¢„æµ‹
        logits = 100.0 * multimodal_emb @ self.answer_embeddings.T
        
        return logits


def attack_pipeline_vqa(vqa_model, image, question, answer_idx, eps, device):
    """
    FAREæ”»å‡»pipeline for VQA:
    1. APGDåŠç²¾åº¦100 iter
    2. æ£€æŸ¥é˜ˆå€¼ï¼Œå¦‚æœæœªè¾¾åˆ°åˆ™ç»§ç»­
    3. APGDå•ç²¾åº¦æ”»å‡»
    4. Targetedæ”»å‡»ï¼ˆå•ç²¾åº¦ï¼‰
    """
    threshold = 0.5  # åˆ†æ•°é˜ˆå€¼
    
    # é˜¶æ®µ1: åŠç²¾åº¦APGD
    adversary_half = AutoAttack(
        vqa_model,
        norm='Linf',
        eps=eps,
        version='custom',
        verbose=False
    )
    adversary_half.attacks_to_run = ['apgd-ce']
    adversary_half.apgd.n_iter = 100
    
    with torch.cuda.amp.autocast():
        adv_image = adversary_half.run_standard_evaluation(
            image.unsqueeze(0),
            torch.tensor([answer_idx]).to(device),
            bs=1
        )
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æ”»å‡»
    with torch.no_grad():
        logits = vqa_model(adv_image, question)
        score = F.softmax(logits, dim=-1)[0, answer_idx].item()
    
    if score < threshold:
        return adv_image  # æ”»å‡»æˆåŠŸï¼Œæå‰è¿”å›
    
    # é˜¶æ®µ2: å•ç²¾åº¦APGD
    adversary_full = AutoAttack(
        vqa_model,
        norm='Linf',
        eps=eps,
        version='custom',
        verbose=False
    )
    adversary_full.attacks_to_run = ['apgd-ce']
    adversary_full.apgd.n_iter = 100
    
    with torch.enable_grad():
        adv_image = adversary_full.run_standard_evaluation(
            adv_image,
            torch.tensor([answer_idx]).to(device),
            bs=1
        )
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦targetedæ”»å‡»
    with torch.no_grad():
        logits = vqa_model(adv_image, question)
        score = F.softmax(logits, dim=-1)[0, answer_idx].item()
    
    if score < threshold:
        return adv_image
    
    # é˜¶æ®µ3: Targetedæ”»å‡»
    adversary_targeted = AutoAttack(
        vqa_model,
        norm='Linf',
        eps=eps,
        version='custom',
        verbose=False
    )
    adversary_targeted.attacks_to_run = ['apgd-dlr']
    adversary_targeted.apgd_targeted.n_iter = 100
    
    with torch.enable_grad():
        adv_image = adversary_targeted.run_standard_evaluation(
            adv_image,
            torch.tensor([answer_idx]).to(device),
            bs=1
        )
    
    return adv_image


def evaluate_vqa(model, tokenizer, samples, answer_vocab, eps, device,
                preprocess, normalizer, is_enhanced=False, mode='eval', gray_box=False):
    """è¯„ä¼°VQA"""
    print(f"ğŸ”„ å¼€å§‹VQAè¯„ä¼° (eps={eps})...")
    
    # åˆ›å»ºVQAæ¨¡å‹ - å¤„ç†DataParallelåŒ…è£…
    base_model = model.module if isinstance(model, torch.nn.DataParallel) else model
    if is_enhanced:
        text_model = base_model.model.encode_text
    else:
        text_model = base_model.encode_text
    
    vqa_model = VQAModel(
        model, text_model, tokenizer, answer_vocab,
        is_enhanced, mode, gray_box
    ).to(device)
    vqa_model.eval()
    
    # è·å–ç­”æ¡ˆç´¢å¼•æ˜ å°„
    answer_to_idx = {ans: i for i, ans in enumerate(answer_vocab)}
    
    clean_correct = 0
    robust_correct = 0
    total = 0
    
    for sample in tqdm(samples, desc="VQA eval"):
        # åŠ è½½å›¾åƒ
        image = Image.open(sample['image_path']).convert('RGB')
        image_tensor = preprocess(image).unsqueeze(0).to(device)
        image_tensor = normalizer(image_tensor)  # åº”ç”¨normalize
        
        question = sample['question']
        gt_answer = sample['answer']
        gt_answers = sample['all_answers']
        
        # æ‰¾åˆ°ground truthåœ¨è¯è¡¨ä¸­çš„ç´¢å¼•
        if gt_answer not in answer_to_idx:
            continue  # è·³è¿‡ä¸åœ¨è¯è¡¨ä¸­çš„ç­”æ¡ˆ
        
        answer_idx = answer_to_idx[gt_answer]
        
        # å¹²å‡€æ ·æœ¬è¯„ä¼°
        with torch.no_grad():
            logits = vqa_model(image_tensor, question)
            pred_idx = logits.argmax(dim=-1).item()
            pred_answer = answer_vocab[pred_idx]
            
            acc = vqa_accuracy(pred_answer, gt_answers)
            clean_correct += acc
        
        # å¯¹æŠ—æ ·æœ¬è¯„ä¼°
        with torch.enable_grad():
            adv_image = attack_pipeline_vqa(
                vqa_model, image_tensor, question, answer_idx, eps, device
            )
        
        with torch.no_grad():
            logits = vqa_model(adv_image, question)
            pred_idx = logits.argmax(dim=-1).item()
            pred_answer = answer_vocab[pred_idx]
            
            acc = vqa_accuracy(pred_answer, gt_answers)
            robust_correct += acc
        
        total += 1
    
    clean_acc = clean_correct / total if total > 0 else 0
    robust_acc = robust_correct / total if total > 0 else 0
    
    print(f"   Clean Accuracy: {clean_acc:.4f} ({clean_acc*100:.2f}%)")
    print(f"   Robust Accuracy: {robust_acc:.4f} ({robust_acc*100:.2f}%)")
    
    return clean_acc, robust_acc


def build_answer_vocabulary(samples, top_k=3000):
    """æ„å»ºç­”æ¡ˆè¯è¡¨ï¼ˆå–æœ€å¸¸è§çš„ç­”æ¡ˆï¼‰"""
    from collections import Counter
    
    answer_counts = Counter()
    for sample in samples:
        for ans in sample['all_answers']:
            answer_counts[ans.lower().strip()] += 1
    
    # å–top_kæœ€å¸¸è§ç­”æ¡ˆ
    most_common = answer_counts.most_common(top_k)
    answer_vocab = [ans for ans, _ in most_common]
    
    print(f"   âœ“ ç­”æ¡ˆè¯è¡¨å¤§å°: {len(answer_vocab)}")
    return answer_vocab


def main():
    parser = argparse.ArgumentParser(description='VQAé²æ£’æ€§è¯„ä¼°')
    
    # æ¨¡å‹é…ç½®
    parser.add_argument('--checkpoint', type=str, required=True)
    parser.add_argument('--clip_model_name', type=str, default='ViT-L-14',
                       help='CLIPæ¨¡å‹æ¶æ„')
    parser.add_argument('--dataset', type=str, default='vqav2',
                       choices=['vqav2', 'textvqa'])
    parser.add_argument('--dataset_root', type=str, required=True)
    
    # æ”»å‡»é…ç½®
    parser.add_argument('--eps', type=float, default=4.0)
    parser.add_argument('--gray_box', action='store_true')
    
    # è¯„ä¼°é…ç½®
    parser.add_argument('--mode', type=str, default='eval')
    parser.add_argument('--max_samples', type=int, default=500)
    parser.add_argument('--device', type=str, default='cuda')
    parser.add_argument('--output_dir', type=str, default='output/vqa_eval')
    
    args = parser.parse_args()
    
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ¨¡å‹
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {args.checkpoint}")
    print(f"ğŸ“¦ CLIPæ¶æ„: {args.clip_model_name}")
    
    if 'fare' in args.checkpoint.lower() or 'tecoa' in args.checkpoint.lower():
        # ä½¿ç”¨ç»Ÿä¸€çš„åŠ è½½å‡½æ•°
        model, preprocess, normalizer = load_baseline_clip_model(
            args.clip_model_name, args.checkpoint
        )
        model = model.to(device)
        tokenizer = open_clip.get_tokenizer(args.clip_model_name)
        is_enhanced = False
    else:
        enhanced_model, preprocess, normalizer = load_enhanced_clip_model(
            args.clip_model_name, args.checkpoint
        )
        model = enhanced_model.to(device)
        tokenizer = open_clip.get_tokenizer(args.clip_model_name)
        is_enhanced = True
    
    # å¤šGPUæ”¯æŒ
    num_gpus = torch.cuda.device_count()
    if num_gpus > 1:
        print(f"ğŸ’» ä½¿ç”¨ {num_gpus} å¼ GPU (DataParallel)")
        model = torch.nn.DataParallel(model)
    
    model.eval()
    
    # åŠ è½½æ•°æ®é›†
    samples = load_vqa_dataset(
        args.dataset, args.dataset_root, 'val', args.max_samples
    )
    
    # æ„å»ºç­”æ¡ˆè¯è¡¨
    answer_vocab = build_answer_vocabulary(samples)
    
    # è¯„ä¼°
    print("=" * 80)
    print(f"ğŸ“Š VQAé²æ£’æ€§è¯„ä¼°")
    print(f"   æ•°æ®é›†: {args.dataset}")
    print(f"   æ¨¡å‹: {args.checkpoint}")
    print(f"   Eps: {args.eps}/255")
    print("=" * 80)
    
    eps_normalized = args.eps / 255.0
    clean_acc, robust_acc = evaluate_vqa(
        model, tokenizer, samples, answer_vocab, eps_normalized, device,
        preprocess, normalizer, is_enhanced, args.mode, args.gray_box
    )
    
    # ä¿å­˜ç»“æœ
    os.makedirs(args.output_dir, exist_ok=True)
    model_name = os.path.basename(args.checkpoint).replace('.pt', '')
    eps_str = f"eps{int(args.eps)}"
    gray_str = "_graybox" if args.gray_box else ""
    
    result_file = os.path.join(
        args.output_dir,
        f"{model_name}_{args.dataset}_{eps_str}{gray_str}_results.json"
    )
    
    results = {
        'model': args.checkpoint,
        'dataset': args.dataset,
        'mode': args.mode,
        'eps': args.eps,
        'gray_box': args.gray_box,
        'clean_acc': clean_acc,
        'robust_acc': robust_acc,
        'num_samples': len(samples),
        'timestamp': datetime.now().isoformat()
    }
    
    with open(result_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print("=" * 80)
    print(f"âœ… ç»“æœå·²ä¿å­˜: {result_file}")


if __name__ == '__main__':
    main()
