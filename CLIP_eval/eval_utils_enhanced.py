"""
å¢å¼ºç‰ˆCLIPè¯„ä¼°å·¥å…·
æ”¯æŒåŠ è½½å®Œæ•´çš„EnhancedClipVisionModelï¼ˆåŒ…å«æ‰€æœ‰è®­ç»ƒçš„å¢å¼ºæ¨¡å—ï¼‰
ç”¨äºæ¨ç†æ—¶ä½¿ç”¨PatchDisturbDetectorã€KeyTokenSelectorç­‰æ¨¡å—
"""

import sys
import os
import torch
import open_clip
from torchvision import transforms

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from train.adversarial_training_clip_enhanced import EnhancedClipVisionModel


def load_enhanced_clip_model(clip_model_name, checkpoint_path, args=None):
    """
    åŠ è½½å®Œæ•´çš„å¢å¼ºCLIPæ¨¡å‹ï¼ˆåŒ…å«æ‰€æœ‰å¢å¼ºæ¨¡å—ï¼‰
    
    Args:
        clip_model_name: CLIPæ¨¡å‹åç§°ï¼ˆå¦‚'ViT-L-14'ï¼‰
        checkpoint_path: checkpointè·¯å¾„
        args: è®­ç»ƒæ—¶çš„å‚æ•°ï¼ˆä»checkpointè‡ªåŠ¨åŠ è½½ï¼Œæˆ–æ‰‹åŠ¨æŒ‡å®šï¼‰
    
    Returns:
        enhanced_model: å®Œæ•´çš„EnhancedClipVisionModel
        preprocessor_no_norm: å›¾åƒé¢„å¤„ç†ï¼ˆä¸å«normalizeï¼‰
        normalizer: normalizeå˜æ¢
    """
    print(f"ğŸ“¦ åŠ è½½å¢å¼ºCLIPæ¨¡å‹...")
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # è·å–è®­ç»ƒå‚æ•°
    if args is None:
        if 'args' not in checkpoint:
            raise ValueError("Checkpointä¸­æ²¡æœ‰argsï¼Œè¯·æ‰‹åŠ¨æä¾›argså‚æ•°")
        # æ­£ç¡®åˆ›å»ºargså¯¹è±¡ï¼šä½¿ç”¨argparse.Namespaceæˆ–SimpleNamespace
        from argparse import Namespace
        args = Namespace(**checkpoint['args'])
    
    # ä½¿ç”¨checkpointä¸­çš„clip_model_nameï¼ˆä¼˜å…ˆçº§é«˜äºå‡½æ•°å‚æ•°ï¼‰
    if hasattr(args, 'clip_model_name'):
        actual_model_name = args.clip_model_name
        print(f"ğŸ“¦ ä»checkpointè¯»å–æ¨¡å‹æ¶æ„: {actual_model_name}")
    else:
        actual_model_name = clip_model_name
        print(f"âš ï¸  ä½¿ç”¨é»˜è®¤æ¨¡å‹æ¶æ„: {actual_model_name}")
    
    # åˆ›å»ºåŸºç¡€CLIPæ¨¡å‹
    print(f"ğŸ”§ åˆ›å»ºæ¨¡å‹ï¼Œæ¶æ„åç§°: {actual_model_name}")
    base_model, _, image_processor = open_clip.create_model_and_transforms(
        actual_model_name, pretrained='openai', device='cpu'
    )
    
    # éªŒè¯åˆ›å»ºçš„æ¨¡å‹ç»´åº¦
    if hasattr(base_model, 'visual') and hasattr(base_model.visual, 'transformer'):
        if hasattr(base_model.visual.transformer, 'width'):
            print(f"ğŸ” åˆ›å»ºçš„æ¨¡å‹ç»´åº¦: {base_model.visual.transformer.width}")
        layer_count = len(base_model.visual.transformer.resblocks)
        print(f"ğŸ” Transformerå±‚æ•°: {layer_count}")
    
    # å›¾åƒé¢„å¤„ç†
    preprocessor_no_norm = transforms.Compose(image_processor.transforms[:-1])
    normalizer = image_processor.transforms[-1]
    
    # åˆ›å»ºEnhancedClipVisionModel (éœ€è¦ä¼ å…¥visual encoderå’Œnormalize)
    # æ³¨æ„ï¼šEnhancedClipVisionModelæœŸæœ›çš„æ˜¯visual encoderï¼Œä¸æ˜¯å®Œæ•´CLIPæ¨¡å‹
    enhanced_model = EnhancedClipVisionModel(base_model.visual, args, normalizer)
    
    # åŠ è½½æƒé‡
    if 'enhanced_model_state_dict' in checkpoint:
        enhanced_model.load_state_dict(checkpoint['enhanced_model_state_dict'])
        print(f"âœ… åŠ è½½å®Œæ•´å¢å¼ºæ¨¡å‹ï¼ˆåŒ…å«æ‰€æœ‰å¢å¼ºæ¨¡å—ï¼‰")
    else:
        enhanced_model.model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âš ï¸  ä»…åŠ è½½åŸºç¡€CLIPæƒé‡ï¼ˆå¢å¼ºæ¨¡å—æœªè®­ç»ƒï¼‰")
    
    enhanced_model.eval()
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   - PatchDisturbDetector: {'âœ…' if args.use_key_token_protection else 'âŒ'}")
    print(f"   - KeyTokenSelector: {'âœ…' if args.use_key_token_protection else 'âŒ'}")
    print(f"   - DualMAEDecoder: {'âœ…' if args.use_mae_recon else 'âŒ'}")
    
    return enhanced_model, preprocessor_no_norm, normalizer


def load_clip_model_for_inference(checkpoint_path, clip_model_name='ViT-L-14', 
                                   use_enhanced_modules=True):
    """
    æ¨ç†æ—¶åŠ è½½CLIPæ¨¡å‹
    
    Args:
        checkpoint_path: checkpointè·¯å¾„
        clip_model_name: CLIPæ¨¡å‹åç§°
        use_enhanced_modules: æ˜¯å¦ä½¿ç”¨å¢å¼ºæ¨¡å—ï¼ˆTrue=ä½¿ç”¨å¢å¼ºæ¨ç†ï¼ŒFalse=ä»…åŸºç¡€CLIPï¼‰
    
    Returns:
        model: CLIPæ¨¡å‹ï¼ˆå¯èƒ½æ˜¯EnhancedClipVisionModelæˆ–åŸºç¡€CLIPï¼‰
        preprocessor_no_norm: å›¾åƒé¢„å¤„ç†
        normalizer: normalizeå˜æ¢
    """
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    if use_enhanced_modules and 'enhanced_model_state_dict' in checkpoint:
        # ä½¿ç”¨å¢å¼ºæ¨¡å—
        return load_enhanced_clip_model(clip_model_name, checkpoint_path)
    else:
        # ä»…ä½¿ç”¨åŸºç¡€CLIP
        print(f"ğŸ“¦ åŠ è½½åŸºç¡€CLIPæ¨¡å‹ï¼ˆä¸ä½¿ç”¨å¢å¼ºæ¨¡å—ï¼‰")
        model, _, image_processor = open_clip.create_model_and_transforms(
            clip_model_name, pretrained='openai', device='cpu'
        )
        
        if 'model_state_dict' in checkpoint:
            model.visual.load_state_dict(checkpoint['model_state_dict'])
        elif 'enhanced_model_state_dict' in checkpoint:
            # ä»å¢å¼ºæ¨¡å‹ä¸­æå–åŸºç¡€CLIPæƒé‡
            enhanced_state = checkpoint['enhanced_model_state_dict']
            clip_state = {k.replace('model.', ''): v 
                         for k, v in enhanced_state.items() 
                         if k.startswith('model.')}
            model.visual.load_state_dict(clip_state)
        
        model.eval()
        preprocessor_no_norm = transforms.Compose(image_processor.transforms[:-1])
        normalizer = image_processor.transforms[-1]
        
        print(f"âœ… åŸºç¡€CLIPæ¨¡å‹åŠ è½½å®Œæˆ")
        return model, preprocessor_no_norm, normalizer


@torch.no_grad()
def enhanced_inference(enhanced_model, images, mode='eval'):
    """
    ä½¿ç”¨å¢å¼ºæ¨¡å—è¿›è¡Œæ¨ç†
    
    Args:
        enhanced_model: EnhancedClipVisionModel
        images: è¾“å…¥å›¾åƒ tensor (B, C, H, W)
        mode: 'eval' = æ¨ç†æ¨¡å¼ï¼ˆä½¿ç”¨å¢å¼ºæ¨¡å—ï¼‰
    
    Returns:
        embeddings: å›¾åƒembedding (B, dim)
        extra_info: é¢å¤–ä¿¡æ¯ï¼ˆæ‰°åŠ¨åˆ†æ•°ã€å…³é”®tokenç­‰ï¼‰
    """
    # ä½¿ç”¨EnhancedClipVisionModelçš„forward
    embeddings, extra_info = enhanced_model(images, mode=mode)
    
    return embeddings, extra_info


# å‘åå…¼å®¹ï¼šå¯¼å‡ºæ ‡å‡†æ¥å£
def load_clip_model(clip_model_name, pretrained, beta=0.):
    """
    æ ‡å‡†æ¥å£ï¼šåŠ è½½CLIPæ¨¡å‹ï¼ˆè‡ªåŠ¨æ£€æµ‹æ˜¯å¦ä¸ºå¢å¼ºæ¨¡å‹ï¼‰
    å…¼å®¹åŸæœ‰çš„eval_utils.pyæ¥å£
    """
    if isinstance(pretrained, str) and os.path.exists(pretrained):
        checkpoint = torch.load(pretrained, map_location='cpu')
        
        # æ£€æµ‹æ˜¯å¦ä¸ºå¢å¼ºæ¨¡å‹
        if 'enhanced_model_state_dict' in checkpoint:
            print("ğŸ” æ£€æµ‹åˆ°å¢å¼ºæ¨¡å‹checkpoint")
            return load_enhanced_clip_model(clip_model_name, pretrained)
    
    # å¦åˆ™ä½¿ç”¨æ ‡å‡†åŠ è½½æ–¹å¼
    from CLIP_eval.eval_utils import load_clip_model as load_clip_model_original
    return load_clip_model_original(clip_model_name, pretrained, beta)
