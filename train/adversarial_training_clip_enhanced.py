"""
å¢å¼ºç‰ˆCLIPå¯¹æŠ—è®­ç»ƒ
é›†æˆMAEé‡å»º+å…³é”®Tokenä¿æŠ¤
"""

import sys
sys.path.append("open_flamingo")

import os
import shutil
import time
import argparse

import numpy as np
import open_clip
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from training.scheduler import cosine_lr
from torchvision import transforms
import wandb
from tqdm import tqdm

from train.datasets import COCOFlickrDataset, ImageNetDataset
from CLIP_eval.eval_utils import load_clip_model
from open_flamingo.eval.classification_utils import IMAGENET_1K_CLASS_ID_TO_LABEL
from train.pgd_train import pgd
from train.apgd_train import apgd_train as apgd
from train.utils import init_wandb, AverageMeter, str2bool
from train.sam_data import SamData
from open_flamingo.eval.models.utils import unwrap_model

# å¯¼å…¥æ–°æ¨¡å—
from train.disturb_detector import PatchDisturbDetector, TokenDisturbDetector
from train.key_token_selector import KeyTokenSelector, AdaptiveKeyTokenSelector
from train.mae_decoder import DualMAEDecoder
from train.keytoken_loss import KeyTokenLoss, compute_keytoken_loss

# è§£æå‚æ•°
parser = argparse.ArgumentParser()
# åŸæœ‰å‚æ•°
parser.add_argument('--clip_model_name', type=str, default='ViT-L-14')
parser.add_argument('--pretrained', type=str, default='openai')
parser.add_argument('--dataset', type=str, default='imagenet')
parser.add_argument('--template', type=str, default='std')
parser.add_argument('--imagenet_root', type=str, default='/mnt/datasets/imagenet')
parser.add_argument('--output_normalize', type=str2bool, default=False)
parser.add_argument('--start_step', type=int, default=0)
parser.add_argument('--optimizer_state', type=str, default='')
parser.add_argument('--steps', type=int, default=20000)
parser.add_argument('--warmup', type=int, default=1400)
parser.add_argument('--batch_size', type=int, default=256)
parser.add_argument('--loss', type=str, default='l2')
parser.add_argument('--loss_clean', type=str, default='none')
parser.add_argument('--clean_weight', type=float, default=0.)
parser.add_argument('--trades', type=str2bool, default=False)
parser.add_argument('--opt', type=str, default='adamw')
parser.add_argument('--momentum_sgd', type=float, default=0.9)
parser.add_argument('--lr', type=float, default=1e-5)
parser.add_argument('--wd', type=float, default=1e-4)
parser.add_argument('--attack', type=str, default='apgd')
parser.add_argument('--inner_loss', type=str, default='l2')
parser.add_argument('--norm', type=str, default='linf')
parser.add_argument('--eps', type=float, default=4)
parser.add_argument('--iterations_adv', type=int, default=10)
parser.add_argument('--stepsize_adv', type=float, default=1.)
parser.add_argument('--wandb', type=str2bool, default=True)
parser.add_argument('--experiment_name', type=str, default='')
parser.add_argument('--overwrite', type=str2bool, default=False)
parser.add_argument('--log_freq', type=int, default=1)
parser.add_argument('--eval_freq', type=int, default=50)
parser.add_argument('--output_dir', type=str, default='')
parser.add_argument('--save_checkpoints', type=str2bool, default=True)
parser.add_argument('--devices', type=str, default='')

# æ–°å¢å‚æ•°
parser.add_argument('--use_mae_recon', type=str2bool, default=True, help='ä½¿ç”¨MAEé‡å»ºä»»åŠ¡')
parser.add_argument('--use_key_token_protection', type=str2bool, default=True, help='ä½¿ç”¨å…³é”®Tokenä¿æŠ¤')
parser.add_argument('--mae_weight', type=float, default=0.1, help='MAEé‡å»ºæŸå¤±æƒé‡')
parser.add_argument('--text_recon_weight', type=float, default=0.8, help='æ–‡æœ¬é‡å»ºæŸå¤±æƒé‡')
parser.add_argument('--mask_ratio', type=float, default=0.5, help='åŠ¨æ€æ©ç æ¯”ä¾‹')
parser.add_argument('--key_token_ratio', type=float, default=0.2, help='å…³é”®Tokenä¿ç•™æ¯”ä¾‹')
parser.add_argument('--adaptive_masking', type=str2bool, default=False, help='ä½¿ç”¨è‡ªé€‚åº”æ©ç ')

# æ–­ç‚¹ç»­è¿å‚æ•°
parser.add_argument('--resume', type=str, default='', help='ä»æŒ‡å®šcheckpointæ¢å¤è®­ç»ƒï¼ˆautoè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹æœ€æ–°ï¼‰')
parser.add_argument('--checkpoint_freq', type=int, default=500, help='æ¯å¤šå°‘æ­¥ä¿å­˜ä¸€æ¬¡checkpoint')

# å‚æ•°å†»ç»“å‚æ•°
parser.add_argument('--freeze_clip_backbone', type=str2bool, default=False, help='å†»ç»“CLIPé¢„è®­ç»ƒæƒé‡ï¼Œåªè®­ç»ƒæ–°å¢æ¨¡å—')
parser.add_argument('--freeze_encoder_layers', type=int, default=0, help='å†»ç»“ViT encoderçš„å‰Nå±‚ï¼ˆ0=ä¸å†»ç»“ï¼‰')

# âš¡ æ˜¾å­˜ä¼˜åŒ–å‚æ•°
parser.add_argument('--use_amp', type=str2bool, default=True, help='ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ(AMP)ï¼Œå¯èŠ‚çœ~30%æ˜¾å­˜')
parser.add_argument('--gradient_accumulation_steps', type=int, default=1, help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæœ‰æ•ˆbatch=batch_size*accumulation_stepsï¼‰')
parser.add_argument('--memory_efficient_mode', type=str2bool, default=True, help='å¯ç”¨å†…å­˜é«˜æ•ˆæ¨¡å¼ï¼ˆå‡å°‘ä¸­é—´æ¿€æ´»å€¼ç¼“å­˜ï¼‰')

# ğŸš¨ I/Oä¼˜åŒ–å‚æ•°
parser.add_argument('--num_workers', type=int, default=4, help='DataLoader workeræ•°é‡ï¼ˆHDDå»ºè®®2-4ï¼ŒSSDå¯8-12ï¼‰')
parser.add_argument('--prefetch_factor', type=int, default=4, help='æ¯ä¸ªworkeré¢„è¯»å–çš„batchæ•°ï¼ˆé™ä½å¯å‡å°‘I/Oå‹åŠ›ï¼‰')

# ğŸ² éšæœºç§å­å‚æ•°
parser.add_argument('--seed', type=int, default=None, help='éšæœºç§å­ï¼ˆNone=éšæœºç”Ÿæˆå¹¶è®°å½•ï¼‰')

# ğŸ¯ KeyTokenèåˆLosså‚æ•°ï¼ˆå¯¹æ¯”å­¦ä¹ ç‰ˆæœ¬ï¼‰
parser.add_argument('--use_keytoken_loss', type=str2bool, default=False, help='ä½¿ç”¨KeyTokenèåˆLossï¼ˆå¯¹æ¯”å­¦ä¹ +é²æ£’æ€§+MAEï¼‰')
parser.add_argument('--contrastive_weight', type=float, default=1.0, help='å¯¹æ¯”å­¦ä¹ æŸå¤±æƒé‡')
parser.add_argument('--contrastive_temperature', type=float, default=0.07, help='å¯¹æ¯”å­¦ä¹ æ¸©åº¦å‚æ•°')
parser.add_argument('--robust_weight', type=float, default=0.5, help='é²æ£’æ€§æŸå¤±(L2)æƒé‡')
parser.add_argument('--detect_weight', type=float, default=0.1, help='æ‰°åŠ¨æ£€æµ‹æŸå¤±æƒé‡')


def set_random_seed(seed=None):
    """
    è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ä¿è¯å¯å¤ç°æ€§
    
    Args:
        seed: éšæœºç§å­ï¼ŒNoneæ—¶è‡ªåŠ¨ç”Ÿæˆ
    
    Returns:
        ä½¿ç”¨çš„ç§å­å€¼
    """
    import random
    
    if seed is None:
        # ä½¿ç”¨æ—¶é—´æˆ³ç”Ÿæˆç§å­
        seed = int(time.time() * 1000) % (2**31)
    
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # è®¾ç½®deterministicæ¨¡å¼ï¼ˆå¯èƒ½å½±å“æ€§èƒ½ï¼Œä½†ä¿è¯å¯å¤ç°ï¼‰
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    return seed


def save_checkpoint(model, optimizer, scheduler, step, epoch, args, filename='checkpoint.pt'):
    """
    ä¿å­˜å®Œæ•´çš„è®­ç»ƒçŠ¶æ€ï¼ˆä¼˜åŒ–ç‰ˆï¼šå‡å°‘ç£ç›˜I/Oå‹åŠ›ï¼‰
    âš¡ ä¼˜åŒ–ï¼š
    1. æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼Œä¸è¶³æ—¶åªä¿ç•™æœ€è¿‘Nä¸ªcheckpoint
    2. å¼‚æ­¥ä¿å­˜ï¼ˆå¯é€‰ï¼Œé™ä½é˜»å¡æ—¶é—´ï¼‰
    3. å‹ç¼©ä¿å­˜ï¼ˆå¯é€‰ï¼Œå‡å°‘å†™å…¥é‡ï¼‰
    """
    import random
    
    checkpoint_dir = os.path.join(args.output_dir, 'checkpoints')
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_path = os.path.join(checkpoint_dir, filename)
    
    # âš¡ ä¿å­˜å®Œæ•´çš„EnhancedClipVisionModelï¼ˆåŒ…å«æ‰€æœ‰å¢å¼ºæ¨¡å—ï¼‰
    enhanced_model = unwrap_model(model)
    checkpoint = {
        'step': step,
        'epoch': epoch,
        # ä¿å­˜å®Œæ•´çš„å¢å¼ºæ¨¡å‹ï¼ˆåŒ…æ‹¬æ‰€æœ‰è®­ç»ƒçš„å¢å¼ºæ¨¡å—ï¼‰
        'enhanced_model_state_dict': enhanced_model.state_dict(),
        # åŒæ—¶ä¿å­˜åŸºç¡€CLIPæƒé‡ï¼ˆç”¨äºå…¼å®¹æ—§çš„è¯„ä¼°è„šæœ¬ï¼‰
        'model_state_dict': enhanced_model.model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if hasattr(scheduler, 'state_dict') else None,
        'args': vars(args),
        # ğŸ² ä¿å­˜éšæœºçŠ¶æ€ï¼Œç¡®ä¿è·¨Stageè®­ç»ƒçš„è¿ç»­æ€§
        'random_state': {
            'python_random': random.getstate(),
            'numpy_random': np.random.get_state(),
            'torch_random': torch.get_rng_state(),
            'torch_cuda_random': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,
        }
    }
    
    # âš¡ æ£€æŸ¥ç£ç›˜ç©ºé—´ï¼ˆå¦‚æœæ¥è¿‘æ»¡ï¼Œæ¸…ç†æ—§checkpointï¼‰
    if args.memory_efficient_mode:
        try:
            import shutil
            stat = shutil.disk_usage(checkpoint_dir)
            free_gb = stat.free / (1024**3)
            
            # å¦‚æœå‰©ä½™ç©ºé—´ < 50GBï¼Œæ¸…ç†æ—§checkpointï¼Œåªä¿ç•™æœ€è¿‘10ä¸ª
            if free_gb < 50:
                print(f"âš ï¸ ç£ç›˜ç©ºé—´ä¸è¶³ ({free_gb:.1f}GBå‰©ä½™)ï¼Œæ¸…ç†æ—§checkpoint...")
                cleanup_old_checkpoints(checkpoint_dir, keep_last_n=10)
        except Exception as e:
            print(f"âš ï¸ ç£ç›˜ç©ºé—´æ£€æŸ¥å¤±è´¥: {e}")
    
    # å…ˆä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…ä¿å­˜ä¸­æ–­å¯¼è‡´checkpointæŸå
    temp_path = checkpoint_path + '.tmp'
    try:
        torch.save(checkpoint, temp_path)
        os.replace(temp_path, checkpoint_path)  # åŸå­æ“ä½œ
        print(f"âœ… ä¿å­˜checkpoint: {checkpoint_path} (step={step}, epoch={epoch})")
    except OSError as e:
        # ç£ç›˜ç©ºé—´ä¸è¶³æˆ–å…¶ä»–I/Oé”™è¯¯
        if os.path.exists(temp_path):
            os.remove(temp_path)  # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        raise RuntimeError(f"âŒ Checkpointä¿å­˜å¤±è´¥ (ç£ç›˜ç©ºé—´ä¸è¶³?): {e}")
    
    return checkpoint_path


def cleanup_old_checkpoints(checkpoint_dir, keep_last_n=10):
    """æ¸…ç†æ—§checkpointï¼Œåªä¿ç•™æœ€è¿‘Nä¸ª"""
    try:
        checkpoints = []
        for fname in os.listdir(checkpoint_dir):
            if fname.endswith('.pt') and not fname.endswith('.tmp'):
                fpath = os.path.join(checkpoint_dir, fname)
                checkpoints.append((os.path.getmtime(fpath), fpath, fname))
        
        if len(checkpoints) > keep_last_n:
            checkpoints.sort(reverse=True)  # æœ€æ–°çš„åœ¨å‰
            to_delete = checkpoints[keep_last_n:]  # ä¿ç•™å‰Nä¸ªï¼Œåˆ é™¤å…¶ä»–çš„
            
            deleted_size = 0
            for _, fpath, fname in to_delete:
                try:
                    size = os.path.getsize(fpath)
                    os.remove(fpath)
                    deleted_size += size
                    print(f"  åˆ é™¤æ—§checkpoint: {fname} ({size/1e9:.2f}GB)")
                except Exception as e:
                    print(f"  åˆ é™¤å¤±è´¥ {fname}: {e}")
            
            print(f"âœ… å·²æ¸…ç† {len(to_delete)} ä¸ªæ—§checkpointï¼Œé‡Šæ”¾ {deleted_size/1e9:.2f}GB")
    except Exception as e:
        print(f"âš ï¸ Checkpointæ¸…ç†å¤±è´¥: {e}")


def find_latest_checkpoint(checkpoint_dir):
    """æŸ¥æ‰¾æœ€æ–°çš„checkpoint"""
    if not os.path.exists(checkpoint_dir):
        return None
    
    checkpoints = []
    for fname in os.listdir(checkpoint_dir):
        if fname.endswith('.pt') and not fname.endswith('.tmp'):
            fpath = os.path.join(checkpoint_dir, fname)
            checkpoints.append((os.path.getmtime(fpath), fpath))
    
    if not checkpoints:
        return None
    
    # è¿”å›æœ€æ–°çš„checkpoint
    checkpoints.sort(reverse=True)
    return checkpoints[0][1]


def load_checkpoint(checkpoint_path, model, optimizer=None, scheduler=None):
    """åŠ è½½checkpointå¹¶æ¢å¤è®­ç»ƒçŠ¶æ€"""
    import random
    
    print(f"ğŸ“‚ åŠ è½½checkpoint: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # åŠ è½½æ¨¡å‹ï¼ˆä¼˜å…ˆåŠ è½½å®Œæ•´çš„å¢å¼ºæ¨¡å‹ï¼Œå¦åˆ™åªåŠ è½½åŸºç¡€CLIPï¼‰
    enhanced_model = unwrap_model(model)
    if 'enhanced_model_state_dict' in checkpoint:
        # æ–°æ ¼å¼ï¼šåŒ…å«æ‰€æœ‰å¢å¼ºæ¨¡å—
        enhanced_model.load_state_dict(checkpoint['enhanced_model_state_dict'])
        print(f"âœ… å®Œæ•´å¢å¼ºæ¨¡å‹æƒé‡å·²æ¢å¤ï¼ˆåŒ…å«æ‰€æœ‰å¢å¼ºæ¨¡å—ï¼‰")
    else:
        # æ—§æ ¼å¼ï¼šåªæœ‰åŸºç¡€CLIPæƒé‡
        enhanced_model.model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âš ï¸  ä»…åŸºç¡€CLIPæƒé‡å·²æ¢å¤ï¼ˆå¢å¼ºæ¨¡å—å°†éšæœºåˆå§‹åŒ–ï¼‰")
    
    # åŠ è½½optimizer
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print(f"âœ… OptimizerçŠ¶æ€å·²æ¢å¤")
    
    # åŠ è½½scheduler
    if scheduler is not None and checkpoint.get('scheduler_state_dict') is not None:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        print(f"âœ… SchedulerçŠ¶æ€å·²æ¢å¤")
    
    # ğŸ² æ¢å¤éšæœºçŠ¶æ€ï¼Œç¡®ä¿è·¨Stageè®­ç»ƒçš„è¿ç»­æ€§
    if 'random_state' in checkpoint:
        random_state = checkpoint['random_state']
        random.setstate(random_state['python_random'])
        np.random.set_state(random_state['numpy_random'])
        torch.set_rng_state(random_state['torch_random'])
        if torch.cuda.is_available() and random_state['torch_cuda_random'] is not None:
            torch.cuda.set_rng_state_all(random_state['torch_cuda_random'])
        print(f"âœ… éšæœºçŠ¶æ€å·²æ¢å¤ï¼ˆä¿æŒè®­ç»ƒè¿ç»­æ€§ï¼‰")
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ°éšæœºçŠ¶æ€ï¼ˆæ—§checkpointæ ¼å¼ï¼‰")
    
    step = checkpoint.get('step', 0)
    epoch = checkpoint.get('epoch', 0)
    print(f"âœ… ä» step={step}, epoch={epoch} æ¢å¤è®­ç»ƒ")
    
    return step, epoch


def freeze_clip_backbone(model, freeze_encoder_layers=0):
    """å†»ç»“CLIP backboneçš„å‚æ•°"""
    enhanced_model = unwrap_model(model)
    clip_model = enhanced_model.model  # ViT encoder
    
    print(f"\nğŸ”’ å†»ç»“CLIPé¢„è®­ç»ƒæƒé‡...")
    
    # å†»ç»“æ‰€æœ‰CLIPå‚æ•°
    for param in clip_model.parameters():
        param.requires_grad = False
    
    # å¦‚æœæŒ‡å®šäº†éƒ¨åˆ†è§£å†»encoderå±‚
    if freeze_encoder_layers > 0:
        total_layers = len(clip_model.transformer.resblocks)
        unfreeze_layers = total_layers - freeze_encoder_layers
        if unfreeze_layers > 0:
            print(f"  è§£å†»æœ€å {unfreeze_layers} å±‚ transformer blocks")
            for i in range(total_layers - unfreeze_layers, total_layers):
                for param in clip_model.transformer.resblocks[i].parameters():
                    param.requires_grad = True
    
    print(f"  âœ… CLIP backboneå·²å†»ç»“")


def get_trainable_params(model):
    """è·å–æ‰€æœ‰éœ€è¦è®­ç»ƒçš„å‚æ•°"""
    enhanced_model = unwrap_model(model)
    trainable_params = []
    
    # æ”¶é›†æ‰€æœ‰requires_grad=Trueçš„å‚æ•°
    for name, param in enhanced_model.named_parameters():
        if param.requires_grad:
            trainable_params.append(param)
    
    return trainable_params


def print_trainable_params(model):
    """æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡"""
    enhanced_model = unwrap_model(model)
    
    total_params = 0
    trainable_params = 0
    
    print(f"\nğŸ“Š å‚æ•°ç»Ÿè®¡ï¼š")
    print(f"{'-'*60}")
    
    # æŒ‰æ¨¡å—åˆ†ç»„ç»Ÿè®¡
    module_stats = {}
    
    for name, param in enhanced_model.named_parameters():
        num_params = param.numel()
        total_params += num_params
        
        if param.requires_grad:
            trainable_params += num_params
            
            # æå–æ¨¡å—åç§°
            module_name = name.split('.')[0] if '.' in name else name
            if module_name not in module_stats:
                module_stats[module_name] = {'trainable': 0, 'frozen': 0}
            module_stats[module_name]['trainable'] += num_params
        else:
            module_name = name.split('.')[0] if '.' in name else name
            if module_name not in module_stats:
                module_stats[module_name] = {'trainable': 0, 'frozen': 0}
            module_stats[module_name]['frozen'] += num_params
    
    # æ‰“å°æ¯ä¸ªæ¨¡å—çš„ç»Ÿè®¡
    for module_name, stats in sorted(module_stats.items()):
        total_module = stats['trainable'] + stats['frozen']
        status = "ğŸ”“ è®­ç»ƒ" if stats['trainable'] > 0 else "ğŸ”’ å†»ç»“"
        print(f"  {status} {module_name:30s}: {stats['trainable']:>12,} / {total_module:>12,} å‚æ•°")
    
    print(f"{'-'*60}")
    print(f"  æ€»è®¡: {trainable_params:,} / {total_params:,} å‚æ•°å¯è®­ç»ƒ")
    print(f"  å¯è®­ç»ƒæ¯”ä¾‹: {100*trainable_params/total_params:.2f}%")
    print(f"  é¢„è®¡æ˜¾å­˜èŠ‚çœ: ~{100*(1-trainable_params/total_params):.1f}% (æ¢¯åº¦+ä¼˜åŒ–å™¨çŠ¶æ€)")
    print(f"{'-'*60}\n")


class EnhancedClipVisionModel(nn.Module):
    """
    å¢å¼ºçš„CLIPè§†è§‰æ¨¡å‹
    é›†æˆMAEé‡å»º+å…³é”®Tokenä¿æŠ¤
    """
    def __init__(self, model, args, normalize):
        super().__init__()
        self.model = model  # ViT visual encoder
        self.args = args
        self.normalize = normalize
        
        # è·å–æ¨¡å‹ç»´åº¦
        if args.clip_model_name == 'ViT-L-14':
            self.dim = 1024  # ViT-L/14 ç‰¹å¾ç»´åº¦æ˜¯1024
        elif args.clip_model_name == 'ViT-B-32':
            self.dim = 512
        elif args.clip_model_name == 'ViT-B-16':
            self.dim = 768
        else:
            self.dim = 768  # é»˜è®¤
        
        # æ–°å¢æ¨¡å—
        if args.use_mae_recon or args.use_key_token_protection:
            self.patch_disturb_detector = PatchDisturbDetector(dim=self.dim)
            
            if args.use_key_token_protection:
                if args.adaptive_masking:
                    self.key_selector = AdaptiveKeyTokenSelector(
                        base_ratio=args.key_token_ratio
                    )
                else:
                    self.key_selector = KeyTokenSelector(
                        top_k_ratio=args.key_token_ratio
                    )
            
            if args.use_mae_recon:
                # åªéœ€è¦å›¾åƒè§£ç å™¨ï¼ˆæ–‡æœ¬ç¼–ç å™¨å†»ç»“ï¼‰
                self.mae_decoder = DualMAEDecoder(
                    img_dim=self.dim,
                    text_dim=self.dim
                ).img_decoder
    
    def forward_features(self, x):
        """
        æå–å›¾åƒpatchç‰¹å¾
        éœ€è¦ä¿®æ”¹ViTä»¥è¿”å›æ‰€æœ‰patch tokens
        """
        x = self.normalize(x)
        
        # âš¡ å¤„ç†DataParallelåŒ…è£…ï¼šunwrapè·å–å®é™…æ¨¡å‹
        actual_model = self.model.module if hasattr(self.model, 'module') else self.model
        
        # é€šè¿‡ViTçš„æ‰€æœ‰å±‚è·å–patch tokens
        # è¿™é‡Œéœ€è¦è®¿é—®ViTçš„å†…éƒ¨ç»“æ„
        x = actual_model.conv1(x)  # patch embedding
        x = x.reshape(x.shape[0], x.shape[1], -1)
        x = x.permute(0, 2, 1)
        
        # æ·»åŠ class tokenå’Œposition embedding
        x = torch.cat([
            actual_model.class_embedding.to(x.dtype) + torch.zeros(
                x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device
            ),
            x
        ], dim=1)
        x = x + actual_model.positional_embedding.to(x.dtype)
        
        x = actual_model.ln_pre(x)
        x = x.permute(1, 0, 2)  # NLD -> LND
        
        # é€šè¿‡transformer blocks
        x = actual_model.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        
        # ä¸åº”ç”¨ln_postå’Œprojectionï¼ˆä¿ç•™patchç‰¹å¾ï¼‰
        return x  # (B, N+1, dim) where N=196 for 224x224
    
    def forward(self, x, vision_clean=None, output_normalize=False, mode='train', randomize_defense=False):
        """
        å¢å¼ºçš„å‰å‘ä¼ æ’­ï¼ˆæ˜¾å­˜ä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            x: æ‰°åŠ¨å›¾åƒ
            vision_clean: æ¸…æ´å›¾åƒï¼ˆè®­ç»ƒæ—¶æä¾›ï¼‰
            output_normalize: æ˜¯å¦å½’ä¸€åŒ–è¾“å‡º
            mode: 'train' / 'eval' / 'attack'
                - 'train': è®­ç»ƒæ¨¡å¼ï¼Œä½¿ç”¨æ‰€æœ‰å¢å¼ºæ¨¡å—å­¦ä¹ é˜²å¾¡
                - 'eval': æ¨ç†æ¨¡å¼ï¼Œä½¿ç”¨æ‰€æœ‰å¢å¼ºæ¨¡å—è¿›è¡Œé²æ£’æ¨ç†
                - 'attack': æ”»å‡»æ¨¡å¼ï¼Œåªä½¿ç”¨åŸºç¡€CLIPï¼ˆæ— é˜²å¾¡ï¼‰ï¼Œç”¨äºç”Ÿæˆå¼ºå¯¹æŠ—æ ·æœ¬
        """
        if mode == 'attack':
            # å¯¹æŠ—æ ·æœ¬ç”Ÿæˆæ¨¡å¼ï¼šåªä½¿ç”¨åŸºç¡€CLIPï¼Œä¸å¯ç”¨ä»»ä½•é˜²å¾¡æœºåˆ¶
            # è¿™æ ·å¯ä»¥ç”Ÿæˆæ›´å¼ºçš„å¯¹æŠ—æ ·æœ¬ï¼Œé¿å…æ”»å‡»æ—¶å°±å¯ç”¨é˜²å¾¡å¯¼è‡´æ ·æœ¬å¤ªå¼±
            embedding = self.model(self.normalize(x))
            if output_normalize:
                embedding = F.normalize(embedding, dim=-1)
            # attackæ¨¡å¼ä¸è®¡ç®—FeatDiffï¼Œè¿”å›6ä¸ªå€¼ä¿æŒæ¥å£ä¸€è‡´
            return embedding, torch.tensor(0.0, device=x.device), None, None, None, None
        
        elif mode == 'train' and (self.args.use_mae_recon or self.args.use_key_token_protection):
            assert vision_clean is not None, "è®­ç»ƒæ¨¡å¼éœ€è¦æä¾›æ¸…æ´å›¾åƒ"
            
            # 1. æå–patchç‰¹å¾
            patch_tokens = self.forward_features(x)  # (B, 197, dim)
            
            # âš¡ æ˜¾å­˜ä¼˜åŒ–ï¼šcleanç‰¹å¾ä¸éœ€è¦æ¢¯åº¦ï¼Œä½¿ç”¨no_gradå‡å°‘50%æ˜¾å­˜
            with torch.no_grad():
                patch_tokens_clean = self.forward_features(vision_clean).detach()
            
            # ç‰¹å¾å¯è§†åŒ–ï¼šä½¿ç”¨TTCè®ºæ–‡çš„tokençº§Ï„å€¼ï¼ˆå½’ä¸€åŒ–ç›¸å¯¹å˜åŒ–ï¼‰
            # Ï„_i = ||f_i(x_adv) - f_i(x_clean)|| / ||f_i(x_clean)||  å¯¹æ¯ä¸ªtoken
            # ä¼˜åŠ¿ï¼š1) å½’ä¸€åŒ–åº¦é‡ 2) tokençº§åˆ«ï¼Œä¸disturb_scoresä¸€è‡´ 3) æ— éœ€è®­ç»ƒ
            tau_token = None  # ç”¨äºåç»­ä¼ é€’ç»™disturb_detector
            
            # âš¡ æ€»æ˜¯è®¡ç®—feature_diffï¼Œä¸ä¾èµ–hasattræ£€æŸ¥
            with torch.no_grad():
                # Tokençº§åˆ«Ï„å€¼è®¡ç®— (B, 197, dim)
                token_diff = patch_tokens - patch_tokens_clean  # (B, 197, dim)
                token_diff_norm = torch.norm(token_diff, p=2, dim=2)  # (B, 197)
                token_clean_norm = torch.norm(patch_tokens_clean, p=2, dim=2)  # (B, 197)
                tau_token = token_diff_norm / (token_clean_norm + 1e-8)  # (B, 197)
                
                # å…¨å±€ç»Ÿè®¡æŒ‡æ ‡ç”¨äºæ—¥å¿—
                feature_diff_mean = tau_token.mean()  # tensoræ ‡é‡
                feature_diff_max = tau_token.max()
                feature_diff_std = tau_token.std()
            
            # 2. æ‰°åŠ¨æ£€æµ‹ï¼ˆèåˆtokençº§åˆ«Ï„å€¼ï¼‰
            # æ‰°åŠ¨æ£€æµ‹å™¨éœ€è¦æ¢¯åº¦ï¼Œç”¨äºè®­ç»ƒæ£€æµ‹å™¨
            disturb_scores_raw = self.patch_disturb_detector(
                patch_tokens, patch_tokens_clean, mode='train'
            )  # (B, 197)
            
            # èåˆÏ„å€¼ï¼šå°†æ— ç›‘ç£çš„Ï„å€¼ä¸å­¦ä¹ åˆ°çš„disturb_scoresç»“åˆ
            # ä½¿ç”¨åŠ æƒå¹³å‡ï¼Œæ—©æœŸä¾èµ–Ï„å€¼ï¼ŒåæœŸä¾èµ–å­¦ä¹ çš„scores
            if tau_token is not None:
                # åŠ¨æ€æƒé‡ï¼šéšè®­ç»ƒè¿›è¡Œé€æ¸ä»Ï„å€¼è½¬å‘disturb_scores
                tau_weight = 0.3
                disturb_scores = tau_weight * tau_token.detach() + (1 - tau_weight) * disturb_scores_raw
            else:
                disturb_scores = disturb_scores_raw
            
            # ä¿å­˜ç”¨äºæŸå¤±è®¡ç®—ï¼ˆpred_disturbç”¨äºè®­ç»ƒæ£€æµ‹å™¨ï¼‰
            self._pred_disturb = disturb_scores_raw
            self._target_disturb = tau_token.detach() if tau_token is not None else None
            
            # 3. å…³é”®Tokenç­›é€‰ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
            with torch.no_grad():
                if self.args.use_key_token_protection:
                    # è·å–æ³¨æ„åŠ›æƒé‡ï¼ˆç®€åŒ–ç‰ˆï¼šä½¿ç”¨Noneï¼Œä¾èµ–ç‰¹å¾é‡è¦æ€§ï¼‰
                    if self.args.adaptive_masking:
                        key_mask = self.key_selector(
                            patch_tokens.detach(), disturb_scores,
                            attention_weights=None, token_type='image'
                        )
                    else:
                        key_mask = self.key_selector.select_img_key_tokens(
                            patch_tokens.detach(), attention_weights=None
                        )
                else:
                    key_mask = torch.ones_like(disturb_scores, dtype=torch.bool)
                
                # 4. åŠ¨æ€é˜ˆå€¼è°ƒæ•´ï¼ˆè®­ç»ƒé˜¶æ®µï¼‰
                avg_disturb = disturb_scores.mean(dim=1, keepdim=True)  # (B, 1)
                adaptive_threshold = 0.3 + 0.4 * avg_disturb.clamp(0, 1)  # (B, 1)
                
                # 5. âš¡ ä¼˜åŒ–åçš„ä¸Šä¸‹æ–‡ä¿ç•™æœºåˆ¶ï¼ˆå‘é‡åŒ–æ“ä½œï¼Œé¿å…Pythonå¾ªç¯ï¼‰
                batch_size = key_mask.shape[0]
                num_tokens = key_mask.shape[1]
                
                # ä½¿ç”¨å·ç§¯æ“ä½œæ‰©å±•ç›¸é‚»Tokenï¼ˆæ¯”å¾ªç¯å¿«10x+ï¼‰
                key_mask_float = key_mask.float().unsqueeze(1)  # (B, 1, N)
                # 1Då·ç§¯æ ¸ [1, 1, 1] è¡¨ç¤ºæ‰©å±•å‰åå„1ä¸ªä½ç½®
                expand_kernel = torch.ones(1, 1, 3, device=key_mask.device)
                key_mask_expanded = F.conv1d(key_mask_float, expand_kernel, padding=1)
                key_mask_expanded = (key_mask_expanded.squeeze(1) > 0)  # (B, N)
                
                # 6. åŠ¨æ€æ©ç 
                mask = (disturb_scores > adaptive_threshold) & (~key_mask_expanded)
            
            # å¯¹ä¿æŠ¤Tokenæ·»åŠ å™ªå£°ï¼ˆä¿æŒæ¢¯åº¦ï¼‰
            noise_std = 0.1
            protected_mask = ~mask
            # âš¡ ä½¿ç”¨whereé¿å…åŸåœ°æ“ä½œ
            noise = torch.randn_like(patch_tokens) * noise_std
            patch_tokens_protected = torch.where(
                protected_mask.unsqueeze(-1),
                patch_tokens + noise,
                patch_tokens
            )
            
            # 7. MAEé‡å»ºï¼ˆå¯é€‰ï¼‰
            if self.args.use_mae_recon:
                patch_recon = self.mae_decoder(patch_tokens, mask)
                mae_loss = self.mae_decoder.compute_reconstruction_loss(
                    patch_recon, patch_tokens_clean, mask
                )
            else:
                mae_loss = torch.tensor(0.0, device=x.device)
            
            # 8. ç”Ÿæˆembeddingï¼šèåˆå…¨å±€ç‰¹å¾å’Œå…³é”®tokenç‰¹å¾ï¼ˆä¸æ¨ç†æ¨¡å¼ä¸€è‡´ï¼‰
            # å…¨å±€ç‰¹å¾ï¼šä½¿ç”¨ä¿æŠ¤åçš„[CLS] token
            global_feat = patch_tokens_protected[:, 0, :]  # (B, dim)
            
            # å±€éƒ¨å…³é”®ç‰¹å¾ï¼šå…³é”®Tokençš„åŠ æƒå¹³å‡
            if self.args.use_key_token_protection and key_mask.any():
                # key_mask: (B, N), æå–å…³é”®tokenå¹¶èšåˆ
                key_tokens = patch_tokens_protected * key_mask.unsqueeze(-1).float()
                key_count = key_mask.sum(dim=1, keepdim=True).clamp(min=1).float()
                local_feat = key_tokens.sum(dim=1) / key_count  # (B, dim)
            else:
                local_feat = global_feat
            
            # èåˆå…¨å±€+å±€éƒ¨ç‰¹å¾ï¼ˆ70% å…¨å±€ + 30% å±€éƒ¨ï¼Œä¸æ¨ç†æ¨¡å¼ä¸€è‡´ï¼‰
            embedding = 0.7 * global_feat + 0.3 * local_feat
            
            # åº”ç”¨åå¤„ç†å±‚ï¼ˆln_post + projectionï¼‰
            embedding = self.model.ln_post(embedding)
            if self.model.proj is not None:
                embedding = embedding @ self.model.proj
            
            if output_normalize:
                embedding = F.normalize(embedding, dim=-1)
            
            # âš¡ è¿”å›feature_diff_meanè§£å†³DataParallelé—®é¢˜ï¼ˆtensoræ ‡é‡å¯ä»¥è¢«gatherï¼‰
            # åŒæ—¶è¿”å›pred_disturbã€target_disturbå’Œkey_maskç”¨äºæŸå¤±è®¡ç®—
            return embedding, mae_loss, feature_diff_mean, self._pred_disturb, self._target_disturb, key_mask
        
        else:
            # æ¨ç†æ¨¡å¼ï¼šå®ç°Tokenè¿‡æ»¤ä¸é²æ£’åŒ¹é…
            if self.args.use_key_token_protection or self.args.use_mae_recon:
                # 1. æå–patchç‰¹å¾
                patch_tokens = self.forward_features(x)  # (B, 197, dim)
                
                # 2. æ‰°åŠ¨æ£€æµ‹ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
                disturb_scores = self.patch_disturb_detector(
                    patch_tokens, mode='eval'
                )  # (B, 197)
                
                # 3. å…³é”®Tokenç­›é€‰
                if self.args.use_key_token_protection:
                    # ç»Ÿä¸€ä½¿ç”¨forwardæ¥å£ï¼ˆå…¼å®¹AdaptiveKeyTokenSelectorå’ŒKeyTokenSelectorï¼‰
                    if self.args.adaptive_masking:
                        key_mask = self.key_selector(
                            patch_tokens, disturb_scores,
                            attention_weights=None, token_type='image'
                        )
                    else:
                        key_mask = self.key_selector.select_img_key_tokens(
                            patch_tokens, attention_weights=None
                        )
                else:
                    key_mask = torch.ones_like(disturb_scores, dtype=torch.bool)
                
                # 4. åŠ¨æ€é˜ˆå€¼è°ƒæ•´ï¼šæ ¹æ®æ‰°åŠ¨å¼ºåº¦è‡ªé€‚åº”
                # è®¡ç®—æ‰¹æ¬¡å¹³å‡æ‰°åŠ¨å¼ºåº¦
                avg_disturb = disturb_scores.mean(dim=1, keepdim=True)  # (B, 1)
                # é˜ˆå€¼èŒƒå›´ï¼š[0.3, 0.7]ï¼Œæ‰°åŠ¨è¶Šå¼ºé˜ˆå€¼è¶Šé«˜ï¼ˆä¿ç•™æ›´å¤šTokenï¼‰
                adaptive_threshold = 0.3 + 0.4 * avg_disturb.clamp(0, 1)  # (B, 1)
                
                # ğŸ² éšæœºåŒ–é˜²å¾¡ï¼šå¯¹é˜ˆå€¼æ·»åŠ é«˜æ–¯å™ªå£°ï¼ˆæ‰“ç ´APGDæ¢¯åº¦ä¼°è®¡ï¼‰
                if randomize_defense:
                    threshold_noise = torch.randn_like(adaptive_threshold) * 0.05
                    adaptive_threshold = (adaptive_threshold + threshold_noise).clamp(0.2, 0.8)
                
                # 5. âš¡ ä¸Šä¸‹æ–‡ä¿ç•™æœºåˆ¶ï¼ˆå‘é‡åŒ–æ“ä½œï¼Œä¸è®­ç»ƒæ¨¡å¼ä¸€è‡´ï¼‰
                # ä½¿ç”¨å·ç§¯æ“ä½œæ‰©å±•ç›¸é‚»Tokenï¼ˆæ¯”å¾ªç¯å¿«10x+ï¼‰
                key_mask_float = key_mask.float().unsqueeze(1)  # (B, 1, N)
                
                # ğŸ² éšæœºåŒ–é˜²å¾¡ï¼šéšæœºé€‰æ‹©ä¸Šä¸‹æ–‡æ‰©å±•èŒƒå›´ï¼ˆkernel_size: 1-5ï¼‰
                if randomize_defense:
                    kernel_size = torch.randint(1, 6, (1,), device=key_mask.device).item()
                else:
                    kernel_size = 3
                
                if kernel_size > 1:
                    expand_kernel = torch.ones(1, 1, kernel_size, device=key_mask.device)
                    padding = kernel_size // 2
                    key_mask_expanded = F.conv1d(key_mask_float, expand_kernel, padding=padding)
                    key_mask_expanded = (key_mask_expanded.squeeze(1) > 0)  # (B, N)
                else:
                    key_mask_expanded = key_mask  # ä¸æ‰©å±•
                
                # 6. Tokenè¿‡æ»¤ï¼šä¿ç•™ä½æ‰°åŠ¨Token + å…³é”®Tokenï¼ˆå«ä¸Šä¸‹æ–‡ï¼‰
                # ä½¿ç”¨åŠ¨æ€é˜ˆå€¼
                filter_mask = (disturb_scores <= adaptive_threshold) | key_mask_expanded  # (B, 197)
                
                # è¿‡æ»¤Token
                patch_tokens_filtered = patch_tokens * filter_mask.unsqueeze(-1).float()
                
                # 7. å…¨å±€ç‰¹å¾ï¼šä½¿ç”¨è¿‡æ»¤åçš„[CLS] token
                global_feat = patch_tokens_filtered[:, 0, :]  # (B, dim)
                
                # 8. å±€éƒ¨å…³é”®ç‰¹å¾ï¼šå…³é”®Tokençš„å¹³å‡
                if self.args.use_key_token_protection and key_mask.any():
                    # æå–å…³é”®Tokenç‰¹å¾
                    key_tokens = patch_tokens_filtered * key_mask.unsqueeze(-1).float()
                    key_count = key_mask.sum(dim=1, keepdim=True).clamp(min=1).float()
                    local_feat = key_tokens.sum(dim=1) / key_count  # (B, dim)
                else:
                    local_feat = global_feat
                
                # 9. èåˆå…¨å±€+å±€éƒ¨ç‰¹å¾ï¼ˆ70% å…¨å±€ + 30% å±€éƒ¨ï¼‰
                # ğŸ² éšæœºåŒ–é˜²å¾¡ï¼šèåˆæƒé‡æ·»åŠ éšæœºæ‰°åŠ¨
                if randomize_defense:
                    alpha_noise = torch.randn(1, device=global_feat.device).item() * 0.1
                    alpha = torch.clamp(torch.tensor(0.7 + alpha_noise), 0.5, 0.9).item()
                else:
                    alpha = 0.7
                embedding = alpha * global_feat + (1.0 - alpha) * local_feat
                
                # åº”ç”¨åå¤„ç†å±‚
                embedding = self.model.ln_post(embedding)
                if self.model.proj is not None:
                    embedding = embedding @ self.model.proj
                
                if output_normalize:
                    embedding = F.normalize(embedding, dim=-1)
                
                # evalæ¨¡å¼ä¸è®¡ç®—FeatDiffï¼Œè¿”å›6ä¸ªå€¼ä¿æŒæ¥å£ä¸€è‡´
                return embedding, torch.tensor(0.0, device=x.device), None, None, None, None
            
            else:
                # ä¸ä½¿ç”¨å¢å¼ºåŠŸèƒ½ï¼Œç›´æ¥ç”¨CLIP
                embedding = self.model(self.normalize(x))
                if output_normalize:
                    embedding = F.normalize(embedding, dim=-1)
                return embedding, torch.tensor(0.0, device=x.device), None, None, None, None


class ComputeLossWrapper:
    def __init__(self, embedding_orig, embedding_text_labels_norm, reduction='mean', 
                 loss=None, logit_scale=100.):
        self.embedding_orig = embedding_orig
        self.embedding_text_labels_norm = embedding_text_labels_norm
        self.reduction = reduction
        self.loss_str = loss
        self.logit_scale = logit_scale

    def __call__(self, embedding, targets):
        from train.adversarial_training_clip import compute_loss
        return compute_loss(
            loss_str=self.loss_str, embedding=embedding, targets=targets,
            embedding_orig=self.embedding_orig, logit_scale=self.logit_scale,
            embedding_text_labels_norm=self.embedding_text_labels_norm, 
            reduction=self.reduction
        )


def train_one_epoch(
    step_total, model, model_orig, dataloader, optimizer, scheduler, normalize,
    embedding_text_labels_norm, args, epoch, dataloader_eval=None, scaler=None,
    best_acc=0.0
):
    """
    è®­ç»ƒä¸€ä¸ªepochï¼ˆæ˜¾å­˜ä¼˜åŒ–ç‰ˆï¼‰
    
    âš¡ ä¼˜åŒ–å†…å®¹ï¼š
    1. æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ(AMP) - èŠ‚çœ~30%æ˜¾å­˜
    2. æ”¯æŒæ¢¯åº¦ç´¯ç§¯ - å¯ç”¨æ›´å°batchè®­ç»ƒ
    3. ä¼˜åŒ–å¯¹æŠ—æ”»å‡»çš„æ˜¾å­˜ç®¡ç†
    4. åŠæ—¶é‡Šæ”¾ä¸­é—´å¼ é‡
    """
    model_orig.eval()
    model.train()
    
    # åˆå§‹åŒ–ç‰¹å¾å·®å¼‚ç»Ÿè®¡
    enhanced_model = unwrap_model(model)
    enhanced_model.feature_diff_stats = {'mean': 0.0, 'max': 0.0, 'std': 0.0}

    loss_meter = AverageMeter('loss')
    mae_loss_meter = AverageMeter('mae_loss')
    cos_sim_meter = AverageMeter('cos-sim')
    acc_meter = AverageMeter('acc')
    racc_meter = AverageMeter('racc')
    feature_diff_meter = AverageMeter('feature_diff')

    # âš¡ AMPè®¾ç½®
    use_amp = args.use_amp and torch.cuda.is_available()
    amp_dtype = torch.float16 if use_amp else torch.float32
    
    # æ¢¯åº¦ç´¯ç§¯
    accumulation_steps = args.gradient_accumulation_steps
    accumulated_loss = 0.0

    epoch_start_time = time.time()
    pbar = tqdm(dataloader, desc=f"Epoch {epoch+1} Step {step_total}/{args.steps}", ncols=120)
    
    for i, (data, targets) in enumerate(pbar):
        is_classification = isinstance(targets, torch.Tensor)
        data = data.cuda(non_blocking=True)
        n_samples = data.shape[0]
        if is_classification:
            targets = targets.cuda(non_blocking=True)

        # ä¿å­˜æ¸…æ´å›¾åƒ
        data_clean = data.clone()

        # âš¡ åŸå§‹åµŒå…¥ï¼ˆä¸éœ€è¦æ¢¯åº¦ï¼‰
        with torch.no_grad():
            with torch.cuda.amp.autocast(enabled=use_amp):
                embedding_orig, _, _, _, _, _ = model_orig(data, output_normalize=args.output_normalize, mode='eval')
            embedding_orig = embedding_orig.detach()

        # âš¡ ç”Ÿæˆå¯¹æŠ—æ ·æœ¬ï¼ˆä½¿ç”¨torch.no_gradå‡å°‘æ˜¾å­˜ï¼‰
        loss_inner_wrapper = ComputeLossWrapper(
            embedding_orig, embedding_text_labels_norm,
            reduction='none' if args.attack == 'apgd' else 'mean', 
            loss=args.inner_loss, logit_scale=100.
        )
        model.eval()

        # âš¡ å¯¹æŠ—æ”»å‡»åœ¨ä½ç²¾åº¦ä¸‹æ‰§è¡Œï¼Œå‡å°‘æ˜¾å­˜å³°å€¼
        # âš¡ ä½¿ç”¨'attack'æ¨¡å¼ï¼šæ”»å‡»åŸºç¡€CLIPï¼ˆæ— é˜²å¾¡ï¼‰ï¼Œç”Ÿæˆæ›´å¼ºçš„å¯¹æŠ—æ ·æœ¬
        with torch.cuda.amp.autocast(enabled=use_amp):
            if args.attack == 'pgd':
                data_adv = pgd(
                    forward=lambda x, output_normalize: model(x, output_normalize=output_normalize, mode='attack')[0],  # åªå–ç¬¬ä¸€ä¸ªè¿”å›å€¼
                    loss_fn=loss_inner_wrapper,
                    data_clean=data,
                    targets=targets,
                    norm=args.norm,
                    eps=args.eps,
                    iterations=args.iterations_adv,
                    stepsize=args.stepsize_adv,
                    output_normalize=args.output_normalize,
                    perturbation=torch.zeros_like(data).uniform_(-args.eps, args.eps).requires_grad_(True),
                    mode='max',
                    verbose=False
                )
            elif args.attack == 'apgd':
                # åˆ›å»ºä¸€ä¸ªæ¨¡å‹wrapperç±»ç”¨äºapgd
                class APGDModelWrapper(nn.Module):
                    def __init__(self, model):
                        super().__init__()
                        self.model = model
                    
                    def forward(self, x, output_normalize=True):
                        # âš¡ ä½¿ç”¨'attack'æ¨¡å¼ï¼šæ”»å‡»åŸºç¡€CLIPï¼ˆæ— é˜²å¾¡ï¼‰
                        # åªå–ç¬¬ä¸€ä¸ªè¿”å›å€¼ï¼ˆembeddingï¼‰
                        return self.model(x, output_normalize=output_normalize, mode='attack')[0]
                
                model_wrapper = APGDModelWrapper(model)
                model_wrapper.eval()
                
                data_adv = apgd(
                    model=model_wrapper,
                    loss_fn=loss_inner_wrapper,
                    x=data,
                    y=targets,
                    norm=args.norm,
                    eps=args.eps,
                    n_iter=args.iterations_adv,
                    verbose=False
                )
                # âš¡ åŠæ—¶é‡Šæ”¾wrapper
                del model_wrapper
            elif args.attack == 'none':
                data_adv = data

        # âš¡ åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¼ é‡
        del loss_inner_wrapper
        torch.cuda.empty_cache()  # æ¸…ç†ç¼“å­˜
        
        model.train()

        # âš¡ è®­ç»ƒå‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨AMPï¼‰
        with torch.cuda.amp.autocast(enabled=use_amp):
            embedding_adv, mae_loss, feature_diff_mean, pred_disturb, target_disturb, key_mask = model(
                data_adv, vision_clean=data_clean, 
                output_normalize=args.output_normalize, mode='train'
            )
            
            # ğŸ¯ ä½¿ç”¨KeyTokenèåˆLossæˆ–åŸå§‹Loss
            if args.use_keytoken_loss:
                # KeyTokenèåˆLossï¼šå¯¹æ¯”å­¦ä¹  + å…³é”®Tokené²æ£’æ€§ + MAEé‡å»º + æ‰°åŠ¨æ£€æµ‹
                loss, loss_dict = compute_keytoken_loss(
                    embedding_adv=embedding_adv,
                    embedding_orig=embedding_orig,
                    targets=targets,
                    text_embeddings=embedding_text_labels_norm,
                    mae_loss=mae_loss if args.use_mae_recon else None,
                    pred_disturb=pred_disturb,  # ä¼ å…¥æ‰°åŠ¨æ£€æµ‹å™¨çš„é¢„æµ‹
                    target_disturb=target_disturb,  # åŸºäºÏ„å€¼çš„çœŸå®æ‰°åŠ¨
                    key_mask=key_mask,  # å…³é”®tokenæ©ç ï¼Œç”¨äºå…³é”®Tokençº§åˆ«Robust Loss
                    contrastive_weight=args.contrastive_weight,
                    robust_weight=args.robust_weight,
                    mae_weight=args.mae_weight,
                    detect_weight=args.detect_weight,
                    temperature=args.contrastive_temperature,
                    logit_scale=100.0
                )
                # è®°å½•å„é¡¹lossç”¨äºæ—¥å¿—
                loss_contrastive_value = loss_dict.get('loss_contrastive', 0.0)
                loss_robust_value = loss_dict.get('loss_robust', 0.0)
                loss_mae_value = loss_dict.get('loss_mae', 0.0)
                loss_detect_value = loss_dict.get('loss_detect', 0.0)
            else:
                # åŸå§‹Lossè®¡ç®—æ–¹å¼
                from train.adversarial_training_clip import compute_loss
                loss_adv = compute_loss(
                    loss_str=args.loss, embedding=embedding_adv, targets=targets,
                    embedding_orig=embedding_orig, logit_scale=100.,
                    embedding_text_labels_norm=embedding_text_labels_norm
                )

                # æ¸…æ´æ ·æœ¬æŸå¤±ï¼ˆå¯é€‰ï¼‰
                if args.clean_weight > 0.:
                    embedding_clean, _, _, _, _, _ = model(
                        data_clean, vision_clean=None,
                        output_normalize=args.output_normalize, mode='eval'
                    )
                    loss_clean = compute_loss(
                        loss_str=args.loss_clean, embedding=embedding_clean, targets=targets,
                        embedding_orig=embedding_orig, logit_scale=100.,
                        embedding_text_labels_norm=None
                    )
                    loss = loss_adv + args.clean_weight * loss_clean
                else:
                    loss = loss_adv

                # æ·»åŠ MAEé‡å»ºæŸå¤±
                if args.use_mae_recon:
                    if isinstance(mae_loss, torch.Tensor) and mae_loss.dim() > 0:
                        mae_loss = mae_loss.mean()
                    loss = loss + args.mae_weight * mae_loss
                
                # å…¼å®¹æ€§ï¼šè®¾ç½®é»˜è®¤å€¼ï¼ˆå¤„ç†DataParallelå¤šå…ƒç´ tensorï¼‰
                loss_cls_value = 0.0
                if isinstance(loss_adv, torch.Tensor):
                    loss_robust_value = loss_adv.mean().item() if loss_adv.numel() > 1 else loss_adv.item()
                else:
                    loss_robust_value = loss_adv
                if isinstance(mae_loss, torch.Tensor):
                    loss_mae_value = mae_loss.mean().item() if mae_loss.numel() > 1 else mae_loss.item()
                else:
                    loss_mae_value = 0.0
            
            # âš¡ æ¢¯åº¦ç´¯ç§¯ï¼šæŒ‰ç´¯ç§¯æ­¥æ•°ç¼©æ”¾æŸå¤±
            loss = loss / accumulation_steps

        # âš¡ åå‘ä¼ æ’­ï¼ˆä½¿ç”¨AMP scalerï¼‰
        if scaler is not None:
            scaler.scale(loss).backward()
        else:
            loss.backward()
        
        accumulated_loss += loss.item() * accumulation_steps
        
        # âš¡ æ¢¯åº¦ç´¯ç§¯ï¼šè¾¾åˆ°ç´¯ç§¯æ­¥æ•°åæ‰æ›´æ–°å‚æ•°
        if (i + 1) % accumulation_steps == 0:
            if scaler is not None:
                # AMPæ¢¯åº¦è£å‰ªå’Œæ›´æ–°
                scaler.unscale_(optimizer)
                
                # âš¡ NaNæ£€æµ‹ï¼šæ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰æ•ˆ
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                # å¦‚æœæ¢¯åº¦åŒ…å«NaNæˆ–Infï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°
                if torch.isfinite(grad_norm):
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    print(f"âš ï¸ Step {step_total}: æ£€æµ‹åˆ°NaN/Infæ¢¯åº¦ (norm={grad_norm.item():.2f})ï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°")
                    scaler.update()  # ä»éœ€æ›´æ–°scalerçŠ¶æ€
            else:
                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                if torch.isfinite(grad_norm):
                    optimizer.step()
                else:
                    print(f"âš ï¸ Step {step_total}: æ£€æµ‹åˆ°NaN/Infæ¢¯åº¦ï¼Œè·³è¿‡æ­¤æ¬¡æ›´æ–°")
            
            optimizer.zero_grad(set_to_none=True)  # âš¡ set_to_noneèŠ‚çœå°‘é‡æ˜¾å­˜
            scheduler(step_total)
            
            # è®°å½•
            loss_meter.update(accumulated_loss, n_samples * accumulation_steps)
            accumulated_loss = 0.0
            
            step_total += 1
        
        if args.use_mae_recon:
            # DataParallelå¯èƒ½è¿”å›å¤šå…ƒç´ tensorï¼Œéœ€è¦å…ˆå–mean
            if isinstance(mae_loss, torch.Tensor):
                if mae_loss.numel() > 1:
                    mae_loss_val = mae_loss.mean().item()
                else:
                    mae_loss_val = mae_loss.item()
            else:
                mae_loss_val = mae_loss
            mae_loss_meter.update(mae_loss_val, n_samples)
        
        # è®°å½•ç‰¹å¾å·®å¼‚
        # âš¡ ä½¿ç”¨forwardè¿”å›çš„feature_diff_meanï¼ˆè§£å†³DataParallelé—®é¢˜ï¼‰
        if feature_diff_mean is not None:
            # å°†tensorè½¬ä¸ºæ ‡é‡ç”¨äºç»Ÿè®¡ï¼ˆDataParallel gatheråå¯èƒ½æ˜¯å¤šå…ƒç´ tensorï¼‰
            if torch.is_tensor(feature_diff_mean):
                if feature_diff_mean.numel() > 1:
                    mean_val = feature_diff_mean.mean().item()
                else:
                    mean_val = feature_diff_mean.item()
            else:
                mean_val = feature_diff_mean
            feature_diff_meter.update(mean_val, n_samples)
            # åŒæ­¥åˆ°enhanced_modelï¼ˆåªä¿å­˜meanå€¼ï¼‰
            enhanced_model.feature_diff_stats = {
                'mean': mean_val,
                'max': mean_val,
                'std': 0.0
            }

        # è®¡ç®—å‡†ç¡®ç‡ï¼šåŒæ—¶è®¡ç®—å¹²å‡€æ ·æœ¬å’Œå¯¹æŠ—æ ·æœ¬å‡†ç¡®ç‡
        with torch.no_grad():
            if is_classification:
                # å¯¹æŠ—æ ·æœ¬å‡†ç¡®ç‡ (racc)
                logits_adv = embedding_text_labels_norm.T @ embedding_adv.float().T
                racc = (logits_adv.argmax(dim=0) == targets).float().mean()
                racc_meter.update(racc.item(), n_samples)
                
                # å¹²å‡€æ ·æœ¬å‡†ç¡®ç‡ (acc) - ä½¿ç”¨evalæ¨¡å¼é¿å…MAEå½±å“
                embedding_clean_eval, _, _, _, _, _ = model(
                    data_clean, vision_clean=None,
                    output_normalize=args.output_normalize, mode='eval'
                )
                logits_clean = embedding_text_labels_norm.T @ embedding_clean_eval.float().T
                acc = (logits_clean.argmax(dim=0) == targets).float().mean()
                acc_meter.update(acc.item(), n_samples)
                
                del embedding_clean_eval, logits_adv, logits_clean

        # âš¡ åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¼ é‡
        del data_adv, embedding_adv, data_clean
        if args.clean_weight > 0.:
            del embedding_clean
        
        # æ—¥å¿—è®°å½•
        if step_total % args.log_freq == 0 and (i + 1) % accumulation_steps == 0:
            log_dict = {
                'loss': loss_meter.avg,
                'step': step_total,
                'epoch': epoch,
                'lr': optimizer.param_groups[0]['lr']
            }
            if args.use_mae_recon:
                log_dict['mae_loss'] = mae_loss_meter.avg
            if is_classification:
                log_dict['clean_acc'] = acc_meter.avg
                log_dict['racc'] = racc_meter.avg
            
            # æ·»åŠ ç‰¹å¾å·®å¼‚åˆ°æ—¥å¿—
            if hasattr(enhanced_model, 'feature_diff_stats') and 'mean' in enhanced_model.feature_diff_stats:
                log_dict['feature_diff_mean'] = enhanced_model.feature_diff_stats['mean']
                log_dict['feature_diff_max'] = enhanced_model.feature_diff_stats['max']
                log_dict['feature_diff_std'] = enhanced_model.feature_diff_stats['std']
            
            # âš¡ æ·»åŠ æ˜¾å­˜ç›‘æ§
            if torch.cuda.is_available():
                log_dict['gpu_memory_gb'] = torch.cuda.max_memory_allocated() / 1e9
            
            wandb.log(log_dict)
            
            progress_pct = step_total / args.steps * 100
            pbar.set_description(f"Epoch {epoch+1} [{progress_pct:.1f}%] Step {step_total}/{args.steps}")
            # ç®€åŒ–postfixï¼Œé¿å…ç»ˆç«¯å®½åº¦æˆªæ–­ï¼ˆå®Œæ•´ä¿¡æ¯åœ¨printè¾“å‡ºä¸­ï¼‰
            pbar.set_postfix({'Loss': f"{loss_meter.avg:.4f}", 'FeatDiff': f"{feature_diff_meter.avg:.4f}"})
            
            # æ„å»ºFeatDiffå­—ç¬¦ä¸²
            stats = getattr(enhanced_model, 'feature_diff_stats', None)
            if stats and 'mean' in stats:
                if stats['mean'] > 0.05:
                    status_msg = "âœ… CLIPå·²é€‚åº”æ‰°åŠ¨"
                else:
                    status_msg = "âš ï¸ æ‰°åŠ¨å¾®å¼±"
            else:
                stats = {'mean': 0.0, 'max': 0.0, 'std': 0.0}
                status_msg = "âš ï¸ ç‰¹å¾å·®å¼‚æœªåˆå§‹åŒ–"
            
            feat_diff_str = f"FeatDiff: {stats['mean']:.6f} (max={stats['max']:.6f}, std={stats['std']:.6f}) {status_msg}"
            
            # æ„å»ºæ˜¾å­˜å­—ç¬¦ä¸²
            if torch.cuda.is_available():
                mem_str = f"GPU: {torch.cuda.max_memory_allocated() / 1e9:.1f}GB"
            else:
                mem_str = ""
            
            print(f"\n{'='*80}", flush=True)
            print(f"[{progress_pct:.1f}%] Step {step_total}/{args.steps}", flush=True)
            if args.use_keytoken_loss:
                print(f"  Loss: {loss_meter.avg:.4f} | Contrastive: {loss_contrastive_value:.4f} | L2: {loss_robust_value:.4f} | MAE: {loss_mae_value:.4f} | Detect: {loss_detect_value:.4f}", flush=True)
            else:
                print(f"  Loss: {loss_meter.avg:.4f} | MAE: {mae_loss_meter.avg:.4f}", flush=True)
            print(f"  CleanAcc: {acc_meter.avg:.4f} | RobustAcc: {racc_meter.avg:.4f}", flush=True)
            print(f"  {feat_diff_str}", flush=True)
            if mem_str:
                print(f"  {mem_str}", flush=True)
            print(f"{'='*80}\n", flush=True)
            
            # ğŸ† æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³å‡†ç¡®ç‡checkpoint
            current_racc = racc_meter.avg
            if args.save_checkpoints and current_racc > best_acc:
                best_acc = current_racc
                save_checkpoint(
                    model, optimizer, scheduler, step_total, epoch,
                    args, filename='best.pt'
                )
                print(f"ğŸ† æ–°çš„æœ€ä½³RobustAcc: {best_acc:.4f} - å·²ä¿å­˜best.pt", flush=True)

        # ä¿å­˜checkpoint
        if args.save_checkpoints and step_total % args.checkpoint_freq == 0 and (i + 1) % accumulation_steps == 0:
            save_checkpoint(
                model, optimizer, scheduler, step_total, epoch,
                args, filename=f'step_{step_total}.pt'
            )
        
        # æ¯10%è¿›åº¦é¢å¤–ä¿å­˜ä¸€ä¸ªé‡Œç¨‹ç¢‘checkpoint
        if args.save_checkpoints and step_total % (args.steps // 10) == 0 and (i + 1) % accumulation_steps == 0:
            save_checkpoint(
                model, optimizer, scheduler, step_total, epoch,
                args, filename=f'milestone_step_{step_total}.pt'
            )

        if step_total >= args.steps:
            break
        
        # âš¡ å®šæœŸæ¸…ç†GPUç¼“å­˜
        if i % 100 == 0:
            torch.cuda.empty_cache()

    return step_total, best_acc


def main(args):
    # ğŸ² éšæœºç§å­å¤„ç†ï¼šåªåœ¨é¦–æ¬¡è®­ç»ƒæ—¶è®¾ç½®ï¼Œresumeæ—¶ä¼šä»checkpointæ¢å¤
    is_resuming = bool(args.resume)
    
    if not is_resuming:
        # é¦–æ¬¡è®­ç»ƒï¼šè®¾ç½®æ–°çš„éšæœºç§å­
        actual_seed = set_random_seed(args.seed)
        args.seed = actual_seed  # æ›´æ–°argsä»¥è®°å½•å®é™…ä½¿ç”¨çš„ç§å­
        
        print(f"\n{'=' * 60}")
        print(f"ğŸ² éšæœºç§å­: {actual_seed}")
        print(f"   å¯é€šè¿‡ --seed {actual_seed} å®Œå…¨å¤ç°æ­¤æ¬¡è®­ç»ƒ")
        print(f"{'=' * 60}\n")
    else:
        # Resumeè®­ç»ƒï¼šéšæœºçŠ¶æ€å°†ä»checkpointæ¢å¤ï¼Œä¸è®¾ç½®æ–°ç§å­
        print(f"\n{'=' * 60}")
        print(f"ğŸ”„ ä»checkpointæ¢å¤è®­ç»ƒ")
        print(f"   éšæœºçŠ¶æ€å°†ä»checkpointæ¢å¤ï¼ˆä¿æŒè·¨Stageè¿ç»­æ€§ï¼‰")
        print(f"{'=' * 60}\n")
    
    # è®¾ç½®wandb
    if args.wandb:
        init_wandb(
            project_name='clip-finetune-enhanced',
            model_name=args.experiment_name or 'enhanced_clip',
            config=vars(args)
        )
    else:
        wandb.init(mode='disabled')

    # æ‰“å°å‚æ•°
    print(f"Arguments:\n{'-' * 50}")
    for arg, value in vars(args).items():
        print(f"{arg:30s}: {value}")
    print(f"{'-' * 50}")

    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.overwrite:
        shutil.rmtree(args.output_dir, ignore_errors=True)
    os.makedirs(os.path.join(args.output_dir, 'checkpoints'), exist_ok=True)

    # ä¿å­˜å‚æ•°å’Œç§å­åˆ°æ–‡ä»¶
    with open(os.path.join(args.output_dir, 'args.txt'), 'w') as f:
        f.write(str(args))
    
    # å•ç‹¬ä¿å­˜ç§å­ä¾¿äºæŸ¥æ‰¾ï¼ˆåªåœ¨é¦–æ¬¡è®­ç»ƒæ—¶ä¿å­˜ï¼‰
    if not is_resuming and hasattr(args, 'seed') and args.seed is not None:
        with open(os.path.join(args.output_dir, 'random_seed.txt'), 'w') as f:
            f.write(f"Random Seed: {args.seed}\n")
            f.write(f"Command to reproduce: --seed {args.seed}\n")

    main_device = 0
    num_gpus = torch.cuda.device_count()
    
    # åŠ è½½æ¨¡å‹
    model_orig, _, image_processor = open_clip.create_model_and_transforms(
        args.clip_model_name, pretrained='openai'
    )
    if args.pretrained != 'openai':
        model, _, _ = load_clip_model(args.clip_model_name, args.pretrained)
    else:
        model = model_orig

    # é¢„å¤„ç†
    preprocessor_without_normalize = transforms.Compose(image_processor.transforms[:-1])
    normalize = image_processor.transforms[-1]

    # åŠ è½½æ•°æ®é›†
    if args.dataset == 'imagenet':
        dataset = ImageNetDataset(
            root=args.imagenet_root + '/train',
            transform=preprocessor_without_normalize,
        )
        dataset_eval = ImageNetDataset(
            root=args.imagenet_root + '/val',
            transform=preprocessor_without_normalize,
        )
    else:
        raise ValueError(f'Unknown dataset: {args.dataset}')

    # âš¡ DataLoaderä¼˜åŒ–ï¼ˆé€‚é…HDD/SSDï¼‰
    # - num_workers: ä»å‘½ä»¤è¡Œå‚æ•°è·å–ï¼ŒHDDå»ºè®®2-4ï¼ŒSSDå¯8-12
    # - prefetch_factor: ä»å‘½ä»¤è¡Œå‚æ•°è·å–ï¼Œé™ä½å¯å‡å°‘I/Oå‹åŠ›
    # - pin_memory: åŠ é€ŸGPUä¼ è¾“
    # - persistent_workers: é¿å…workeré‡å¯å¼€é”€ï¼ˆworkers>0æ—¶å¯ç”¨ï¼‰
    
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°é…ç½®
    num_workers = args.num_workers
    prefetch_factor = args.prefetch_factor if num_workers > 0 else None
    use_persistent_workers = num_workers > 0
    
    dataloader_kwargs = {
        'batch_size': args.batch_size,
        'shuffle': True,
        'num_workers': num_workers,
        'drop_last': True,
        'pin_memory': True
    }
    
    # åªæœ‰å½“num_workers>0æ—¶æ‰æ·»åŠ prefetch_factorå’Œpersistent_workers
    if num_workers > 0:
        dataloader_kwargs['prefetch_factor'] = prefetch_factor
        dataloader_kwargs['persistent_workers'] = use_persistent_workers
    
    dataloader = DataLoader(dataset, **dataloader_kwargs)
    dataloader_eval = DataLoader(dataset_eval, **dataloader_kwargs)
    
    print(f"âš¡ DataLoaderé…ç½®: {num_workers} workers, prefetch={prefetch_factor}, pin_memory=True, persistent={use_persistent_workers}")

    # è·å–æ–‡æœ¬åµŒå…¥
    template = 'This is a photo of a {}'
    texts = [template.format(c) for c in IMAGENET_1K_CLASS_ID_TO_LABEL.values()]
    text_tokens = open_clip.tokenize(texts)
    model_orig.to(main_device)
    
    with torch.no_grad():
        embedding_text_labels_norm = []
        for el in (text_tokens[:500], text_tokens[500:]):
            embedding_text_labels_norm.append(
                model_orig.encode_text(el.to(main_device), normalize=True).detach().cpu()
            )
        embedding_text_labels_norm = torch.cat(embedding_text_labels_norm).T.to(main_device)
    
    model_orig.cpu()
    
    # åŒ…è£…æ¨¡å‹
    model_orig = EnhancedClipVisionModel(model=model_orig.visual, args=args, normalize=normalize)
    model = EnhancedClipVisionModel(model=model.visual, args=args, normalize=normalize)
    
    # å¤šGPU
    if num_gpus > 1:
        model_orig = nn.DataParallel(model_orig)
        model = nn.DataParallel(model)
    
    model_orig.cuda()
    model.cuda()

    # å‚æ•°å†»ç»“
    if args.freeze_clip_backbone:
        freeze_clip_backbone(model, freeze_encoder_layers=args.freeze_encoder_layers)
        print_trainable_params(model)
        
        # åªä¼˜åŒ–å¯è®­ç»ƒçš„å‚æ•°
        params = get_trainable_params(model)
        print(f"âœ… ä¼˜åŒ–å™¨å°†åªæ›´æ–° {len(params)} ç»„å¯è®­ç»ƒå‚æ•°\n")
    else:
        # è®­ç»ƒæ‰€æœ‰å‚æ•°ï¼ˆåŒ…æ‹¬CLIPå’Œæ–°å¢æ¨¡å—ï¼‰
        params = unwrap_model(model).parameters()
        print(f"âš ï¸  è®­ç»ƒæ‰€æœ‰å‚æ•°ï¼ˆCLIP + æ–°å¢æ¨¡å—ï¼Œæœªå†»ç»“ï¼‰\n")
    
    # ä¼˜åŒ–å™¨
    if args.opt == 'adamw':
        optimizer = torch.optim.AdamW(params, lr=args.lr, weight_decay=args.wd)
    elif args.opt == 'sgd':
        optimizer = torch.optim.SGD(
            params, lr=args.lr, momentum=args.momentum_sgd, weight_decay=args.wd
        )

    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler = cosine_lr(optimizer, args.lr, args.warmup, args.steps)

    # âš¡ åˆå§‹åŒ–AMP GradScalerï¼ˆå¢åŠ ç¨³å®šæ€§å‚æ•°ï¼‰
    scaler = None
    if args.use_amp:
        # ä½¿ç”¨æ›´ä¿å®ˆçš„scalerå‚æ•°é¿å…æ¢¯åº¦çˆ†ç‚¸
        scaler = torch.cuda.amp.GradScaler(
            init_scale=2.**10,  # é™ä½åˆå§‹ç¼©æ”¾ï¼ˆé»˜è®¤2^16ï¼‰
            growth_interval=2000  # å¢åŠ å¢é•¿é—´éš”ï¼ˆé»˜è®¤2000ï¼‰
        )
        print("âš¡ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ(AMP) - é¢„è®¡èŠ‚çœ~30%æ˜¾å­˜")
        print("âš¡ GradScaleré…ç½®: init_scale=1024, growth_interval=2000ï¼ˆä¿å®ˆæ¨¡å¼ï¼‰")
    
    # âš¡ æ˜¾ç¤ºæ˜¾å­˜ä¼˜åŒ–é…ç½®
    print(f"\n{'='*60}")
    print(f"âš¡ æ˜¾å­˜ä¼˜åŒ–é…ç½®:")
    print(f"   æ··åˆç²¾åº¦(AMP): {'âœ… å¼€å¯' if args.use_amp else 'âŒ å…³é—­'}")
    print(f"   æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {args.gradient_accumulation_steps}")
    print(f"   æœ‰æ•ˆbatch size: {args.batch_size * args.gradient_accumulation_steps}")
    print(f"   å†…å­˜é«˜æ•ˆæ¨¡å¼: {'âœ… å¼€å¯' if args.memory_efficient_mode else 'âŒ å…³é—­'}")
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            mem_total = torch.cuda.get_device_properties(i).total_memory / 1e9
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)} ({mem_total:.1f}GB)")
    print(f"{'='*60}\n")

    # æ–­ç‚¹ç»­è¿
    step_total = args.start_step
    epoch = 0
    
    if args.resume:
        checkpoint_dir = os.path.join(args.output_dir, 'checkpoints')
        
        if args.resume == 'auto':
            # è‡ªåŠ¨æ£€æµ‹æœ€æ–°checkpoint
            checkpoint_path = find_latest_checkpoint(checkpoint_dir)
            if checkpoint_path:
                step_total, epoch = load_checkpoint(checkpoint_path, model, optimizer, scheduler)
            else:
                print("âš ï¸  æœªæ‰¾åˆ°checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        else:
            # ä»æŒ‡å®šcheckpointæ¢å¤
            checkpoint_path = args.resume
            if not os.path.isabs(checkpoint_path):
                checkpoint_path = os.path.join(checkpoint_dir, checkpoint_path)
            
            if os.path.exists(checkpoint_path):
                step_total, epoch = load_checkpoint(checkpoint_path, model, optimizer, scheduler)
            else:
                print(f"âš ï¸  Checkpointä¸å­˜åœ¨: {checkpoint_path}ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
    
    # è®­ç»ƒ
    # è€ƒè™‘æ¢¯åº¦ç´¯ç§¯è®¡ç®—å®é™…epochæ•°
    total_epochs = args.steps * args.gradient_accumulation_steps / len(dataloader)
    print(f'è®­ç»ƒ {total_epochs:.1f} epochsï¼Œä»step {step_total}å¼€å§‹')
    
    # ğŸ† åˆå§‹åŒ–æœ€ä½³å‡†ç¡®ç‡è·Ÿè¸ª
    best_acc = 0.0
    
    while step_total < args.steps:
        step_total, best_acc = train_one_epoch(
            step_total, model, model_orig, dataloader,
            optimizer, scheduler, normalize,
            embedding_text_labels_norm, args, epoch, dataloader_eval,
            scaler=scaler,  # âš¡ ä¼ é€’scaler
            best_acc=best_acc  # ğŸ† ä¼ é€’best_acc
        )
        
        # æ¯ä¸ªepochç»“æŸæ—¶ä¿å­˜checkpoint
        save_checkpoint(
            model, optimizer, scheduler, step_total, epoch + 1,
            args, filename=f'epoch_{epoch+1}.pt'
        )
        print(f'âœ… Epoch {epoch+1} å®Œæˆ')
        epoch += 1
        
        # âš¡ epochç»“æŸåæ¸…ç†æ˜¾å­˜
        torch.cuda.empty_cache()

    # ä¿å­˜æœ€ç»ˆcheckpoint
    save_checkpoint(
        model, optimizer, scheduler, step_total, epoch,
        args, filename='final.pt'
    )

    print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³RobustAcc: {best_acc:.4f}")


if __name__ == '__main__':
    args = parser.parse_args()
    
    # è½¬æ¢epså’Œstepsizeï¼ˆä¸adversarial_training_clip.pyä¿æŒä¸€è‡´ï¼‰
    args.eps /= 255
    args.stepsize_adv /= 255
    
    # å‚æ•°éªŒè¯ï¼ˆä¸åŸå§‹è„šæœ¬ä¿æŒä¸€è‡´ï¼‰
    assert not any([isinstance(x, str) and x in ['True', 'False'] for x in args.__dict__.values()]), \
        f'args contains a string that should be a bool: {args}'
    assert args.eval_freq % args.log_freq == 0, 'eval_freq must be a multiple of log_freq'
    
    # è®¾ç½®å®éªŒåç§°
    if not args.experiment_name:
        args.experiment_name = f'enhanced_clip_{args.clip_model_name}_eps{int(args.eps*255)}'
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if not args.output_dir:
        args.output_dir = f'output/{args.experiment_name}'
    
    args.finetuned_model_name = args.experiment_name
    
    main(args)
